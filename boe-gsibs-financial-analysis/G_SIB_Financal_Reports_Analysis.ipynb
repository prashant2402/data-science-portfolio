{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyYaQOU4z6WY"
   },
   "source": [
    "# üè¶ Quarterly Earnings Reports Analysis for Bank of England Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a comprehensive pipeline for analyzing quarterly financial disclosures from **Global Systemically Important Banks (G-SIBs)**. It was developed as part of a data science employer project in collaboration with the **Bank of England** and the **University of Cambridge**.\n",
    "\n",
    "We combine **financial data extraction**, **language model summarization**, and **credit risk metric analysis** to derive structured insights from messy, unstructured documents such as:\n",
    "\n",
    "- üìÑ Quarterly financial reports (PDFs)\n",
    "- üó£Ô∏è Earnings call transcripts (SeekingAlpha & official bank websites)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objective\n",
    "\n",
    "To extract meaningful insights and risk indicators from G-SIB disclosures using a combination of:\n",
    "- **AI-driven summarization** (Claude, GPT-4o, Mistral)\n",
    "- **Topic modeling & sentiment analysis** (BERTopic, FinBERT)\n",
    "- **Metric extraction** (Google Document AI)\n",
    "- **Semantic + lexical search fusion** (FAISS + BM25)\n",
    "\n",
    "All outputs are mapped to a unified **credit risk framework**, including metrics like:\n",
    "- **CET1 Ratio**\n",
    "- **NPL Ratio**\n",
    "- **Coverage**\n",
    "- **PCL**\n",
    "- **Texas Ratio**\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Notebook Flow\n",
    "\n",
    "1. **Setup & Dependencies**\n",
    "2. **Configuration & API Keys (via `.env`)**\n",
    "3. **Data Ingestion (Reports + Transcripts)**\n",
    "4. **Document Processing Pipelines**\n",
    "5. **Metrics & Risk Analysis**\n",
    "6. **LLM-based Insights Generation**\n",
    "7. **Visualization & Benchmarking**\n",
    "\n",
    "---\n",
    "\n",
    "> **Disclaimer:** This project uses paid APIs (e.g., Claude, GPT-4, Document AI). Processing multiple transcripts may incur costs. Credentials should be stored securely via `.env` files and never pushed to public repositories.\n",
    "\n",
    "---\n",
    "\n",
    "üìé Learn more in the [README.md](./README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IMPORTANT: Using this notebook incurs costs.**  This implementation leverages the Claude API and OpenAI GPT4o, which is a paid service.\n",
    "\n",
    "While we employ batch processing and prompt caching to reduce costs by approximately 75%, expect to incur a cost of roughly $1 per Q&A transcript processed.\n",
    "\n",
    "**Cost Justification:** Despite the cost, this is our most advanced and cutting-edge solution for parsing unstructured earnings call transcripts.  The superior accuracy and insights derived from Claude significantly outweigh the expenses, especially when considering the time savings and enhanced analysis compared to other methods.  \n",
    "\n",
    "**General Applicability:** This solution is not limited to seekingalpha transcripts format but it effectively parses *any* bank transcript format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è **Note for Users & Reviewers:**  \n",
    "> This notebook comes with a fully processed SQLite database (`gsib_analysis_FINAL.db`) located in `/data`.  \n",
    "> You can skip all transcript and document parsing steps and go straight to **visualization, scoring, and benchmarking**.\n",
    "\n",
    "‚úÖ Simply run the config + setup cells at the top of this notebook, then skip to:\n",
    "- üìä Visualization & Reporting\n",
    "- üîç Metric Exploration\n",
    "- üí¨ GPT-powered Answer Generation (optional)\n",
    "\n",
    "This allows you to view insights instantly without incurring compute/API costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXHqkUmd0LxR"
   },
   "source": [
    "## üöÄ Install Packages and Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "IepMyz0IMUs9",
    "outputId": "c65245f7-3747-436a-efd8-587b8c6c652e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pymupdf pypdf pydrive2 pandas openai google-cloud-documentai google-auth \\\n",
    "                google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client \\\n",
    "                sentence-transformers transformers torch numpy plotly nltk spacy scikit-learn \\\n",
    "                ipywidgets bertopic faiss-cpu langchain rank-bm25 \\\n",
    "                langchain-community huggingface_hub beautifulsoup4 PyPDF2 anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDtqc7WAMnpN",
    "outputId": "4542229b-6317-4f0d-fe5d-ed28dcea1f28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/prash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/prash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/prash/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2025-03-28 17:05:46,621 - root - INFO - Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# üìå Standard Library Imports\n",
    "# ===========================\n",
    "import asyncio\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ===========================\n",
    "# ü§ñ AI & LLM API Integrations\n",
    "# ===========================\n",
    "import anthropic\n",
    "\n",
    "# ===========================\n",
    "# üìå Machine Learning & NLP\n",
    "# ===========================\n",
    "import faiss\n",
    "\n",
    "# =========================================\n",
    "# üìå PDF & Document Processing\n",
    "# =========================================\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# ==============================\n",
    "# üìå Text Processing & Spell Checking\n",
    "# ==============================\n",
    "\n",
    "import nest_asyncio\n",
    "import nltk\n",
    "\n",
    "# ================================\n",
    "# üìå Numerical & Data Processing\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import pypdf\n",
    "import requests\n",
    "import spacy\n",
    "import torch\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from bertopic import BERTopic\n",
    "from bs4 import BeautifulSoup\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from IPython.display import Markdown, display\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from openai import OpenAI\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# =====================\n",
    "# üìå Utility Libraries\n",
    "# =====================\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from umap import UMAP\n",
    "\n",
    "# =============================\n",
    "# üìå Configure Warnings & Logging\n",
    "# =============================\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# =============================\n",
    "# üîΩ Download Required NLTK Data\n",
    "# =============================\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# =============================\n",
    "# ‚öôÔ∏è Configure GPU (if available)\n",
    "# =============================\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ‚öôÔ∏è Set Random Seeds for Reproducibility\n",
    "# =============================\n",
    "def set_seeds(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# =============================\n",
    "# ‚è© Enable tqdm Progress Bars\n",
    "# =============================\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EALDmouQ_JVo"
   },
   "source": [
    "## üéØ Configuration & Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E6QOaNDK_A5f"
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file (if present)\n",
    "load_dotenv()\n",
    "\n",
    "# ================================\n",
    "# üìå Selected Entities & Parameters\n",
    "# ================================\n",
    "\n",
    "SELECTED_BANKS = [\"JPMorgan Chase\", \"UBS\"]  # Banks to include in analysis\n",
    "\n",
    "RISK_TYPES_AI_INSIGHTS = [\n",
    "    \"credit_risk\",\n",
    "    # \"liquidity_risk\",\n",
    "    # \"market_risk\",\n",
    "    # \"group_risk\",\n",
    "]  # Risk types to generate insights for\n",
    "\n",
    "# ================================\n",
    "# üìÇ Local Storage Paths\n",
    "# ================================\n",
    "\n",
    "DB_PATH = os.getenv(\"DB_PATH\", \"./data/gsib_analysis_FINAL.db\")\n",
    "\n",
    "CALL_TRANSCRIPTS_DOWNLOAD_FOLDER = os.getenv(\n",
    "    \"CALL_TRANSCRIPTS_DOWNLOAD_FOLDER\", \"./data/earnings_transcripts\"\n",
    ")\n",
    "\n",
    "QUARTERLY_REPORTS_DOWNLOAD_FOLDER = os.getenv(\n",
    "    \"QUARTERLY_REPORTS_DOWNLOAD_FOLDER\", \"./data/quarterly_reports\"\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# üîê API Keys & Credentials\n",
    "# ================================\n",
    "\n",
    "# Google Cloud Document AI\n",
    "GOOGLE_AUTH_JSON_FILE = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\", \"./secrets/google-auth.json\")\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = GOOGLE_AUTH_JSON_FILE\n",
    "\n",
    "# Search APIs\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_CX = os.getenv(\"GOOGLE_CX\")\n",
    "\n",
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "BRIGHTDATA_API_KEY = os.getenv(\"BRIGHTDATA_API_KEY\")\n",
    "\n",
    "# OpenAI / Anthropic\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = os.getenv(\"ANTHROPIC_API_KEY\", \"\")\n",
    "\n",
    "# ================================\n",
    "# ü§ñ Google Document AI Config\n",
    "# ================================\n",
    "\n",
    "DOCUMENT_AI_PROJECT_ID = os.getenv(\"DOCUMENT_AI_PROJECT_ID\", \"974627024770\")\n",
    "DOCUMENT_AI_LOCATION = os.getenv(\"DOCUMENT_AI_LOCATION\", \"eu\")\n",
    "DOCUMENT_AI_PROCESSOR_ID = os.getenv(\"DOCUMENT_AI_PROCESSOR_ID\", \"9d55d1c14e76de83\")\n",
    "DOCUMENT_AI_OUTPUT_DIR = os.getenv(\"DOCUMENT_AI_OUTPUT_DIR\", \"output\")\n",
    "# Optional processor version (if needed)\n",
    "DOCUMENT_AI_PROCESSOR_VERSION = os.getenv(\"DOCUMENT_AI_PROCESSOR_VERSION\", None)\n",
    "\n",
    "# ================================\n",
    "# ‚ö° Global Caches (for performance)\n",
    "# ================================\n",
    "\n",
    "_MODEL_CACHE = {}\n",
    "_TOKENIZER_CACHE = {}\n",
    "_PIPELINE_CACHE = {}\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Disable parallelism for tokenizers\n",
    "\n",
    "# ================================\n",
    "# üìò Utility Functions\n",
    "# ================================\n",
    "\n",
    "def md_print(text):\n",
    "    \"\"\"Display formatted markdown in notebook output\"\"\"\n",
    "    from IPython.display import Markdown, display\n",
    "    display(Markdown(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3zMw2L85FgV"
   },
   "source": [
    "## üìÇ Database Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mcn1mMDikGdX",
    "outputId": "c8636275-30bb-4a18-f260-960fae3d7280"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:06:22,968 - GSIBDatabase - INFO - WAL mode enabled for database\n",
      "2025-03-28 17:06:22,968 - GSIBDatabase - INFO - WAL mode enabled for database\n"
     ]
    }
   ],
   "source": [
    "class GSIBDatabase:\n",
    "    \"\"\"Database manager for G-SIB quarterly announcements analysis system\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str = \"gsib_analysis.db\",\n",
    "        log_level=logging.INFO,\n",
    "        enable_wal=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize database connection and logger\n",
    "\n",
    "        Args:\n",
    "            db_path: Path to SQLite database file\n",
    "            log_level: Logging level\n",
    "            enable_wal: Whether to enable WAL mode (default: True)\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(\"GSIBDatabase\")\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "\n",
    "        # Initialize database if it doesn't exist\n",
    "        if not os.path.exists(db_path):\n",
    "            self.init_database()\n",
    "\n",
    "        # Enable WAL mode if requested\n",
    "        if enable_wal:\n",
    "            self._enable_wal_mode()\n",
    "\n",
    "    def _enable_wal_mode(self):\n",
    "        \"\"\"Enable Write-Ahead Logging mode for better concurrency and performance\"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        try:\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "            conn.execute(\"PRAGMA synchronous=NORMAL;\")\n",
    "            conn.commit()\n",
    "            self.logger.info(\"WAL mode enabled for database\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error enabling WAL mode: {str(e)}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def checkpoint_wal(self, mode: str = \"PASSIVE\"):\n",
    "        \"\"\"\n",
    "        Checkpoint the WAL file to manage its size\n",
    "\n",
    "        Args:\n",
    "            mode: Checkpoint mode ('PASSIVE', 'FULL', 'RESTART', or 'TRUNCATE')\n",
    "                 PASSIVE: Only checkpoint if no readers are active\n",
    "                 FULL: Checkpoint even if it delays reads/writes\n",
    "                 RESTART: Checkpoint and restart WAL file\n",
    "                 TRUNCATE: Checkpoint and truncate WAL file\n",
    "        \"\"\"\n",
    "        valid_modes = [\"PASSIVE\", \"FULL\", \"RESTART\", \"TRUNCATE\"]\n",
    "        if mode not in valid_modes:\n",
    "            mode = \"PASSIVE\"\n",
    "            self.logger.warning(\n",
    "                f\"Invalid checkpoint mode, using PASSIVE. Valid modes: {valid_modes}\",\n",
    "            )\n",
    "\n",
    "        conn = self.get_db_connection()\n",
    "        try:\n",
    "            conn.execute(f\"PRAGMA wal_checkpoint({mode});\")\n",
    "            self.logger.info(f\"WAL checkpoint completed with mode: {mode}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during WAL checkpoint: {str(e)}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def cursor_to_dict(self, cursor, rows):\n",
    "        \"\"\"\n",
    "        Convert database cursor rows to a list of dictionaries.\n",
    "\n",
    "        Args:\n",
    "            cursor: Database cursor object\n",
    "            rows: Fetched rows from cursor\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries where keys are column names\n",
    "        \"\"\"\n",
    "        # Get column names from cursor description\n",
    "        columns = [column[0] for column in cursor.description]\n",
    "        # Convert rows to dictionaries\n",
    "        return [dict(zip(columns, row)) for row in rows]\n",
    "\n",
    "    def begin_transaction(self):\n",
    "        \"\"\"\n",
    "        Begin a new transaction and return a connection object\n",
    "\n",
    "        Returns:\n",
    "            SQLite connection with active transaction\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        return conn\n",
    "\n",
    "    def commit_transaction(self, conn):\n",
    "        \"\"\"\n",
    "        Commit an active transaction\n",
    "\n",
    "        Args:\n",
    "            conn: SQLite connection with active transaction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error committing transaction: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def rollback_transaction(self, conn):\n",
    "        \"\"\"\n",
    "        Roll back an active transaction\n",
    "\n",
    "        Args:\n",
    "            conn: SQLite connection with active transaction\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn.rollback()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error rolling back transaction: {str(e)}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def execute_in_transaction(self, func):\n",
    "        \"\"\"\n",
    "        Execute a function within a transaction context\n",
    "\n",
    "        Args:\n",
    "            func: Function to execute. Must accept a connection object as its first argument.\n",
    "                 The function should not commit or close the connection.\n",
    "\n",
    "        Returns:\n",
    "            The result of the function call\n",
    "        \"\"\"\n",
    "        conn = self.begin_transaction()\n",
    "        try:\n",
    "            result = func(conn)\n",
    "            conn.commit()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Transaction failed and was rolled back: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_db_connection(self):\n",
    "        \"\"\"Get a connection to the SQLite database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path, isolation_level=\"EXCLUSIVE\")\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        return conn\n",
    "\n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database with required tables based on provided schema\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create banks table\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS banks (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            bank_name TEXT UNIQUE NOT NULL\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Create reports table\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS reports (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            bank_id INTEGER NOT NULL,\n",
    "            report_name TEXT NOT NULL,\n",
    "            year INTEGER NOT NULL,\n",
    "            quarter TEXT NOT NULL,\n",
    "            report_date TEXT,\n",
    "            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (bank_id) REFERENCES banks (id) ON DELETE CASCADE,\n",
    "            UNIQUE (bank_id, year, quarter)\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Create metrics table\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS metrics (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            report_id INTEGER NOT NULL,\n",
    "            metric_name TEXT NOT NULL,\n",
    "            metric_value REAL,\n",
    "            formatted_value TEXT,\n",
    "            in_typical_range BOOLEAN,\n",
    "            FOREIGN KEY (report_id) REFERENCES reports (id) ON DELETE CASCADE,\n",
    "            UNIQUE (report_id, metric_name)\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Analyst conversations table (combines all Q&A for one analyst in a report)\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS analyst_conversations (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            report_id INTEGER NOT NULL,\n",
    "            analyst_name TEXT NOT NULL,\n",
    "            analyst_company TEXT,\n",
    "            topic_id INTEGER,\n",
    "            topic_probability REAL,\n",
    "            FOREIGN KEY (report_id) REFERENCES reports (id) ON DELETE CASCADE\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "        # Topics table\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS topics (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            name TEXT NOT NULL,\n",
    "            keywords TEXT NOT NULL,  -- JSON array of keywords\n",
    "            category TEXT NOT NULL   -- e.g., 'CREDIT_RISK', 'MARKET_RISK', etc.\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Individual QA pairs\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS qa_pairs (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            conversation_id INTEGER NOT NULL,\n",
    "            question TEXT NOT NULL,          -- Original raw question\n",
    "            answer TEXT NOT NULL,            -- Original raw answer\n",
    "            answer_speaker TEXT NOT NULL,\n",
    "            answer_role TEXT NOT NULL,\n",
    "            FOREIGN KEY (conversation_id) REFERENCES analyst_conversations (id) ON DELETE CASCADE\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Simplified preprocessed text for NLP and topic modeling\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS preprocessed_text (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            qa_pair_id INTEGER NOT NULL,\n",
    "            text_type TEXT NOT NULL,         -- 'question', 'answer', or 'combined'\n",
    "            preprocessing_level TEXT NOT NULL, -- 'processed'\n",
    "            processed_text TEXT NOT NULL,\n",
    "            processing_metadata TEXT,        -- JSON with additional processing info\n",
    "            FOREIGN KEY (qa_pair_id) REFERENCES qa_pairs (id) ON DELETE CASCADE,\n",
    "            UNIQUE (qa_pair_id, text_type, preprocessing_level)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Credit risk mention keywords\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS credit_risk_keywords (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            keyword TEXT UNIQUE NOT NULL,\n",
    "            metric_category TEXT NOT NULL,  -- 'Credit Risk'\n",
    "            relevance_score REAL            -- Indicates how strongly the keyword relates to the category\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Credit risk mentions in conversations with sentiment\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS credit_risk_mentions (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            conversation_id INTEGER NOT NULL,\n",
    "            keyword_id INTEGER NOT NULL,\n",
    "            mention_count INTEGER NOT NULL,\n",
    "            mention_context TEXT,  -- Sample of text containing the mention\n",
    "            positive_score REAL NOT NULL,\n",
    "            negative_score REAL NOT NULL,\n",
    "            neutral_score REAL NOT NULL,\n",
    "            compound_score REAL NOT NULL, -- -1 to 1 scale\n",
    "            FOREIGN KEY (conversation_id) REFERENCES analyst_conversations (id) ON DELETE CASCADE,\n",
    "            FOREIGN KEY (keyword_id) REFERENCES credit_risk_keywords (id) ON DELETE CASCADE,\n",
    "            UNIQUE (conversation_id, keyword_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Sentiment analysis at conversation level\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS conversation_sentiment (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            conversation_id INTEGER NOT NULL,\n",
    "            positive_score REAL NOT NULL,\n",
    "            negative_score REAL NOT NULL,\n",
    "            neutral_score REAL NOT NULL,\n",
    "            compound_score REAL NOT NULL, -- -1 to 1 scale\n",
    "            FOREIGN KEY (conversation_id) REFERENCES analyst_conversations (id) ON DELETE CASCADE,\n",
    "            UNIQUE (conversation_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Sentiment analysis per speaker per topic\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS speaker_topic_sentiment (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            conversation_id INTEGER NOT NULL,\n",
    "            speaker_name TEXT NOT NULL,\n",
    "            speaker_role TEXT NOT NULL,\n",
    "            positive_score REAL NOT NULL,\n",
    "            negative_score REAL NOT NULL,\n",
    "            neutral_score REAL NOT NULL,\n",
    "            compound_score REAL NOT NULL, -- -1 to 1 scale\n",
    "            FOREIGN KEY (conversation_id) REFERENCES analyst_conversations (id) ON DELETE CASCADE,\n",
    "            UNIQUE (conversation_id, speaker_name, speaker_role)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Vector embeddings table (for FAISS integration)\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE vector_embeddings (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            preprocessed_text_id INTEGER NOT NULL,\n",
    "            embedding_model TEXT NOT NULL,\n",
    "            preprocessing_level TEXT NOT NULL,\n",
    "            vector_data BLOB,\n",
    "            chunk_text TEXT,\n",
    "            metadata TEXT,\n",
    "            FOREIGN KEY (preprocessed_text_id) REFERENCES preprocessed_text (id) ON DELETE CASCADE,\n",
    "            UNIQUE (preprocessed_text_id, embedding_model, preprocessing_level)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Create faiss_indices table if not exists\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS faiss_indices (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            name TEXT NOT NULL,\n",
    "            embedding_dim INTEGER NOT NULL,\n",
    "            model_name TEXT NOT NULL,\n",
    "            config TEXT,  -- JSON with index parameters\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Create search_queries table if not exists\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS search_queries (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            query_text TEXT NOT NULL,\n",
    "            embedding BLOB,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)\n",
    "        \"\"\")\n",
    "\n",
    "        # Create search_results table if not exists\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS search_results (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            query_id INTEGER NOT NULL,\n",
    "            qa_pair_id INTEGER NOT NULL,\n",
    "            semantic_score REAL,\n",
    "            lexical_score REAL,\n",
    "            reranker_score REAL,\n",
    "            final_score REAL,\n",
    "            FOREIGN KEY (query_id) REFERENCES search_queries (id),\n",
    "            FOREIGN KEY (qa_pair_id) REFERENCES qa_pairs (id)\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        self.logger.info(f\"Database initialized at {self.db_path}\")\n",
    "\n",
    "    def get_llm_cleaned_entries_for_processing(\n",
    "        self,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get entries with llm_cleaned level but missing tokenized level\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of entries to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with qa_pair_id, text_type, and processed_text\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Find QA pairs with llm_cleaned but missing other preprocessing levels\n",
    "            query = \"\"\"\n",
    "            SELECT DISTINCT pt.qa_pair_id, pt.text_type, pt.processed_text\n",
    "            FROM preprocessed_text pt\n",
    "            WHERE pt.preprocessing_level = 'llm_cleaned'\n",
    "            AND NOT EXISTS (\n",
    "                SELECT 1 FROM preprocessed_text pt2\n",
    "                WHERE pt2.qa_pair_id = pt.qa_pair_id\n",
    "                AND pt2.text_type = pt.text_type\n",
    "                AND pt2.preprocessing_level = 'tokenized'\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving llm_cleaned entries: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Bank Methods\n",
    "    def add_bank(self, bank_name: str) -> int:\n",
    "        \"\"\"\n",
    "        Add a new bank to the database\n",
    "\n",
    "        Args:\n",
    "            bank_name: Name of the bank\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created bank\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO banks (bank_name) VALUES (?)\",\n",
    "                (bank_name,),\n",
    "            )\n",
    "\n",
    "            conn.commit()\n",
    "            if cursor.rowcount == 0:\n",
    "                # Bank already exists, get its ID\n",
    "                cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "                return cursor.fetchone()[0]\n",
    "            return cursor.lastrowid\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding bank: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_all_banks(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all banks from the database\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with bank information\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\"SELECT id, bank_name FROM banks ORDER BY bank_name\")\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving banks: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Report Methods\n",
    "    def add_report(\n",
    "        self,\n",
    "        bank_id: int,\n",
    "        report_name: str,\n",
    "        year: int,\n",
    "        quarter: str,\n",
    "        report_date: Optional[str] = None,\n",
    "    ) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Add a new report to the database\n",
    "\n",
    "        Args:\n",
    "            bank_id: ID of the bank\n",
    "            report_name: Name of the report\n",
    "            year: Year of the report\n",
    "            quarter: Quarter of the report (Q1, Q2, Q3, Q4)\n",
    "            report_date: Date of the report in ISO format (YYYY-MM-DD)\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created report\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO reports\n",
    "                (bank_id, report_name, year, quarter, report_date)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (bank_id, report_name, year, quarter, report_date),\n",
    "            )\n",
    "\n",
    "            report_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return report_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding report: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def delete_report_data(self, report_id: int):\n",
    "        \"\"\"\n",
    "        Delete all data associated with a report\n",
    "\n",
    "        Args:\n",
    "            report_id: ID of the report\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                DELETE FROM reports WHERE id = ?\n",
    "                \"\"\",\n",
    "                (report_id,),\n",
    "            )\n",
    "\n",
    "            conn.commit()\n",
    "            self.logger.info(f\"Deleted all data for report ID {report_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error deleting report data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_reports_by_bank(self, bank_id: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all reports for a specific bank\n",
    "\n",
    "        Args:\n",
    "            bank_id: ID of the bank\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with report information\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, report_name, year, quarter, report_date\n",
    "                FROM reports\n",
    "                WHERE bank_id = ?\n",
    "                ORDER BY year DESC, quarter DESC\n",
    "                \"\"\",\n",
    "                (bank_id,),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving reports: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_report_id(self, bank_name: str, year: int, quarter: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Get report ID for a specific bank, year, and quarter\n",
    "\n",
    "        Args:\n",
    "            bank_name: Name of the bank\n",
    "            year: Report year\n",
    "            quarter: Report quarter\n",
    "\n",
    "        Returns:\n",
    "            Report ID if found, None otherwise\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT r.id\n",
    "                FROM reports r, banks b\n",
    "                WHERE r.bank_id = b.id AND b.bank_name = ?  AND year = ? AND quarter = ?\n",
    "                LIMIT 1\n",
    "                \"\"\",\n",
    "                (bank_name, year, quarter),\n",
    "            )\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0][\"id\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving report ID: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_report_by_id(self, id: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Get report by id.\n",
    "\n",
    "        Args:\n",
    "            report_id: Report Id\n",
    "\n",
    "        Returns:\n",
    "            Report if found, None otherwise\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT r.id, r.year, r.quarter, b.bank_name\n",
    "                FROM reports r, banks b\n",
    "                WHERE r.bank_id = b.id AND r.id = ?\n",
    "                LIMIT 1\n",
    "                \"\"\",\n",
    "                (id,),\n",
    "            )\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving report: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_bank_by_report(self, report_id: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get report ID for a specific bank, year, and quarter\n",
    "\n",
    "        Args:\n",
    "            report_id: Report Id\n",
    "\n",
    "        Returns:\n",
    "            Bank Name if found, None otherwise\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT b.bank_name\n",
    "                FROM reports r, banks b\n",
    "                WHERE r.bank_id = b.id AND r.id = ?\n",
    "                LIMIT 1\n",
    "                \"\"\",\n",
    "                (report_id,),\n",
    "            )\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0][\"bank_name\"]\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving bank name: {str(e)}\")\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Metrics Methods\n",
    "    def add_metric(\n",
    "        self,\n",
    "        report_id: int,\n",
    "        metric_name: str,\n",
    "        metric_value: float,\n",
    "        formatted_value: Optional[str] = None,\n",
    "        in_typical_range: Optional[bool] = None,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add a financial metric to a report\n",
    "\n",
    "        Args:\n",
    "            report_id: ID of the report\n",
    "            metric_name: Name of the metric\n",
    "            metric_value: Numeric value of the metric\n",
    "            formatted_value: Formatted string representation of the value\n",
    "            in_typical_range: Whether the value is within typical range\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created metric\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO metrics\n",
    "                (report_id, metric_name, metric_value, formatted_value, in_typical_range)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    report_id,\n",
    "                    metric_name,\n",
    "                    metric_value,\n",
    "                    formatted_value,\n",
    "                    in_typical_range,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            metric_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return metric_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding metric: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_metrics_by_report(self, report_id: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all metrics for a specific report\n",
    "\n",
    "        Args:\n",
    "            report_id: ID of the report\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with metric information\n",
    "        \"\"\"\n",
    "        if not report_id:\n",
    "            return []\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, metric_name, metric_value, formatted_value, in_typical_range\n",
    "                FROM metrics\n",
    "                WHERE report_id = ?\n",
    "                ORDER BY metric_name\n",
    "                \"\"\",\n",
    "                (report_id,),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error retrieving metrics for report {report_id}: {str(e)}\",\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Method to get metric value from list of dicts\n",
    "    def get_metric_value(self, metrics_list: List[Dict], metric_name: str) -> float:\n",
    "        \"\"\"Helper to get metric value from list of metric dictionaries\"\"\"\n",
    "        for metric in metrics_list:\n",
    "            if metric.get(\"metric_name\") == metric_name:\n",
    "                return metric.get(\"value\", 0)\n",
    "        return 0\n",
    "\n",
    "    # Method to get metric formatted_value from list of dicts\n",
    "    def get_metric_formatted_value(\n",
    "        self, metrics_list: List[Dict], metric_name: str\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"Helper to get metric value from list of metric dictionaries\"\"\"\n",
    "        for metric in metrics_list:\n",
    "            if metric.get(\"metric_name\") == metric_name:\n",
    "                return metric.get(\"formatted_value\", 0)\n",
    "        return None\n",
    "\n",
    "    def generate_enhanced_metrics_summary(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate an enhanced metrics summary with bank name, year, and quarter columns\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with concise metrics and metadata\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "\n",
    "        try:\n",
    "            # Query to join reports, banks, and metrics\n",
    "            query = \"\"\"\n",
    "            SELECT\n",
    "                b.bank_name,\n",
    "                r.year,\n",
    "                r.quarter,\n",
    "                r.report_name,\n",
    "                m.metric_name,\n",
    "                m.formatted_value\n",
    "            FROM\n",
    "                reports r\n",
    "                JOIN banks b ON r.bank_id = b.id\n",
    "                JOIN metrics m ON r.id = m.report_id\n",
    "            ORDER BY\n",
    "                b.bank_name,\n",
    "                r.year DESC,\n",
    "                r.quarter DESC,\n",
    "                r.report_name\n",
    "            \"\"\"\n",
    "\n",
    "            # Execute query and get results\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "\n",
    "            # Pivot metrics to columns\n",
    "            summary_df = df.pivot_table(\n",
    "                index=[\"bank_name\", \"year\", \"quarter\", \"report_name\"],\n",
    "                columns=\"metric_name\",\n",
    "                values=\"formatted_value\",\n",
    "                aggfunc=\"first\",\n",
    "            ).reset_index()\n",
    "\n",
    "            return summary_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating enhanced summary: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Conversation Methods\n",
    "    def add_analyst_conversation(\n",
    "        self,\n",
    "        report_id: int,\n",
    "        analyst_name: str,\n",
    "        analyst_company: Optional[str] = None,\n",
    "        topic_id: Optional[int] = None,\n",
    "        topic_probability: Optional[float] = None,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add a new analyst conversation to the database\n",
    "\n",
    "        Args:\n",
    "            report_id: ID of the report\n",
    "            analyst_name: Name of the analyst\n",
    "            analyst_company: Company of the analyst\n",
    "            topic_id: ID of the main topic\n",
    "            topic_probability: Probability of the topic\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created conversation\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO analyst_conversations\n",
    "                (report_id, analyst_name, analyst_company, topic_id, topic_probability)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (report_id, analyst_name, analyst_company, topic_id, topic_probability),\n",
    "            )\n",
    "\n",
    "            conversation_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return conversation_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding conversation: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def add_qa_pair(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        question: str,\n",
    "        answer: str,\n",
    "        answer_speaker: str,\n",
    "        answer_role: str,\n",
    "    ) -> int:\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO qa_pairs\n",
    "                (conversation_id, question, answer, answer_speaker, answer_role)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (conversation_id, question, answer, answer_speaker, answer_role),\n",
    "            )\n",
    "\n",
    "            qa_pair_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return qa_pair_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding QA pair: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def add_preprocessed_text(\n",
    "        self,\n",
    "        qa_pair_id: int,\n",
    "        text_type: str,\n",
    "        preprocessing_level: str,\n",
    "        processed_text: str,\n",
    "        processing_metadata: Optional[Dict] = None,\n",
    "    ) -> int:\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            metadata_json = None\n",
    "            if processing_metadata:\n",
    "                metadata_json = json.dumps(processing_metadata)\n",
    "\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO preprocessed_text\n",
    "                (qa_pair_id, text_type, preprocessing_level, processed_text, processing_metadata)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    qa_pair_id,\n",
    "                    text_type,\n",
    "                    preprocessing_level,\n",
    "                    processed_text,\n",
    "                    metadata_json,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            preprocessed_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return preprocessed_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding preprocessed text: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_conversations_to_analyze(\n",
    "        self,\n",
    "        limit: Optional[int] = None,\n",
    "        new_only: bool = True,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get conversations for topic analysis\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations\n",
    "            new_only: Only return conversations without existing topics\n",
    "\n",
    "        Returns:\n",
    "            List of conversation dictionaries with metadata\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            if new_only:\n",
    "                # Get conversations without topics\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT ac.id, ac.report_id, ac.analyst_name, ac.analyst_company\n",
    "                    FROM analyst_conversations ac\n",
    "                    WHERE ac.topic_id IS NULL\n",
    "                    ORDER BY ac.id\n",
    "                    \"\"\",\n",
    "                )\n",
    "            else:\n",
    "                # Get all conversations\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT ac.id, ac.report_id, ac.analyst_name, ac.analyst_company\n",
    "                    FROM analyst_conversations ac\n",
    "                    ORDER BY ac.id\n",
    "                    \"\"\",\n",
    "                )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            conversations = self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "            if limit and len(conversations) > limit:\n",
    "                conversations = conversations[:limit]\n",
    "\n",
    "            return conversations\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting conversations to analyze: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_conversation_metadata(self, conversation_id: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get metadata for a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with conversation metadata\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT ac.analyst_name, ac.analyst_company,\n",
    "                       r.year, r.quarter, b.bank_name\n",
    "                FROM analyst_conversations ac\n",
    "                JOIN reports r ON ac.report_id = r.id\n",
    "                JOIN banks b ON r.bank_id = b.id\n",
    "                WHERE ac.id = ?\n",
    "                \"\"\",\n",
    "                (conversation_id,),\n",
    "            )\n",
    "\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0]\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting conversation metadata: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def update_conversation_topic(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        topic_id: int,\n",
    "        topic_probability: float,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Update the topic for a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            topic_id: ID of the topic\n",
    "            topic_probability: Probability of the topic\n",
    "\n",
    "        Returns:\n",
    "            Success flag\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                UPDATE analyst_conversations\n",
    "                SET topic_id = ?, topic_probability = ?\n",
    "                WHERE id = ?\n",
    "                \"\"\",\n",
    "                (topic_id, topic_probability, conversation_id),\n",
    "            )\n",
    "\n",
    "            conn.commit()\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error updating conversation topic: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_qa_pairs(self, conversation_id: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all QA pairs for a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "\n",
    "        Returns:\n",
    "            List of QA pair dictionaries\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, question, answer, answer_speaker, answer_role\n",
    "                FROM qa_pairs\n",
    "                WHERE conversation_id = ?\n",
    "                \"\"\",\n",
    "                (conversation_id,),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting QA pairs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_unprocessed_qa_pairs(self, limit: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get QA pairs that haven't been processed yet\n",
    "\n",
    "        Finds QA pairs that don't have any corresponding entries in the\n",
    "        preprocessed_text table.\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of QA pairs to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with QA pair information\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Find QA pairs without any preprocessing\n",
    "            query = \"\"\"\n",
    "            SELECT qp.id, qp.question, qp.answer\n",
    "            FROM qa_pairs qp\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM preprocessed_text pt\n",
    "                WHERE pt.qa_pair_id = qp.id\n",
    "            )\n",
    "            \"\"\"\n",
    "\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving unprocessed QA pairs: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def add_credit_risk_mention(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        keyword_id: str,\n",
    "        mention_count: int,\n",
    "        mention_contexts: List[str],\n",
    "        positive_score: float = 0.33,\n",
    "        negative_score: float = 0.33,\n",
    "        neutral_score: float = 0.34,\n",
    "        compound_score: float = 0.0,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Add a credit risk mention\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            keyword_id: ID or name of the keyword\n",
    "            mention_count: Number of mentions\n",
    "            mention_contexts: List of context excerpts\n",
    "            positive_score: Positive sentiment score\n",
    "            negative_score: Negative sentiment score\n",
    "            neutral_score: Neutral sentiment score\n",
    "            compound_score: Compound sentiment score\n",
    "\n",
    "        Returns:\n",
    "            Success flag\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO credit_risk_mentions\n",
    "                (conversation_id, keyword_id, mention_count, mention_context,\n",
    "                 positive_score, negative_score, neutral_score, compound_score)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (conversation_id, keyword_id)\n",
    "                DO UPDATE SET\n",
    "                    mention_count = excluded.mention_count,\n",
    "                    mention_context = excluded.mention_context,\n",
    "                    positive_score = excluded.positive_score,\n",
    "                    negative_score = excluded.negative_score,\n",
    "                    neutral_score = excluded.neutral_score,\n",
    "                    compound_score = excluded.compound_score\n",
    "                \"\"\",\n",
    "                (\n",
    "                    conversation_id,\n",
    "                    keyword_id,\n",
    "                    mention_count,\n",
    "                    json.dumps(mention_contexts),\n",
    "                    positive_score,\n",
    "                    negative_score,\n",
    "                    neutral_score,\n",
    "                    compound_score,\n",
    "                ),\n",
    "            )\n",
    "            conn.commit()\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding credit risk mention: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def add_credit_risk_keyword(\n",
    "        self,\n",
    "        keyword: str,\n",
    "        metric_category: str,\n",
    "        relevance_score: float = 1.0,\n",
    "    ) -> int:\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"INSERT OR IGNORE INTO credit_risk_keywords (keyword, metric_category, relevance_score) VALUES (?, ?, ?)\",\n",
    "                (keyword, metric_category, relevance_score),\n",
    "            )\n",
    "            conn.commit()\n",
    "            return cursor.lastrowid or self.get_credit_risk_keyword_id(keyword)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding credit risk keyword: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_credit_risk_keyword_id(self, keyword: str) -> int:\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"SELECT id FROM credit_risk_keywords WHERE keyword = ?\",\n",
    "                (keyword,),\n",
    "            )\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0][\"id\"]\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_all_conversations(self) -> List[Dict]:\n",
    "        \"\"\"Get all analyst conversation IDs from the database\"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, report_id, analyst_name, analyst_company\n",
    "                FROM analyst_conversations\n",
    "                ORDER BY id\n",
    "                \"\"\",\n",
    "            )\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving conversations: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_conversation_text(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        level: str = \"lemmatized\",\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get combined preprocessed text for all QA pairs in a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            level: Preprocessing level to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Combined preprocessed text for the conversation\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT pt.processed_text\n",
    "                FROM preprocessed_text pt\n",
    "                JOIN qa_pairs qp ON pt.qa_pair_id = qp.id\n",
    "                WHERE qp.conversation_id = ?\n",
    "                AND pt.text_type = 'combined'\n",
    "                AND pt.preprocessing_level = ?\n",
    "                \"\"\",\n",
    "                (conversation_id, level),\n",
    "            )\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if results:\n",
    "                # Combine all processed texts\n",
    "                return \" \".join([result[0] for result in results])\n",
    "            self.logger.warning(\n",
    "                f\"No preprocessed text found for conversation {conversation_id}\",\n",
    "            )\n",
    "            return \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving conversation text: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_speaker_text(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        speaker_name: str,\n",
    "        speaker_role: str,\n",
    "        level: str = \"lemmatized\",\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get all text from a specific speaker in a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            speaker_name: Name of the speaker\n",
    "            speaker_role: Role of the speaker (e.g., \"analyst\", \"executive\")\n",
    "            level: Preprocessing level to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Combined preprocessed text for the speaker\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT pt.processed_text\n",
    "                FROM preprocessed_text pt\n",
    "                JOIN qa_pairs qp ON pt.qa_pair_id = qp.id\n",
    "                WHERE qp.conversation_id = ?\n",
    "                AND qp.answer_speaker = ?\n",
    "                AND qp.answer_role = ?\n",
    "                AND pt.text_type = 'answer'\n",
    "                AND pt.preprocessing_level = ?\n",
    "                \"\"\",\n",
    "                (conversation_id, speaker_name, speaker_role, level),\n",
    "            )\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if results:\n",
    "                return \" \".join([result[0] for result in results])\n",
    "            self.logger.warning(\n",
    "                f\"No text found for speaker {speaker_name} in conversation {conversation_id}\",\n",
    "            )\n",
    "            return \"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving speaker text: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_analyst_questions(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        analyst_name: str,\n",
    "        level: str = \"lemmatized\",\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Get all questions from a specific analyst in a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            analyst_name: Name of the analyst\n",
    "            level: Preprocessing level to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Combined preprocessed questions from the analyst\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # First get all qa_pairs from this analyst\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT qp.id\n",
    "                FROM qa_pairs qp\n",
    "                JOIN analyst_conversations ac ON qp.conversation_id = ac.id\n",
    "                WHERE qp.conversation_id = ?\n",
    "                AND ac.analyst_name = ?\n",
    "                \"\"\",\n",
    "                (conversation_id, analyst_name),\n",
    "            )\n",
    "            qa_pair_ids = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "            if not qa_pair_ids:\n",
    "                self.logger.warning(\n",
    "                    f\"No questions found for analyst {analyst_name} in conversation {conversation_id}\",\n",
    "                )\n",
    "                return \"\"\n",
    "\n",
    "            # Then get the preprocessed text for these qa_pairs\n",
    "            question_texts = []\n",
    "            for qa_pair_id in qa_pair_ids:\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT processed_text\n",
    "                    FROM preprocessed_text\n",
    "                    WHERE qa_pair_id = ?\n",
    "                    AND text_type = 'question'\n",
    "                    AND preprocessing_level = ?\n",
    "                    \"\"\",\n",
    "                    (qa_pair_id, level),\n",
    "                )\n",
    "                result = cursor.fetchone()\n",
    "                if result:\n",
    "                    question_texts.append(result[0])\n",
    "\n",
    "            return \" \".join(question_texts)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving analyst questions: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_all_conversation_speakers(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Get all unique speakers in a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with speaker names and roles\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT DISTINCT answer_speaker, answer_role\n",
    "                FROM qa_pairs\n",
    "                WHERE conversation_id = ?\n",
    "                \"\"\",\n",
    "                (conversation_id,),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            results = self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "            # Convert to the expected format\n",
    "            speakers = [\n",
    "                {\"name\": row[\"answer_speaker\"], \"role\": row[\"answer_role\"]}\n",
    "                for row in results\n",
    "            ]\n",
    "\n",
    "            return speakers\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving conversation speakers: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_bank_conversations(\n",
    "        self,\n",
    "        bank_name: str,\n",
    "        year: int,\n",
    "        quarter: str,\n",
    "    ) -> List[int]:\n",
    "        \"\"\"Get conversation IDs for a bank's quarterly report\"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT ac.id\n",
    "                FROM analyst_conversations ac\n",
    "                JOIN reports r ON ac.report_id = r.id\n",
    "                JOIN banks b ON r.bank_id = b.id\n",
    "                WHERE b.bank_name = ? AND r.year = ? AND r.quarter = ?\n",
    "                \"\"\",\n",
    "                (bank_name, year, quarter),\n",
    "            )\n",
    "            return [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting bank conversations: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Sentiment Analysis Methods\n",
    "    def add_conversation_sentiment(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        positive_score: float,\n",
    "        negative_score: float,\n",
    "        neutral_score: float,\n",
    "        compound_score: float,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add sentiment analysis for a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            positive_score: Positive sentiment score\n",
    "            negative_score: Negative sentiment score\n",
    "            neutral_score: Neutral sentiment score\n",
    "            compound_score: Compound sentiment score\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created sentiment entry\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT OR REPLACE INTO conversation_sentiment\n",
    "                (conversation_id, positive_score, negative_score, neutral_score, compound_score)\n",
    "                VALUES (?, ?, ?, ?, ?)\n",
    "                \"\"\",\n",
    "                (\n",
    "                    conversation_id,\n",
    "                    positive_score,\n",
    "                    negative_score,\n",
    "                    neutral_score,\n",
    "                    compound_score,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            sentiment_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return sentiment_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding conversation sentiment: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_conversation_sentiment(self, conversation_id: int) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Get sentiment analysis for a specific conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with sentiment information or None if not found\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT positive_score, negative_score, neutral_score, compound_score\n",
    "                FROM conversation_sentiment\n",
    "                WHERE conversation_id = ?\n",
    "                \"\"\",\n",
    "                (conversation_id,),\n",
    "            )\n",
    "\n",
    "            result = cursor.fetchone()\n",
    "            if result:\n",
    "                return self.cursor_to_dict(cursor, [result])[0]\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving conversation sentiment: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def add_speaker_sentiment(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        speaker_name: str,\n",
    "        speaker_role: str,\n",
    "        positive_score: float,\n",
    "        negative_score: float,\n",
    "        neutral_score: float,\n",
    "        compound_score: float,\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Add sentiment analysis for a speaker\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            speaker_name: Name of the speaker\n",
    "            speaker_role: Role of the speaker\n",
    "            positive_score: Positive sentiment score\n",
    "            negative_score: Negative sentiment score\n",
    "            neutral_score: Neutral sentiment score\n",
    "            compound_score: Compound sentiment score\n",
    "\n",
    "        Returns:\n",
    "            ID of the newly created sentiment entry\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO speaker_topic_sentiment\n",
    "                (conversation_id, speaker_name, speaker_role, positive_score, negative_score, neutral_score, compound_score)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ON CONFLICT (conversation_id, speaker_name, speaker_role)\n",
    "                DO UPDATE SET\n",
    "                    positive_score = excluded.positive_score,\n",
    "                    negative_score = excluded.negative_score,\n",
    "                    neutral_score = excluded.neutral_score,\n",
    "                    compound_score = excluded.compound_score\n",
    "                \"\"\",\n",
    "                (\n",
    "                    conversation_id,\n",
    "                    speaker_name,\n",
    "                    speaker_role,\n",
    "                    positive_score,\n",
    "                    negative_score,\n",
    "                    neutral_score,\n",
    "                    compound_score,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            sentiment_id = cursor.lastrowid\n",
    "            conn.commit()\n",
    "            return sentiment_id\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding speaker sentiment: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_credit_risk_mentions(self, conversation_ids: List[int]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get credit risk mentions for multiple conversations\n",
    "\n",
    "        Args:\n",
    "            conversation_ids: List of conversation IDs\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with mention information\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Convert list to string of comma-separated IDs\n",
    "            placeholders = \",\".join(\"?\" * len(conversation_ids))\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT id, keyword_id, mention_count, mention_context, positive_score, negative_score, neutral_score, compound_score\n",
    "                FROM credit_risk_mentions\n",
    "                WHERE conversation_id IN ({placeholders})\n",
    "            \"\"\"\n",
    "\n",
    "            cursor.execute(query, conversation_ids)\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            return self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving credit risk mentions: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def update_credit_risk_mention_sentiment(\n",
    "        self,\n",
    "        mention_id: int,\n",
    "        positive_score: float,\n",
    "        negative_score: float,\n",
    "        neutral_score: float,\n",
    "        compound_score: float,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Update sentiment scores for a credit risk mention\n",
    "\n",
    "        Args:\n",
    "            mention_id: ID of the mention\n",
    "            positive_score: Positive sentiment score\n",
    "            negative_score: Negative sentiment score\n",
    "            neutral_score: Neutral sentiment score\n",
    "            compound_score: Compound sentiment score\n",
    "\n",
    "        Returns:\n",
    "            Success flag\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                UPDATE credit_risk_mentions\n",
    "                SET positive_score = ?, negative_score = ?, neutral_score = ?, compound_score = ?\n",
    "                WHERE id = ?\n",
    "                \"\"\",\n",
    "                (\n",
    "                    positive_score,\n",
    "                    negative_score,\n",
    "                    neutral_score,\n",
    "                    compound_score,\n",
    "                    mention_id,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            conn.commit()\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error updating credit risk mention sentiment: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_conversations_for_sentiment_analysis(\n",
    "        self,\n",
    "        limit: Optional[int] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get conversations that need sentiment analysis\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations\n",
    "\n",
    "        Returns:\n",
    "            List of conversation dictionaries with metadata\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Get conversations that need sentiment analysis\n",
    "            query = \"\"\"\n",
    "            SELECT DISTINCT ac.id as conversation_id,\n",
    "                ac.analyst_name, ac.analyst_company,\n",
    "                r.year, r.quarter, b.bank_name\n",
    "            FROM analyst_conversations ac\n",
    "            JOIN reports r ON ac.report_id = r.id\n",
    "            JOIN banks b ON r.bank_id = b.id\n",
    "            LEFT JOIN conversation_sentiment cs ON ac.id = cs.conversation_id\n",
    "            \"\"\"\n",
    "\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            self.logger.info(f\"Query returned {len(rows)} rows\")\n",
    "            results = self.cursor_to_dict(cursor, rows)\n",
    "            self.logger.info(f\"Converted to {len(results)} dictionaries\")\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error getting conversations for sentiment analysis: {str(e)}\",\n",
    "            )\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_qa_answer(self, qa_pair_id: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Get answer text for a QA pair\n",
    "\n",
    "        Args:\n",
    "            qa_pair_id: ID of the QA pair\n",
    "\n",
    "        Returns:\n",
    "            Answer text or None if not found\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\"SELECT answer FROM qa_pairs WHERE id = ?\", (qa_pair_id,))\n",
    "            row = cursor.fetchone()\n",
    "            if row:\n",
    "                return self.cursor_to_dict(cursor, [row])[0][\"answer\"]\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting QA answer: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_vector_embeddings(\n",
    "        self,\n",
    "        embedding_model: str,\n",
    "        preprocessing_level: str,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all vector embeddings for a specific model and preprocessing level\n",
    "\n",
    "        Args:\n",
    "            embedding_model: Name of the embedding model\n",
    "            preprocessing_level: Preprocessing level\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with embedding information\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT\n",
    "                    ve.id, ve.conversation_id, ve.vector_id, ve.vector_data,\n",
    "                    ve.chunk_text, ve.metadata\n",
    "                FROM vector_embeddings ve\n",
    "                WHERE ve.embedding_model = ? AND ve.preprocessing_level = ?\n",
    "                \"\"\",\n",
    "                (embedding_model, preprocessing_level),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            results = self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "            # Parse JSON metadata\n",
    "            for data in results:\n",
    "                if data[\"metadata\"]:\n",
    "                    data[\"metadata\"] = json.loads(data[\"metadata\"])\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving vector embeddings: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    # Topic Methods\n",
    "    def add_or_update_topic(\n",
    "        self,\n",
    "        topic_id: int,\n",
    "        name: str,\n",
    "        keywords: List[str],\n",
    "        category: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Add or update a topic\n",
    "\n",
    "        Args:\n",
    "            topic_id: ID of the topic\n",
    "            name: Name of the topic\n",
    "            keywords: List of keywords for the topic\n",
    "            category: Category of the topic\n",
    "\n",
    "        Returns:\n",
    "            Success flag\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Check if topic exists\n",
    "            cursor.execute(\"SELECT id FROM topics WHERE id = ?\", (topic_id,))\n",
    "            if cursor.fetchone():\n",
    "                # Update existing topic\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    UPDATE topics\n",
    "                    SET name = ?, keywords = ?, category = ?\n",
    "                    WHERE id = ?\n",
    "                    \"\"\",\n",
    "                    (name, json.dumps(keywords), category, topic_id),\n",
    "                )\n",
    "            else:\n",
    "                # Create new topic\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO topics (id, name, keywords, category)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                    \"\"\",\n",
    "                    (topic_id, name, json.dumps(keywords), category),\n",
    "                )\n",
    "\n",
    "            conn.commit()\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error adding or updating topic: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def get_topics_by_category(self, category: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get all topics for a specific category\n",
    "\n",
    "        Args:\n",
    "            category: Category of topics to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries with topic information\n",
    "        \"\"\"\n",
    "        conn = self.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, name, keywords, category\n",
    "                FROM topics\n",
    "                WHERE category = ?\n",
    "                ORDER BY name\n",
    "                \"\"\",\n",
    "                (category,),\n",
    "            )\n",
    "\n",
    "            rows = cursor.fetchall()\n",
    "            result = self.cursor_to_dict(cursor, rows)\n",
    "\n",
    "            # Parse JSON keywords\n",
    "            for topic in result:\n",
    "                topic[\"keywords\"] = json.loads(topic[\"keywords\"])\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving topics: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def upload_to_gdrive(self, file_path: str, folder_id=None):\n",
    "        \"\"\"\n",
    "        Upload a file to Google Drive\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the local file\n",
    "            folder_id: Optional Google Drive folder ID to upload to\n",
    "\n",
    "        Returns:\n",
    "            file_id: ID of the uploaded file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create timestamp for versioning\n",
    "            base_path = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "            versioned_filename = f\"{base_path}_processed_{timestamp}.db\"\n",
    "\n",
    "            # File metadata\n",
    "            file_metadata = {\n",
    "                \"title\": versioned_filename,\n",
    "                \"mimeType\": \"application/x-sqlite3\",  # for SQLite DB files\n",
    "            }\n",
    "\n",
    "            # If folder_id is specified, add it to metadata\n",
    "            if folder_id:\n",
    "                file_metadata[\"parents\"] = [{\"id\": folder_id}]\n",
    "\n",
    "            # Create file instance\n",
    "            file = drive.CreateFile(file_metadata)\n",
    "\n",
    "            # Set content (using the original file path)\n",
    "            file.SetContentFile(file_path)\n",
    "\n",
    "            # Upload\n",
    "            file.Upload()\n",
    "\n",
    "            print(\n",
    "                f\"Successfully uploaded {file_path} to Google Drive as {versioned_filename}\",\n",
    "            )\n",
    "            print(f\"File ID: {file['id']}\")\n",
    "\n",
    "            return file[\"id\"]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading file: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Initialize database\n",
    "db = GSIBDatabase(DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7Ee2yoTL4U8"
   },
   "source": [
    "## üéôÔ∏è Earnings Call Transcript Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTOC9WolFoHZ"
   },
   "outputs": [],
   "source": [
    "class ClaudeTranscriptProcessor:\n",
    "    \"\"\"\n",
    "    Transcript processor using Claude API for enhanced text analysis.\n",
    "    Integrates with GSIBDatabase for persistent storage.\n",
    "    Supports both direct message and batch processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db, api_key=None, model=\"claude-3-sonnet-20240229\"):\n",
    "        \"\"\"Initialize the Claude Transcript Processor\"\"\"\n",
    "        self.logger = logging.getLogger(\"ClaudeTranscriptProcessor\")\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.db = db\n",
    "\n",
    "        # Initialize Claude client\n",
    "        self.api_key = api_key or os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Claude API key must be provided or set as ANTHROPIC_API_KEY environment variable\"\n",
    "            )\n",
    "        self.claude = self._get_anthropic_client()\n",
    "\n",
    "        # Set model\n",
    "        self.model = model\n",
    "\n",
    "        # Constants\n",
    "        self.unidentified_analyst_name = \"Unidentified Analyst\"\n",
    "        self.unknown_firm = \"Unknown firm\"\n",
    "        self.host_operator_text = \"Operator\"\n",
    "\n",
    "    def _get_anthropic_client(self) -> anthropic.Anthropic:\n",
    "        \"\"\"Get Anthropic client with API key\"\"\"\n",
    "        return anthropic.Anthropic(api_key=self.api_key)\n",
    "\n",
    "    def _read_pdf(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Read and extract text from PDF file.\"\"\"\n",
    "        self.logger.info(f\"Reading PDF from {file_path}\")\n",
    "\n",
    "        try:\n",
    "            # Open the PDF file\n",
    "            doc = fitz.open(file_path)\n",
    "            text_lines = []\n",
    "\n",
    "            for page in doc:\n",
    "                # Extract text with layout preservation\n",
    "                rect = page.rect\n",
    "                text = page.get_text(\"text\", flags=4)  # preserve layout\n",
    "\n",
    "                # Split into lines, strip whitespace, and filter empty lines\n",
    "                page_lines = [t.strip() for t in text.split(\"\\n\") if t.strip()]\n",
    "\n",
    "                # Skip footer lines and known patterns\n",
    "                footer_pattern = re.compile(\n",
    "                    r\"(?:Page \\d+ of \\d+$|https://seekingalpha\\.com/article/\\d+(?:[-\\w]+)*)\"\n",
    "                )\n",
    "                page_lines = [\n",
    "                    line for line in page_lines if not footer_pattern.search(line)\n",
    "                ]\n",
    "\n",
    "                text_lines.extend(page_lines)\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Extracted {len(text_lines)} clean lines from {file_path}\"\n",
    "            )\n",
    "            return text_lines\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_chunks(\n",
    "        self, text: str, chunk_size: int = 3000, overlap: int = 250\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Create simple overlapping chunks from text.\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        text_length = len(text)\n",
    "\n",
    "        while start < text_length:\n",
    "            # Calculate end position for current chunk\n",
    "            end = min(start + chunk_size, text_length)\n",
    "\n",
    "            # Include overlap unless at the end\n",
    "            if end < text_length:\n",
    "                end = min(end + overlap, text_length)\n",
    "\n",
    "            # Extract chunk\n",
    "            chunk = text[start:end].strip()\n",
    "            if chunk:  # Only add non-empty chunks\n",
    "                chunks.append(chunk)\n",
    "\n",
    "            # Move start position, accounting for overlap\n",
    "            start = end - overlap if end < text_length else text_length\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _get_metadata_extraction_prompt(self) -> List[Dict]:\n",
    "        \"\"\"System prompt for metadata extraction with strict JSON formatting\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"You are a JSON extraction tool. Your only job is to extract metadata as JSON.\n",
    "\n",
    "    Extract all participant information from this transcript chunk in these categories:\n",
    "    1. Bank name\n",
    "    2. Quarter (Q1, Q2, Q3, Q4) and Year\n",
    "    3. Bank executive participants with their roles\n",
    "    4. Analyst participants with their firms\n",
    "\n",
    "    CRUCIAL INSTRUCTIONS:\n",
    "    - RESPOND WITH NOTHING BUT VALID JSON. No explanations, no descriptions, no \"Based on the transcript\" phrases.\n",
    "    - If information is missing, use empty strings or empty arrays.\n",
    "    - Never include comments or backticks in your response.\n",
    "\n",
    "    FORMAT:\n",
    "    {\n",
    "        \"bank_name\": \"string\",\n",
    "        \"quarter\": \"string\",\n",
    "        \"year\": \"string\",\n",
    "        \"executives\": [\n",
    "            {\"name\": \"string\", \"role\": \"string\"}\n",
    "        ],\n",
    "        \"analysts\": [\n",
    "            {\"name\": \"string\", \"firm\": \"string\"}\n",
    "        ]\n",
    "    }\"\"\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def _get_metadata_processing_prompt(self) -> List[Dict]:\n",
    "        \"\"\"System prompt for consolidating metadata with strict JSON formatting\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"You are a JSON processing tool. Your only purpose is to consolidate JSON metadata.\n",
    "\n",
    "    Process this list of transcript participants using these rules:\n",
    "\n",
    "    1. Name Consolidation:\n",
    "    - For each person, find and use their most complete name from any mention\n",
    "    - Example: If you see \"Pam\" and later \"Pam Kaur\", use \"Pam Kaur\"\n",
    "    - Match variations like \"Ben\" and \"Benjamin\"\n",
    "\n",
    "    2. Role and Firm:\n",
    "    - Use the first role found for a person\n",
    "    - Keep the firm if it appears with role\n",
    "    - For analysts, keep their research firm\n",
    "\n",
    "    3. Categorization:\n",
    "    - Executives: Anyone with a role at the main bank\n",
    "    - Analysts: People from external firms\n",
    "\n",
    "    4. Bank and Quarter:\n",
    "    - Determine the most likely bank name, quarter, and year from the data\n",
    "    - For bank names, standardize to these formats when applicable:\n",
    "        - \"JPMorgan Chase\", \"Bank of America\", \"Citigroup\", \"Goldman Sachs\",\n",
    "        - \"Morgan Stanley\", \"HSBC\", \"Barclays\", \"UBS\", \"Santander\"\n",
    "\n",
    "    CRUCIAL INSTRUCTIONS:\n",
    "    - RESPOND WITH NOTHING BUT VALID JSON. No explanations, no descriptions, no \"Here is the consolidated data\" phrases.\n",
    "    - Never include comments, backticks, or markdown in your response.\n",
    "    - The first character of your response must be \"{\" and the last must be \"}\".\n",
    "\n",
    "    FORMAT:\n",
    "    {\n",
    "        \"bank_name\": \"Main bank name\",\n",
    "        \"quarter\": \"Q1/Q2/Q3/Q4\",\n",
    "        \"year\": \"YYYY\",\n",
    "        \"executives\": [\n",
    "            {\"name\": \"Most complete name found\", \"role\": \"Role found\"}\n",
    "        ],\n",
    "        \"analysts\": [\n",
    "            {\"name\": \"Most complete name found\", \"firm\": \"Research firm\"}\n",
    "        ]\n",
    "    }\"\"\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    def _get_qa_analysis_prompt(self, metadata: str) -> List[Dict]:\n",
    "        \"\"\"System prompt for Q&A pair extraction with improved JSON formatting constraints\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"\"\"You are a JSON extraction API that outputs only valid JSON. You must analyze financial transcript chunks and extract structured Q&A pairs.\n",
    "\n",
    "    TRANSCRIPT METADATA:\n",
    "    {metadata}\n",
    "\n",
    "    USE EXACTLY THESE STANDARDIZED BANK NAMES:\n",
    "    - \"JPMorgan Chase\" (not JPMorgan Chase & Co, JP Morgan, or similar variants)\n",
    "    - \"Bank of America\" (not Bank of America Corporation or BofA)\n",
    "    - \"Citigroup\" (not Citibank or Citi)\n",
    "    - \"Goldman Sachs\" (not Goldman Sachs Group)\n",
    "    - \"Morgan Stanley\" (not Morgan Stanley & Co)\n",
    "    - \"HSBC\" (not HSBC Holdings or HSBC Bank)\n",
    "    - \"Barclays\" (not Barclays PLC)\n",
    "    - \"UBS\" (not UBS Group or UBS AG)\n",
    "    - \"Santander\" (not Banco Santander)\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"EXTRACTION INSTRUCTIONS:\n",
    "    1. If the chunk contains financial transcript Q&A pairs, extract them as structured JSON\n",
    "    2. If the chunk contains ONLY prepared remarks, introduction, or no Q&A content, return valid JSON with empty qa_pairs array\n",
    "    3. NEVER include the transcript title, date, or other metadata as text in your output\n",
    "    4. NEVER refer to the \"transcript chunk\" in your response\n",
    "    5. DO NOT describe what you're doing - ONLY OUTPUT VALID JSON\n",
    "\n",
    "    EXTRACTION RULES:\n",
    "    - Extract only COMPLETE Q&A exchanges between analysts and executives\n",
    "    - Identify the analyst name and their company\n",
    "    - Preserve the exact verbatim question text\n",
    "    - Include each executive's name, role, and verbatim answer\n",
    "    - Skip operator introductions and transitions\n",
    "    - Only extract text actually present in the transcript\n",
    "\n",
    "    HANDLING PARTIAL SEGMENTS:\n",
    "    - If a question is cut off mid-sentence, DO NOT include it\n",
    "    - If an answer is cut off mid-sentence, DO NOT include it\n",
    "    - If you find incomplete exchanges, ignore them completely\n",
    "    - NEVER return raw transcript text in the output\n",
    "    - If chunk begins mid-answer or mid-question, skip that partial exchange\n",
    "    - DO NOT try to complete partial text with your own words\n",
    "    - If no complete Q&A pairs can be found, return JSON with empty qa_pairs array\n",
    "\n",
    "    CRITICAL FORMAT RULES:\n",
    "    - First character must be \"{\" and last character must be \"}\"\n",
    "    - No text before or after the JSON object\n",
    "    - No explanation, even if you find no Q&A pairs\n",
    "    - No backticks, no markdown formatting\n",
    "    - If the transcript chunk contains no Q&A, still output valid JSON with empty qa_pairs array\"\"\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"OUTPUT FORMAT (exactly as shown):\n",
    "    {\n",
    "        \"metadata\": {\n",
    "            \"year\": \"YYYY\",\n",
    "            \"quarter\": \"QX\",\n",
    "            \"company\": \"STANDARDIZED BANK NAME FROM THE LIST ABOVE\"\n",
    "        },\n",
    "        \"qa_pairs\": [\n",
    "            {\n",
    "                \"qa_pair\": {\n",
    "                    \"analyst_name\": \"Full analyst name\",\n",
    "                    \"analyst_company\": \"Analyst firm\",\n",
    "                    \"questions\": [\"Full verbatim question text including all parts\"],\n",
    "                    \"answers\": [\n",
    "                        {\n",
    "                            \"speaker_name\": \"Executive name\",\n",
    "                            \"speaker_role\": \"Executive role\",\n",
    "                            \"answer_text\": \"Full verbatim answer text\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    If no complete Q&A pairs can be extracted, return:\n",
    "    {\n",
    "        \"metadata\": {\n",
    "            \"year\": \"YYYY\",\n",
    "            \"quarter\": \"QX\",\n",
    "            \"company\": \"STANDARDIZED BANK NAME FROM THE LIST ABOVE\"\n",
    "        },\n",
    "        \"qa_pairs\": []\n",
    "    }\"\"\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    async def extract_metadata_direct(\n",
    "        self, transcript: str, chunk_size: int = 3000, overlap: int = 200\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract metadata using direct Claude API calls\n",
    "\n",
    "        Args:\n",
    "            transcript: Full transcript text\n",
    "            chunk_size: Size of each chunk to process\n",
    "            overlap: Overlap between chunks\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of consolidated metadata\n",
    "        \"\"\"\n",
    "        # Create overlapping chunks for processing\n",
    "        chunks = self.create_chunks(transcript, chunk_size=chunk_size, overlap=overlap)\n",
    "        self.logger.info(\n",
    "            f\"Processing {len(chunks)} chunks for metadata extraction (direct method)\"\n",
    "        )\n",
    "\n",
    "        # Process each chunk directly\n",
    "        all_chunk_results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                # Create direct message request\n",
    "                response = await asyncio.to_thread(\n",
    "                    self.claude.messages.create,\n",
    "                    model=self.model,\n",
    "                    max_tokens=2048,\n",
    "                    system=self._get_metadata_extraction_prompt(),\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Extract participant info from this chunk:\\n\\n{chunk}\",\n",
    "                        },\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                chunk_result = response.content[0].text.strip()\n",
    "                try:\n",
    "                    # Validate it's proper JSON\n",
    "                    json_result = json.loads(chunk_result)\n",
    "                    all_chunk_results.append(chunk_result)\n",
    "                    self.logger.info(\n",
    "                        f\"Successfully processed metadata chunk {i + 1}/{len(chunks)}\"\n",
    "                    )\n",
    "                except json.JSONDecodeError:\n",
    "                    self.logger.warning(f\"Received non-JSON response from chunk {i}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing metadata chunk {i}: {e}\")\n",
    "\n",
    "        # Combine all chunk results into a single JSON\n",
    "        if all_chunk_results:\n",
    "            chunk_results_json = (\n",
    "                '{\"metadata_chunks\": [' + \",\".join(all_chunk_results) + \"]}\"\n",
    "            )\n",
    "\n",
    "            # Process and consolidate the metadata\n",
    "            return await self._process_metadata_direct(chunk_results_json)\n",
    "        return {}\n",
    "\n",
    "    async def _process_metadata_direct(self, raw_metadata: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process the collected raw metadata to get final consolidated metadata (direct method)\n",
    "\n",
    "        Args:\n",
    "            raw_metadata: JSON string of collected metadata chunks\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of consolidated metadata\n",
    "        \"\"\"\n",
    "        if not raw_metadata.strip():\n",
    "            self.logger.warning(\"No metadata to process\")\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            # Process metadata with Claude directly\n",
    "            response = await asyncio.to_thread(\n",
    "                self.claude.messages.create,\n",
    "                model=self.model,\n",
    "                max_tokens=2048,\n",
    "                system=self._get_metadata_processing_prompt(),\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Process this JSON of transcript participants:\\n\\n{raw_metadata}\",\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            result = response.content[0].text\n",
    "            try:\n",
    "                # Parse the response as JSON\n",
    "                metadata = json.loads(result)\n",
    "                self.logger.info(\n",
    "                    f\"Processed metadata for {metadata.get('bank_name', 'Unknown Bank')}\"\n",
    "                )\n",
    "                return metadata\n",
    "            except json.JSONDecodeError:\n",
    "                self.logger.error(\n",
    "                    \"Failed to parse metadata JSON from Claude's response\"\n",
    "                )\n",
    "                return {}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing metadata: {e}\", exc_info=True)\n",
    "            return {}\n",
    "\n",
    "    async def process_qa_direct(\n",
    "        self,\n",
    "        transcript: str,\n",
    "        metadata: Dict,\n",
    "        chunk_size: int = 4000,\n",
    "        overlap: int = 300,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process transcript to extract structured Q&A pairs using direct Claude API calls\n",
    "\n",
    "        Args:\n",
    "            transcript: Full transcript text\n",
    "            metadata: Processed metadata dictionary\n",
    "            chunk_size: Size of each chunk to process\n",
    "            overlap: Overlap between chunks\n",
    "\n",
    "        Returns:\n",
    "            Dict with metadata and Q&A pairs\n",
    "        \"\"\"\n",
    "        # Create JSON string from metadata for Claude's context\n",
    "        metadata_json = json.dumps(metadata, indent=2)\n",
    "\n",
    "        # Create overlapping chunks for processing\n",
    "        chunks = self.create_chunks(transcript, chunk_size=chunk_size, overlap=overlap)\n",
    "        self.logger.info(\n",
    "            f\"Processing {len(chunks)} chunks for Q&A extraction (direct method)\"\n",
    "        )\n",
    "\n",
    "        # Process each chunk directly\n",
    "        chunk_results = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            try:\n",
    "                # Create direct message request\n",
    "                response = await asyncio.to_thread(\n",
    "                    self.claude.messages.create,\n",
    "                    model=self.model,\n",
    "                    max_tokens=4096,  # Larger tokens for detailed Q&A extraction\n",
    "                    system=self._get_qa_analysis_prompt(metadata_json),\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"Extract all Q&A pairs from this transcript chunk. DO NOT RETURN THE RAW TEXT. ONLY RETURN JSON:\\n\\n{chunk}\",\n",
    "                        },\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                # Parse the JSON response\n",
    "                content = response.content[0].text\n",
    "                try:\n",
    "                    parsed_result = json.loads(content)\n",
    "                    chunk_results.append(parsed_result)\n",
    "                    self.logger.info(\n",
    "                        f\"Successfully processed Q&A chunk {i + 1}/{len(chunks)}\"\n",
    "                    )\n",
    "                except json.JSONDecodeError as e:\n",
    "                    self.logger.error(\n",
    "                        f\"Failed to parse JSON for Q&A chunk {i + 1}: {e}\"\n",
    "                    )\n",
    "                    self.logger.debug(f\"First 200 chars of response: {content[:200]}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing Q&A chunk {i}: {e}\")\n",
    "\n",
    "        # Merge results from all chunks\n",
    "        return self._merge_qa_results(chunk_results)\n",
    "\n",
    "    async def extract_transcript_metadata(\n",
    "        self,\n",
    "        transcript: str,\n",
    "        chunk_size: int = 3000,\n",
    "        overlap: int = 200,\n",
    "        use_batch: bool = False,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract metadata using either batch or direct processing with Claude API\n",
    "\n",
    "        Args:\n",
    "            transcript: Full transcript text\n",
    "            chunk_size: Size of each chunk to process\n",
    "            overlap: Overlap between chunks\n",
    "            use_batch: Whether to use batch processing (default: False)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of consolidated metadata\n",
    "        \"\"\"\n",
    "        if use_batch:\n",
    "            # Create overlapping chunks for processing\n",
    "            chunks = self.create_chunks(\n",
    "                transcript, chunk_size=chunk_size, overlap=overlap\n",
    "            )\n",
    "            self.logger.info(f\"Processing {len(chunks)} chunks for metadata extraction\")\n",
    "\n",
    "            # Create batch requests for Claude API\n",
    "            requests = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                request = Request(\n",
    "                    custom_id=f\"chunk_{i}\",\n",
    "                    params=MessageCreateParamsNonStreaming(\n",
    "                        model=self.model,\n",
    "                        max_tokens=2048,\n",
    "                        system=self._get_metadata_extraction_prompt(),\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": f\"Extract participant info from this chunk:\\n\\n{chunk}\",\n",
    "                            },\n",
    "                        ],\n",
    "                    ),\n",
    "                )\n",
    "                requests.append(request)\n",
    "\n",
    "            # Process batch requests\n",
    "            try:\n",
    "                message_batch = self.claude.messages.batches.create(requests=requests)\n",
    "                batch_id = message_batch.id\n",
    "                self.logger.info(\n",
    "                    f\"Created batch {batch_id} with {len(chunks)} requests\"\n",
    "                )\n",
    "\n",
    "                # Monitor batch processing until complete\n",
    "                while True:\n",
    "                    batch_status = self.claude.messages.batches.retrieve(batch_id)\n",
    "                    counts = batch_status.request_counts\n",
    "                    self.logger.info(\n",
    "                        f\"Batch status: Processing={counts.processing}, \"\n",
    "                        f\"Succeeded={counts.succeeded}, Errored={counts.errored}\",\n",
    "                    )\n",
    "\n",
    "                    if batch_status.processing_status == \"ended\":\n",
    "                        break\n",
    "                    time.sleep(5)  # Check status every 5 seconds\n",
    "\n",
    "                # Collect results from all chunks\n",
    "                all_chunk_results = []\n",
    "                for result in self.claude.messages.batches.results(batch_id):\n",
    "                    if result.result.type == \"succeeded\":\n",
    "                        # Parse response JSON\n",
    "                        chunk_result = result.result.message.content[0].text.strip()\n",
    "                        try:\n",
    "                            # Validate it's proper JSON\n",
    "                            json_result = json.loads(chunk_result)\n",
    "                            all_chunk_results.append(chunk_result)\n",
    "                        except json.JSONDecodeError:\n",
    "                            self.logger.warning(\n",
    "                                f\"Received non-JSON response from chunk {result.custom_id}\"\n",
    "                            )\n",
    "                    else:\n",
    "                        self.logger.error(\n",
    "                            f\"Error in chunk {result.custom_id}: {result.result.error}\"\n",
    "                        )\n",
    "\n",
    "                # Combine all chunk results into a single JSON\n",
    "                chunk_results_json = (\n",
    "                    '{\"metadata_chunks\": [' + \",\".join(all_chunk_results) + \"]}\"\n",
    "                )\n",
    "\n",
    "                # Process and consolidate the metadata\n",
    "                return self._process_metadata_raw(chunk_results_json)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in batch processing: {e}\", exc_info=True)\n",
    "                return {}\n",
    "        else:\n",
    "            # Use direct method\n",
    "            return await self.extract_metadata_direct(transcript, chunk_size, overlap)\n",
    "\n",
    "    def _process_metadata_raw(self, raw_metadata: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process the collected raw metadata to get final consolidated metadata\n",
    "\n",
    "        Args:\n",
    "            raw_metadata: JSON string of collected metadata chunks\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of consolidated metadata\n",
    "        \"\"\"\n",
    "        if not raw_metadata.strip():\n",
    "            self.logger.warning(\"No metadata to process\")\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            # Process metadata with Claude to consolidate and standardize\n",
    "            response = self.claude.messages.create(\n",
    "                model=self.model,\n",
    "                max_tokens=4096,\n",
    "                system=self._get_metadata_processing_prompt(),\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Process this JSON of transcript participants:\\n\\n{raw_metadata}\",\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            result = response.content[0].text\n",
    "            try:\n",
    "                # Parse the response as JSON\n",
    "                metadata = json.loads(result)\n",
    "                self.logger.info(\n",
    "                    f\"Processed metadata for {metadata.get('bank_name', 'Unknown Bank')}\"\n",
    "                )\n",
    "                return metadata\n",
    "            except json.JSONDecodeError:\n",
    "                self.logger.error(\n",
    "                    \"Failed to parse metadata JSON from Claude's response\"\n",
    "                )\n",
    "                return {}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing metadata: {e}\", exc_info=True)\n",
    "            return {}\n",
    "\n",
    "    async def process_transcript_qa(\n",
    "        self,\n",
    "        transcript: str,\n",
    "        metadata: Dict,\n",
    "        chunk_size: int = 4000,\n",
    "        overlap: int = 300,\n",
    "        use_batch: bool = False,\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process transcript to extract structured Q&A pairs using Claude API\n",
    "\n",
    "        Args:\n",
    "            transcript: Full transcript text\n",
    "            metadata: Processed metadata dictionary\n",
    "            chunk_size: Size of each chunk to process\n",
    "            overlap: Overlap between chunks\n",
    "            use_batch: Whether to use batch processing (default: False)\n",
    "\n",
    "        Returns:\n",
    "            Dict with metadata and Q&A pairs\n",
    "        \"\"\"\n",
    "        if use_batch:\n",
    "            # Create JSON string from metadata for Claude's context\n",
    "            metadata_json = json.dumps(metadata, indent=2)\n",
    "\n",
    "            # Create overlapping chunks for processing\n",
    "            chunks = self.create_chunks(\n",
    "                transcript, chunk_size=chunk_size, overlap=overlap\n",
    "            )\n",
    "            self.logger.info(f\"Processing {len(chunks)} chunks for Q&A extraction\")\n",
    "\n",
    "            # Create batch requests for Claude API\n",
    "            requests = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                request = Request(\n",
    "                    custom_id=f\"chunk_{i}\",\n",
    "                    params=MessageCreateParamsNonStreaming(\n",
    "                        model=self.model,\n",
    "                        max_tokens=4096,  # Larger tokens for detailed Q&A extraction\n",
    "                        system=self._get_qa_analysis_prompt(metadata_json),\n",
    "                        messages=[\n",
    "                            {\n",
    "                                \"role\": \"user\",\n",
    "                                \"content\": f\"Extract all Q&A pairs from this transcript chunk. DO NOT RETURN THE RAW TEXT. ONLY RETURN JSON:\\n\\n{chunk}\",\n",
    "                            },\n",
    "                        ],\n",
    "                    ),\n",
    "                )\n",
    "                requests.append(request)\n",
    "\n",
    "            # Process batch requests\n",
    "            try:\n",
    "                message_batch = self.claude.messages.batches.create(requests=requests)\n",
    "                batch_id = message_batch.id\n",
    "                self.logger.info(\n",
    "                    f\"Created batch {batch_id} with {len(chunks)} requests for Q&A extraction\"\n",
    "                )\n",
    "\n",
    "                # Monitor batch processing until complete\n",
    "                while True:\n",
    "                    batch_status = self.claude.messages.batches.retrieve(batch_id)\n",
    "                    counts = batch_status.request_counts\n",
    "                    self.logger.info(\n",
    "                        f\"Q&A batch status: Processing={counts.processing}, \"\n",
    "                        f\"Succeeded={counts.succeeded}, Errored={counts.errored}\",\n",
    "                    )\n",
    "\n",
    "                    if batch_status.processing_status == \"ended\":\n",
    "                        break\n",
    "                    time.sleep(5)  # Check status every 5 seconds\n",
    "\n",
    "                # Collect and process results\n",
    "                chunk_results = []\n",
    "                for result in self.claude.messages.batches.results(batch_id):\n",
    "                    if result.result.type == \"succeeded\":\n",
    "                        try:\n",
    "                            # Parse the JSON response\n",
    "                            content = result.result.message.content[0].text\n",
    "                            parsed_result = json.loads(content)\n",
    "                            chunk_results.append(parsed_result)\n",
    "                            self.logger.info(\n",
    "                                f\"Successfully processed Q&A chunk {result.custom_id}\"\n",
    "                            )\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            self.logger.error(\n",
    "                                f\"Failed to parse JSON for Q&A chunk {result.custom_id}: {e}\"\n",
    "                            )\n",
    "                    else:\n",
    "                        self.logger.error(\n",
    "                            f\"Error in Q&A chunk {result.custom_id}: {result.result.error}\"\n",
    "                        )\n",
    "\n",
    "                # Merge results from all chunks\n",
    "                return self._merge_qa_results(chunk_results)\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error in Q&A batch processing: {e}\", exc_info=True)\n",
    "                return {\"metadata\": {}, \"qa_pairs\": []}\n",
    "        else:\n",
    "            # Use direct method\n",
    "            return await self.process_qa_direct(\n",
    "                transcript, metadata, chunk_size, overlap\n",
    "            )\n",
    "\n",
    "    def _merge_qa_results(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Merge Q&A results from multiple chunks with deduplication\n",
    "\n",
    "        Args:\n",
    "            results: List of chunk results\n",
    "\n",
    "        Returns:\n",
    "            Merged results dictionary\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return {\"metadata\": {}, \"qa_pairs\": []}\n",
    "\n",
    "        self.logger.info(f\"Merging {len(results)} Q&A batch results\")\n",
    "\n",
    "        # Extract metadata from the most complete chunk\n",
    "        all_metadata = [r.get(\"metadata\", {}) for r in results if r.get(\"metadata\")]\n",
    "        final_metadata = {\"company\": \"Unknown\", \"year\": \"Unknown\", \"quarter\": \"Unknown\"}\n",
    "\n",
    "        # Merge metadata, preferring non-empty values\n",
    "        for key in [\"company\", \"year\", \"quarter\"]:\n",
    "            values = [meta.get(key) for meta in all_metadata if meta.get(key)]\n",
    "            if values:\n",
    "                final_metadata[key] = values[0]\n",
    "\n",
    "        # Combine Q&A pairs with deduplication\n",
    "        all_qa_pairs = []\n",
    "        seen_pairs = {}  # Track duplicates\n",
    "\n",
    "        for result in results:\n",
    "            qa_pairs = result.get(\"qa_pairs\", [])\n",
    "            if not isinstance(qa_pairs, list):\n",
    "                continue\n",
    "\n",
    "            for qa_pair in qa_pairs:\n",
    "                try:\n",
    "                    qa_data = qa_pair.get(\"qa_pair\", {})\n",
    "                    if not qa_data:\n",
    "                        continue\n",
    "\n",
    "                    # Get key fields\n",
    "                    analyst_name = qa_data.get(\"analyst_name\", \"Unknown Analyst\")\n",
    "                    questions = qa_data.get(\"questions\", [])\n",
    "                    answers = qa_data.get(\"answers\", [])\n",
    "\n",
    "                    # Skip if no meaningful content\n",
    "                    if not questions or not answers:\n",
    "                        continue\n",
    "\n",
    "                    # Create hash for deduplication\n",
    "                    question_text = \" \".join(questions)[\n",
    "                        :100\n",
    "                    ]  # First 100 chars of question\n",
    "                    answer_preview = (\n",
    "                        answers[0].get(\"answer_text\", \"\")[:100] if answers else \"\"\n",
    "                    )\n",
    "                    qa_hash = hash(f\"{analyst_name}_{question_text}_{answer_preview}\")\n",
    "\n",
    "                    if qa_hash not in seen_pairs:\n",
    "                        # Store original position for sequencing\n",
    "                        qa_data[\"original_position\"] = len(all_qa_pairs)\n",
    "                        all_qa_pairs.append({\"qa_pair\": qa_data})\n",
    "                        seen_pairs[qa_hash] = len(all_qa_pairs) - 1\n",
    "                    else:\n",
    "                        # Compare lengths and take longer version if significant difference\n",
    "                        existing_idx = seen_pairs[qa_hash]\n",
    "                        existing_qa = all_qa_pairs[existing_idx][\"qa_pair\"]\n",
    "\n",
    "                        # If new version is significantly longer, replace existing\n",
    "                        new_length = len(str(qa_data))\n",
    "                        existing_length = len(str(existing_qa))\n",
    "                        if new_length > existing_length * 1.2:  # 20% longer threshold\n",
    "                            all_qa_pairs[existing_idx][\"qa_pair\"] = qa_data\n",
    "                            self.logger.info(\"Replaced QA pair with longer version\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing QA pair: {e}\", exc_info=True)\n",
    "                    continue\n",
    "\n",
    "        # Sort by original position to maintain chronological order\n",
    "        all_qa_pairs.sort(key=lambda x: x[\"qa_pair\"].get(\"original_position\", 0))\n",
    "\n",
    "        # Remove temporary position field\n",
    "        for qa_pair in all_qa_pairs:\n",
    "            qa_pair[\"qa_pair\"].pop(\"original_position\", None)\n",
    "\n",
    "        self.logger.info(f\"Merged {len(all_qa_pairs)} unique QA pairs\")\n",
    "        return {\"metadata\": final_metadata, \"qa_pairs\": all_qa_pairs}\n",
    "\n",
    "    async def process_pdf_transcript(\n",
    "        self, pdf_path: str, use_batch: bool = False, selected_banks=None\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a single PDF transcript file\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "            use_batch: Whether to use batch processing (default: False)\n",
    "            selected_banks: List of banks to process (if None, process all)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with metadata and Q&A pairs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract text from PDF\n",
    "            text_lines = self._read_pdf(pdf_path)\n",
    "            if not text_lines:\n",
    "                self.logger.warning(f\"No text extracted from {pdf_path}\")\n",
    "                return {\"metadata\": {}, \"qa_pairs\": []}\n",
    "\n",
    "            # Join lines for processing\n",
    "            transcript_text = \"\\n\".join(text_lines)\n",
    "\n",
    "            # Extract metadata\n",
    "            metadata = await self.extract_transcript_metadata(\n",
    "                transcript_text, use_batch=use_batch\n",
    "            )\n",
    "            if not metadata:\n",
    "                self.logger.warning(f\"Failed to extract metadata from {pdf_path}\")\n",
    "                return {\"metadata\": {}, \"qa_pairs\": []}\n",
    "\n",
    "            # Process Q&A pairs\n",
    "            qa_result = await self.process_transcript_qa(\n",
    "                transcript_text, metadata, use_batch=use_batch\n",
    "            )\n",
    "\n",
    "            return qa_result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error processing PDF transcript {pdf_path}: {e}\", exc_info=True\n",
    "            )\n",
    "            return {\"metadata\": {}, \"qa_pairs\": []}\n",
    "\n",
    "    def store_to_database(self, structured_data: Dict) -> int:\n",
    "        \"\"\"\n",
    "        Store structured transcript data to database, always replacing existing data\n",
    "\n",
    "        Args:\n",
    "            structured_data: Dictionary with metadata and Q&A pairs\n",
    "\n",
    "        Returns:\n",
    "            ID of the report in the database\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Storing transcript data to database\")\n",
    "\n",
    "        def _save_transcript_to_db(conn):\n",
    "            # Get metadata\n",
    "            metadata = structured_data.get(\"metadata\", {})\n",
    "            company = metadata.get(\"company\", \"Unknown\")\n",
    "            year = metadata.get(\"year\", \"Unknown\")\n",
    "            quarter = metadata.get(\"quarter\", \"Unknown\")\n",
    "\n",
    "            # Add bank and get ID\n",
    "            bank_id = self.db.add_bank(company)\n",
    "\n",
    "            # Check if report already exists and always delete it\n",
    "            report_id = self.db.get_report_id(bank_id, year, quarter)\n",
    "            if report_id:\n",
    "                self.logger.info(f\"Deleting existing report ID: {report_id}\")\n",
    "                self.db.delete_report_data(report_id)\n",
    "\n",
    "            # Create a new report\n",
    "            report_id = self.db.add_report(\n",
    "                bank_id=bank_id,\n",
    "                report_name=f\"{company} {year} {quarter} Earnings Call\",\n",
    "                year=year,\n",
    "                quarter=quarter,\n",
    "            )\n",
    "\n",
    "            # Process analyst conversation groups\n",
    "            analyst_conversations = {}  # Track conversations by analyst\n",
    "\n",
    "            for qa_item in structured_data.get(\"qa_pairs\", []):\n",
    "                qa_pair = qa_item.get(\"qa_pair\", {})\n",
    "                if not qa_pair:\n",
    "                    continue\n",
    "\n",
    "                analyst_name = qa_pair.get(\"analyst_name\", \"Unknown Analyst\")\n",
    "                analyst_company = qa_pair.get(\"analyst_company\", \"Unknown Firm\")\n",
    "\n",
    "                # Create unique key for this analyst\n",
    "                analyst_key = f\"{analyst_name}_{analyst_company}\"\n",
    "\n",
    "                # Create conversation if not seen this analyst yet\n",
    "                if analyst_key not in analyst_conversations:\n",
    "                    conversation_id = self.db.add_analyst_conversation(\n",
    "                        report_id=report_id,\n",
    "                        analyst_name=analyst_name,\n",
    "                        analyst_company=analyst_company,\n",
    "                    )\n",
    "                    analyst_conversations[analyst_key] = conversation_id\n",
    "                else:\n",
    "                    conversation_id = analyst_conversations[analyst_key]\n",
    "\n",
    "                # Get topic information if available\n",
    "                topic = qa_pair.get(\"topic\", \"\")\n",
    "\n",
    "                # Process questions and answers\n",
    "                questions = qa_pair.get(\"questions\", [])\n",
    "                answers = qa_pair.get(\"answers\", [])\n",
    "\n",
    "                # Store each question-answer pair\n",
    "                for i, question in enumerate(questions):\n",
    "                    # Store each answer for this question\n",
    "                    for answer in answers:\n",
    "                        answer_text = answer.get(\"answer_text\", \"\")\n",
    "\n",
    "                        # Add to database\n",
    "                        qa_pair_id = self.db.add_qa_pair(\n",
    "                            conversation_id=conversation_id,\n",
    "                            question=question,\n",
    "                            answer=answer_text,\n",
    "                            answer_speaker=answer.get(\"speaker_name\", \"Unknown\"),\n",
    "                            answer_role=answer.get(\"speaker_role\", \"Unknown\"),\n",
    "                        )\n",
    "\n",
    "            return report_id\n",
    "\n",
    "        try:\n",
    "            # Use the database's transaction wrapper\n",
    "            return self.db.execute_in_transaction(_save_transcript_to_db)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing transcript data: {e}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    async def process_transcript_batch(\n",
    "        self,\n",
    "        folder_path: str,\n",
    "        batch_size: int = 4,\n",
    "        use_batch: bool = False,\n",
    "        selected_banks=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Process multiple transcripts in parallel batches\n",
    "\n",
    "        Args:\n",
    "            folder_path: Path to folder containing PDF transcripts\n",
    "            batch_size: Number of concurrent processes\n",
    "            use_batch: Whether to use batch processing for Claude API (default: False)\n",
    "            selected_banks: List of banks to process (if None, process all)\n",
    "        \"\"\"\n",
    "        # Get list of PDF files\n",
    "        pdf_files = list(Path(folder_path).glob(\"*.pdf\"))\n",
    "        self.logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "        # Track processing results\n",
    "        results = []\n",
    "\n",
    "        # Process files sequentially (still using async for Claude API calls)\n",
    "        for pdf_file in pdf_files:\n",
    "            try:\n",
    "                # Process the transcript\n",
    "                self.logger.info(f\"Processing {pdf_file}\")\n",
    "                result = await self.process_pdf_transcript(\n",
    "                    str(pdf_file), use_batch=use_batch, selected_banks=selected_banks\n",
    "                )\n",
    "\n",
    "                # Store to database if we have valid data\n",
    "                if result and result.get(\"metadata\") and result.get(\"qa_pairs\"):\n",
    "                    report_id = self.store_to_database(result)\n",
    "                    self.logger.info(\n",
    "                        f\"Successfully processed and stored {pdf_file} (Report ID: {report_id})\"\n",
    "                    )\n",
    "                    results.append(\n",
    "                        {\"file\": pdf_file.name, \"success\": True, \"report_id\": report_id}\n",
    "                    )\n",
    "                else:\n",
    "                    self.logger.warning(f\"No valid data extracted from {pdf_file}\")\n",
    "                    results.append(\n",
    "                        {\n",
    "                            \"file\": pdf_file.name,\n",
    "                            \"success\": False,\n",
    "                            \"error\": \"No valid data extracted\",\n",
    "                        }\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error processing {pdf_file}: {e}\", exc_info=True)\n",
    "                results.append(\n",
    "                    {\"file\": pdf_file.name, \"success\": False, \"error\": str(e)}\n",
    "                )\n",
    "\n",
    "        # Return processing results\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k26XH-EMlBq1",
    "outputId": "b8fa876c-801d-430e-d0d0-83d1e77f11e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ClaudeTranscriptProcessor:Found 16 PDF files to process\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 897 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 20 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/20\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 15 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/15\n",
      "INFO:ClaudeTranscriptProcessor:Merging 15 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 29 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 1)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 1533 clean lines from earnings_transcripts/UBS Group AG (UBS) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 36 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 23/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 24/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 25/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 26/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 27/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 28/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 29/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 30/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 31/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 32/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 33/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 34/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 35/36\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 36/36\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 27 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 18/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 19/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 20/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 21/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 22/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 23/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 24/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 25/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 26/27\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 27/27\n",
      "INFO:ClaudeTranscriptProcessor:Merging 27 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 19 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 2)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 859 clean lines from earnings_transcripts/UBS Group AG (UBS) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 22 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/22\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 17 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/17\n",
      "INFO:ClaudeTranscriptProcessor:Merging 17 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 11 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 3)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 726 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 16 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/16\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 12 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/12\n",
      "INFO:ClaudeTranscriptProcessor:Merging 12 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 16 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 4)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 894 clean lines from earnings_transcripts/UBS Group AG (UBS) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 23 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 23/23\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS Group AG\n",
      "INFO:ClaudeTranscriptProcessor:Processing 17 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/17\n",
      "INFO:ClaudeTranscriptProcessor:Merging 17 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 10 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 5)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 870 clean lines from earnings_transcripts/UBS Group AG (UBS) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 22 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/22\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 16 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/16\n",
      "INFO:ClaudeTranscriptProcessor:Merging 16 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 14 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 6)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 1218 clean lines from earnings_transcripts/UBS Group AG (UBS) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 29 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 23/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 24/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 25/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 26/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 27/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 28/29\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 29/29\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 22 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 18/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 19/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 20/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 21/22\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 22/22\n",
      "INFO:ClaudeTranscriptProcessor:Merging 22 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 17 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q2 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 7)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 921 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 21 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/21\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 16 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/16\n",
      "INFO:ClaudeTranscriptProcessor:Merging 16 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 24 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 8)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 862 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 20 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/20\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/20\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 15 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/15\n",
      "INFO:ClaudeTranscriptProcessor:Merging 15 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 22 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 9)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 926 clean lines from earnings_transcripts/UBS Group AG (UBS) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 23 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/23\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 23/23\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 17 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/17\n",
      "INFO:ClaudeTranscriptProcessor:Merging 17 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 13 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q3 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 10)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 690 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 15 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/15\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/15\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 12 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/12\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/12\n",
      "INFO:ClaudeTranscriptProcessor:Merging 12 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 22 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q4 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 11)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 796 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 18 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/18\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/18\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 14 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/14\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/14\n",
      "INFO:ClaudeTranscriptProcessor:Merging 14 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 23 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q2 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 12)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 864 clean lines from earnings_transcripts/UBS Group AG (UBS) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 21 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/21\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 16 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/16\n",
      "ERROR:ClaudeTranscriptProcessor:Error processing Q&A chunk 13: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/16\n",
      "INFO:ClaudeTranscriptProcessor:Merging 15 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 14 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 13)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/UBS Group AG (UBS) Q4 2024 Earnings Conference Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/UBS Group AG (UBS) Q4 2024 Earnings Conference Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 1276 clean lines from earnings_transcripts/UBS Group AG (UBS) Q4 2024 Earnings Conference Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 32 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 22/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 23/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 24/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 25/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 26/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 27/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 28/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 29/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 30/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 31/32\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 32/32\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for UBS\n",
      "INFO:ClaudeTranscriptProcessor:Processing 24 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 17/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 18/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 19/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 20/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 21/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 22/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 23/24\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 24/24\n",
      "INFO:ClaudeTranscriptProcessor:Merging 24 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Merged 16 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/UBS Group AG (UBS) Q4 2024 Earnings Conference Call Transcript _ Seeking Alpha.pdf (Report ID: 14)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 744 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 17 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/17\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/17\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 13 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/13\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/13\n",
      "INFO:ClaudeTranscriptProcessor:Merging 13 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Replaced QA pair with longer version\n",
      "INFO:ClaudeTranscriptProcessor:Merged 25 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q1 2023 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 15)\n",
      "INFO:ClaudeTranscriptProcessor:Processing earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Reading PDF from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Extracted 919 clean lines from earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf\n",
      "INFO:ClaudeTranscriptProcessor:Processing 21 chunks for metadata extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 1/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 2/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 3/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 4/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 5/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 6/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 7/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 8/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 9/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 10/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 11/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 12/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 13/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 14/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 15/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 16/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 17/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 18/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 19/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 20/21\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed metadata chunk 21/21\n",
      "INFO:ClaudeTranscriptProcessor:Processed metadata for JPMorgan Chase\n",
      "INFO:ClaudeTranscriptProcessor:Processing 16 chunks for Q&A extraction (direct method)\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 1/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 2/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 3/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 4/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 5/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 6/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 7/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 8/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 9/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 10/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 11/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 12/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 13/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 14/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 15/16\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed Q&A chunk 16/16\n",
      "INFO:ClaudeTranscriptProcessor:Merging 16 Q&A batch results\n",
      "INFO:ClaudeTranscriptProcessor:Replaced QA pair with longer version\n",
      "INFO:ClaudeTranscriptProcessor:Merged 23 unique QA pairs\n",
      "INFO:ClaudeTranscriptProcessor:Storing transcript data to database\n",
      "INFO:ClaudeTranscriptProcessor:Successfully processed and stored earnings_transcripts/JPMorgan Chase & Co. (JPM) Q3 2024 Earnings Call Transcript _ Seeking Alpha.pdf (Report ID: 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 16 files\n"
     ]
    }
   ],
   "source": [
    "# Apply nest_asyncio to allow nested event loops in Colab\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "# Function to process transcripts in Colab\n",
    "async def process_transcripts_colab(db, folder_path, use_batch=False):\n",
    "    \"\"\"Process transcripts in Colab environment\"\"\"\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = ClaudeTranscriptProcessor(db, model=CLAUDE_API_MODEL)\n",
    "\n",
    "    # Process transcripts (direct method by default)\n",
    "    results = await processor.process_transcript_batch(\n",
    "        folder_path,\n",
    "        use_batch=use_batch,\n",
    "        selected_banks=SELECTED_BANKS,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run this instead of asyncio.run()\n",
    "results = await process_transcripts_colab(db, CALL_TRANSCRIPTS_DOWNLOAD_FOLDER)\n",
    "print(f\"Processed {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_0nbutX_0s_"
   },
   "source": [
    "## üìà Credit-Risk Metrics Extractor\n",
    "### Features\n",
    "* **Smart PDF Processing**: Identifies relevant pages containing financial metrics tables\n",
    "* **Document AI Integration**: Uses Google Cloud's Document AI for entity extraction\n",
    "* **Bank-Specific Adaptability**: Automatically detects bank report formats and applies appropriate entity mappings\n",
    "* **Standardized Metrics Calculation**: Calculates key credit risk indicators including:\n",
    "\n",
    "    * Non-Performing Loan (NPL) Ratio\n",
    "    * Coverage Ratio\n",
    "    * Provision for Credit Losses (PCL)\n",
    "    * Common Equity Tier 1 (CET1) Ratio\n",
    "    * Texas Ratio\n",
    "\n",
    "\n",
    "* **Batch Processing**: Handles multiple financial reports in a single run\n",
    "* **Output Generation**: Creates structured JSON and CSV outputs for further analysis\n",
    "\n",
    "### Metrics Explained\n",
    "\n",
    "1. **NPL Ratio**: Measures the percentage of non-performing loans in the total loan portfolio\n",
    "2. **Coverage Ratio**: Shows how much of the non-performing loans are covered by allowances\n",
    "3. **PCL**: Provision for Credit Losses for the current period\n",
    "4. **CET1 Ratio**: Common Equity Tier 1 Capital Ratio, a measure of bank's core capital\n",
    "5. **Texas Ratio**: Measures a bank's credit problems relative to its capital resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kM7aU5d2An46"
   },
   "outputs": [],
   "source": [
    "class BankingMetricsCalculator:\n",
    "    \"\"\"Extract banking metrics from financial reports using Document AI and LLMs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        project_id: str,\n",
    "        location: str,\n",
    "        processor_id: str,\n",
    "        output_dir: str,\n",
    "        db: GSIBDatabase,\n",
    "        processor_version_id: Optional[str] = None,\n",
    "        batch_size: int = 10,\n",
    "    ):\n",
    "        \"\"\"Initialize the extractor with API credentials and parameters.\n",
    "\n",
    "        Args:\n",
    "            project_id: Google Cloud project ID\n",
    "            location: Google Cloud region (e.g., 'eu')\n",
    "            processor_id: Document AI processor ID\n",
    "            output_dir: Directory to save extraction results\n",
    "            db: Initialized GSIBDatabase instance\n",
    "            processor_version_id: Optional Document AI processor version ID\n",
    "            batch_size: Maximum batch size for API calls\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.info(\"Initializing BankingMetricsCalculator\")\n",
    "\n",
    "        # Store parameters\n",
    "        self.project_id = project_id\n",
    "        self.location = location\n",
    "        self.processor_id = processor_id\n",
    "        self.batch_size = batch_size\n",
    "        self.db = db  # Store the database instance\n",
    "\n",
    "        # Setup output directory\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        # Initialize clients\n",
    "        self.docai_client = self._setup_docai_client(processor_version_id)\n",
    "\n",
    "        self.target_metrics = [\n",
    "            \"NPL_Ratio\",  # Non-Performing Loan Ratio\n",
    "            \"Coverage_Ratio\",  # Coverage Ratio\n",
    "            \"PCL\",  # Provision for Credit Losses\n",
    "            \"CET1_Ratio\",  # Common Equity Tier 1 Ratio\n",
    "            \"Texas_Ratio\",  # Texas Ratio\n",
    "        ]\n",
    "        self._define_metrics()\n",
    "\n",
    "        self.logger.info(\"BankingMetricsCalculator initialized successfully\")\n",
    "\n",
    "    def _setup_docai_client(\n",
    "        self,\n",
    "        processor_version_id: Optional[str],\n",
    "    ) -> documentai.DocumentProcessorServiceClient:\n",
    "        \"\"\"Set up and return a Document AI client.\n",
    "\n",
    "        Args:\n",
    "            processor_version_id: Optional processor version ID\n",
    "\n",
    "        Returns:\n",
    "            Configured Document AI client\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If client initialization fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            endpoint = f\"{self.location}-documentai.googleapis.com\"\n",
    "            self.logger.debug(f\"Setting up Document AI client with endpoint {endpoint}\")\n",
    "\n",
    "            opts = ClientOptions(api_endpoint=endpoint)\n",
    "            client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "            # Set up processor path\n",
    "            if processor_version_id:\n",
    "                self.processor_name = client.processor_version_path(\n",
    "                    self.project_id,\n",
    "                    self.location,\n",
    "                    self.processor_id,\n",
    "                    processor_version_id,\n",
    "                )\n",
    "                self.logger.debug(f\"Using processor version: {processor_version_id}\")\n",
    "            else:\n",
    "                self.processor_name = client.processor_path(\n",
    "                    self.project_id,\n",
    "                    self.location,\n",
    "                    self.processor_id,\n",
    "                )\n",
    "\n",
    "            self.logger.debug(f\"Document AI processor path: {self.processor_name}\")\n",
    "            return client\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Failed to initialize Document AI client: {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "            raise RuntimeError(f\"Document AI client initialization failed: {e}\")\n",
    "\n",
    "    def _define_metrics(self) -> None:\n",
    "        \"\"\"Define target metrics and their calculation methods with bank-specific entity mappings.\"\"\"\n",
    "        # Define entity mappings for different banks\n",
    "        self.entity_mappings = {\n",
    "            # UBS entity mappings\n",
    "            \"UBS\": {\n",
    "                \"nonperforming_loans\": \"total_nonperforming_assets\",\n",
    "                \"nonperforming_assets\": \"total_nonperforming_assets\",\n",
    "                \"total_loans\": \"total_loans\",\n",
    "                \"loan_loss_allowance\": \"total_allowance_for_loan_losses\",\n",
    "                \"cet1_capital\": \"cet1_capital\",\n",
    "                \"risk_weighted_assets\": \"risk_weighted_assets\",\n",
    "                \"provision_for_credit_losses\": \"provision_for_credit_losses\",\n",
    "                \"tangible_equity\": \"tangible_equity_attributable_shareholders\",\n",
    "            },\n",
    "            # JPMorgan entity mappings\n",
    "            \"JPMorgan Chase\": {\n",
    "                \"nonperforming_loans\": \"total_nonaccrual_loans\",  # Primary source\n",
    "                \"nonperforming_assets\": \"total_nonperforming_assets\",  # Primary source\n",
    "                \"total_loans\": \"total_loans\",\n",
    "                \"loan_loss_allowance\": \"total_allowance_for_loan_losses\",\n",
    "                \"cet1_capital\": \"cet1_capital\",\n",
    "                \"risk_weighted_assets\": \"risk_weighted_assets\",\n",
    "                \"provision_for_credit_losses\": \"provision_for_credit_losses\",\n",
    "                \"tangible_equity\": \"tangible_equity\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Define metrics properties with calculation logic\n",
    "        self.metrics_info = {\n",
    "            \"NPL_Ratio\": {\n",
    "                \"description\": \"Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.\",\n",
    "                \"calculation\": \"Non-performing loans / Total loans * 100\",\n",
    "                \"standardized_entities\": [\"nonperforming_loans\", \"total_loans\"],\n",
    "                \"compute\": lambda vals: (vals[0] / vals[1]) * 100\n",
    "                if vals[1] and vals[1] > 0\n",
    "                else None,\n",
    "                \"typical_range\": [0.1, 5.0],\n",
    "                \"format\": \"%.2f%%\",\n",
    "            },\n",
    "            \"Coverage_Ratio\": {\n",
    "                \"description\": \"Coverage Ratio shows how much of the non-performing loans are covered by allowances.\",\n",
    "                \"calculation\": \"Loan loss allowance / Non-performing loans * 100\",\n",
    "                \"standardized_entities\": [\"loan_loss_allowance\", \"nonperforming_loans\"],\n",
    "                \"compute\": lambda vals: (vals[0] / vals[1]) * 100\n",
    "                if vals[1] and vals[1] > 0\n",
    "                else None,\n",
    "                \"typical_range\": [20.0, 500.0],\n",
    "                \"format\": \"%.2f%%\",\n",
    "            },\n",
    "            \"PCL\": {\n",
    "                \"description\": \"Provision for Credit Losses for the current period.\",\n",
    "                \"calculation\": \"Direct value from Provision for Credit Losses line item\",\n",
    "                \"standardized_entities\": [\"provision_for_credit_losses\"],\n",
    "                \"compute\": lambda vals: vals[0],\n",
    "                \"typical_range\": [0, 4000],\n",
    "                \"format\": \"%.0f million\",\n",
    "            },\n",
    "            \"CET1_Ratio\": {\n",
    "                \"description\": \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
    "                \"calculation\": \"Common Equity Tier 1 Capital / Risk Weighted Assets * 100\",\n",
    "                \"standardized_entities\": [\"cet1_capital\", \"risk_weighted_assets\"],\n",
    "                \"compute\": lambda vals: (vals[0] / vals[1]) * 100\n",
    "                if vals[1] and vals[1] > 0\n",
    "                else None,\n",
    "                \"typical_range\": [4.0, 20.0],\n",
    "                \"format\": \"%.1f%%\",\n",
    "            },\n",
    "            \"Texas_Ratio\": {\n",
    "                \"description\": \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
    "                \"calculation\": \"Non-performing assets / (Tangible equity + Loan loss allowance) * 100\",\n",
    "                \"standardized_entities\": [\n",
    "                    \"nonperforming_assets\",\n",
    "                    \"tangible_equity\",\n",
    "                    \"loan_loss_allowance\",\n",
    "                ],\n",
    "                \"compute\": lambda vals: (vals[0] / (vals[1] + vals[2])) * 100\n",
    "                if (\n",
    "                    vals[1] is not None\n",
    "                    and vals[2] is not None\n",
    "                    and vals[1] + vals[2] > 0\n",
    "                )\n",
    "                else None,\n",
    "                \"typical_range\": [0, 100.0],  # Concerning when over 100%\n",
    "                \"format\": \"%.2f%%\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _clean_numeric_value(self, value):\n",
    "        \"\"\"\n",
    "        Convert string values with currency symbols, commas, etc. to float.\n",
    "\n",
    "        Args:\n",
    "            value: The value to clean and convert\n",
    "\n",
    "        Returns:\n",
    "            float: The cleaned numeric value, or None if conversion fails\n",
    "        \"\"\"\n",
    "        if value is None:\n",
    "            return None\n",
    "\n",
    "        # If value is already a number, return it\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "\n",
    "        try:\n",
    "            # If value is a string, clean and convert\n",
    "            if isinstance(value, str):\n",
    "                # Remove currency symbols, commas, parentheses (negative values), and whitespace\n",
    "                cleaned = (\n",
    "                    value.replace(\"$\", \"\")\n",
    "                    .replace(\"‚Ç¨\", \"\")\n",
    "                    .replace(\"¬£\", \"\")\n",
    "                    .replace(\"CHF\", \"\")\n",
    "                )\n",
    "                cleaned = cleaned.replace(\",\", \"\").replace(\" \", \"\")\n",
    "\n",
    "                # Handle parentheses notation for negative numbers: (123) -> -123\n",
    "                if cleaned.startswith(\"(\") and cleaned.endswith(\")\"):\n",
    "                    cleaned = \"-\" + cleaned[1:-1]\n",
    "\n",
    "                # Handle percentage values\n",
    "                if cleaned.endswith(\"%\"):\n",
    "                    cleaned = cleaned.rstrip(\"%\")\n",
    "                    return float(cleaned) / 100.0\n",
    "\n",
    "                return float(cleaned)\n",
    "            self.logger.warning(\n",
    "                f\"Unexpected value type: {type(value)} for value: {value}\",\n",
    "            )\n",
    "            return None\n",
    "        except (ValueError, TypeError) as e:\n",
    "            self.logger.warning(\n",
    "                f\"Failed to convert value '{value}' to numeric: {str(e)}\",\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    def _detect_bank_and_period(\n",
    "        self,\n",
    "        entities: Dict[str, Any],\n",
    "    ) -> Tuple[str, int, str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Detect bank name, year, and quarter from entities\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (bank_name, year, quarter, report_date)\n",
    "        \"\"\"\n",
    "        # Determine bank name\n",
    "        bank_name = entities.get(\"bank_name\")\n",
    "        if not bank_name:\n",
    "            # If bank_name not explicitly present, infer from available fields\n",
    "            if \"tangible_equity_attributable_shareholders\" in entities:\n",
    "                bank_name = \"UBS\"\n",
    "            else:\n",
    "                bank_name = \"JPMorgan Chase\"\n",
    "\n",
    "        # Normalize bank name\n",
    "        if \"jpmorgan\" in bank_name.lower() or \"chase\" in bank_name.lower():\n",
    "            bank_name = \"JPMorgan Chase\"\n",
    "        elif \"ubs\" in bank_name.lower():\n",
    "            bank_name = \"UBS\"\n",
    "\n",
    "        # Extract report period\n",
    "        report_period = entities.get(\"report_period\", \"\")\n",
    "        report_date = None\n",
    "\n",
    "        # Pattern 1: QNYR format (e.g., \"4Q23\" or \"1Q2023\")\n",
    "        qy_pattern = re.compile(r\"(\\d)Q(\\d{2,4})\")\n",
    "        qy_match = qy_pattern.search(report_period)\n",
    "\n",
    "        # Pattern 2: ISO date format (e.g., \"31.12.24\" or \"31.12.2024\")\n",
    "        date_pattern = re.compile(r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})\")\n",
    "        date_match = date_pattern.search(report_period)\n",
    "\n",
    "        if qy_match:\n",
    "            quarter = f\"Q{qy_match.group(1)}\"\n",
    "            year_str = qy_match.group(2)\n",
    "            # Convert 2-digit year to 4-digit\n",
    "            year = int(year_str)\n",
    "            if len(year_str) == 2:\n",
    "                year = 2000 + year if year < 50 else 1900 + year\n",
    "        elif date_match:\n",
    "            # Extract date components\n",
    "            day = int(date_match.group(1))\n",
    "            month = int(date_match.group(2))\n",
    "            year_str = date_match.group(3)\n",
    "\n",
    "            # Convert 2-digit year to 4-digit\n",
    "            year = int(year_str)\n",
    "            if len(year_str) == 2:\n",
    "                year = 2000 + year if year < 50 else 1900 + year\n",
    "\n",
    "            # Determine quarter from month\n",
    "            quarter_map = {\n",
    "                1: \"Q1\",\n",
    "                2: \"Q1\",\n",
    "                3: \"Q1\",\n",
    "                4: \"Q2\",\n",
    "                5: \"Q2\",\n",
    "                6: \"Q2\",\n",
    "                7: \"Q3\",\n",
    "                8: \"Q3\",\n",
    "                9: \"Q3\",\n",
    "                10: \"Q4\",\n",
    "                11: \"Q4\",\n",
    "                12: \"Q4\",\n",
    "            }\n",
    "            quarter = quarter_map[month]\n",
    "\n",
    "            # Format ISO date\n",
    "            report_date = f\"{year}-{month:02d}-{day:02d}\"\n",
    "        else:\n",
    "            # Default fallback\n",
    "            current_date = datetime.now()\n",
    "            year = current_date.year\n",
    "            quarter = f\"Q{(current_date.month - 1) // 3 + 1}\"\n",
    "            self.logger.warning(\n",
    "                f\"Couldn't determine period from '{report_period}', using current: {year} {quarter}\",\n",
    "            )\n",
    "\n",
    "        return bank_name, year, quarter, report_date\n",
    "\n",
    "    def _normalize_jpmorgan_entities(self, entities):\n",
    "        \"\"\"Handle JPMorgan-specific entity calculations.\"\"\"\n",
    "        # Clean all entity values first, but skip report_period, bank_name\n",
    "        cleaned_entities = {}\n",
    "        for k, v in entities.items():\n",
    "            if k in [\"report_period\", \"bank_name\"]:\n",
    "                cleaned_entities[k] = v  # Keep report_period, bank_name as is\n",
    "            else:\n",
    "                cleaned_value = self._clean_numeric_value(v)\n",
    "                if cleaned_value is not None:\n",
    "                    cleaned_entities[k] = cleaned_value\n",
    "\n",
    "        # If we have tangible book value per share and share count, calculate total tangible equity\n",
    "        if (\n",
    "            \"tangible_book_value_per_share\" in cleaned_entities\n",
    "            and \"common_shares_at_period_end\" in cleaned_entities\n",
    "        ):\n",
    "            cleaned_entities[\"tangible_equity\"] = (\n",
    "                cleaned_entities[\"tangible_book_value_per_share\"]\n",
    "                * cleaned_entities[\"common_shares_at_period_end\"]\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f\"Calculated tangible_equity: {cleaned_entities['tangible_equity']}\",\n",
    "            )\n",
    "\n",
    "        # Use total_nonperforming_assets if total_nonaccrual_loans is not available\n",
    "        if (\n",
    "            \"total_nonaccrual_loans\" not in cleaned_entities\n",
    "            and \"total_nonperforming_assets\" in cleaned_entities\n",
    "        ):\n",
    "            cleaned_entities[\"total_nonaccrual_loans\"] = cleaned_entities[\n",
    "                \"total_nonperforming_assets\"\n",
    "            ]\n",
    "\n",
    "        # Use provision_for_loan_losses if provision_for_credit_losses is not available\n",
    "        if (\n",
    "            \"provision_for_credit_losses\" not in cleaned_entities\n",
    "            and \"provision_for_loan_losses\" in cleaned_entities\n",
    "        ):\n",
    "            cleaned_entities[\"provision_for_credit_losses\"] = cleaned_entities[\n",
    "                \"provision_for_loan_losses\"\n",
    "            ]\n",
    "\n",
    "        return cleaned_entities\n",
    "\n",
    "    def _save_to_database(\n",
    "        self,\n",
    "        file_name: str,\n",
    "        metrics: Dict[str, Dict[str, Any]],\n",
    "        entities: Dict[str, Any],\n",
    "    ) -> int:\n",
    "        \"\"\"\n",
    "        Save metrics and entities to database using the GSIBDatabase wrapper.\n",
    "\n",
    "        Args:\n",
    "            file_name: Original file name\n",
    "            metrics: Dictionary of calculated metrics\n",
    "            entities: Dictionary of extracted entities\n",
    "\n",
    "        Returns:\n",
    "            report_id: The database ID of the created report\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conn = self.db.begin_transaction()\n",
    "            # Detect bank and period\n",
    "            bank_name, year, quarter, report_date = self._detect_bank_and_period(\n",
    "                entities,\n",
    "            )\n",
    "            # Add bank and get ID\n",
    "            bank_id = self.db.add_bank(bank_name)\n",
    "\n",
    "            # Query for existing report with matching bank_id, year, and quarter\n",
    "            report_id = self.db.get_report_id(\n",
    "                bank_name=bank_name,\n",
    "                year=year,\n",
    "                quarter=quarter,\n",
    "            )\n",
    "\n",
    "            # If no existing report found, create a new one\n",
    "            if report_id is None:\n",
    "                report_id = self.db.add_report(\n",
    "                    bank_id=bank_id,\n",
    "                    report_name=file_name,\n",
    "                    year=year,\n",
    "                    quarter=quarter,\n",
    "                    report_date=report_date,\n",
    "                )\n",
    "                self.logger.info(\n",
    "                    f\"Created new report entry: {bank_name} {year} {quarter}\",\n",
    "                )\n",
    "            else:\n",
    "                self.logger.info(\n",
    "                    f\"Using existing report entry: {bank_name} {year} {quarter}\",\n",
    "                )\n",
    "\n",
    "            # Save metrics\n",
    "            for metric_name, metric_data in metrics.items():\n",
    "                try:\n",
    "                    self.db.add_metric(\n",
    "                        report_id=report_id,\n",
    "                        metric_name=metric_name,\n",
    "                        metric_value=metric_data.get(\"value\"),\n",
    "                        formatted_value=metric_data.get(\"formatted_value\"),\n",
    "                        in_typical_range=metric_data.get(\"in_typical_range\", 0),\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error saving metric {metric_name}: {e}\")\n",
    "\n",
    "            self.logger.info(f\"Saved to database: {bank_name} {year} {quarter}\")\n",
    "            self.db.commit_transaction(conn)\n",
    "            return report_id\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving to database: {e}\", exc_info=True)\n",
    "            self.db.rollback_transaction(conn)\n",
    "            return None\n",
    "\n",
    "    def _save_enhanced_summary(self) -> Path:\n",
    "        \"\"\"\n",
    "        Generate and save enhanced summary to CSV\n",
    "\n",
    "        Returns:\n",
    "            Path to saved CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            summary_df = self.db.generate_enhanced_metrics_summary()\n",
    "\n",
    "            if not summary_df.empty:\n",
    "                # Save to CSV\n",
    "                summary_path = f\"{self.output_dir}/enhanced_metrics_summary.csv\"\n",
    "                summary_df.to_csv(summary_path, index=False)\n",
    "                self.logger.info(f\"Enhanced summary saved to {summary_path}\")\n",
    "                return summary_path\n",
    "            self.logger.warning(\"No data available for enhanced summary\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving enhanced summary: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def _calculate_metrics(self, entities: dict) -> dict:\n",
    "        \"\"\"Calculate banking metrics from extracted entities.\n",
    "\n",
    "        Args:\n",
    "            entities: Dictionary of extracted entities and values\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of calculated metrics with values and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Calculating metrics from extracted entities\")\n",
    "        self.logger.debug(f\"Available entities: {entities}\")\n",
    "\n",
    "        # Detect which bank's report we're working with\n",
    "        bank_name, _, _, _ = self._detect_bank_and_period(entities)\n",
    "        self.logger.info(f\"Detected bank: {bank_name}\")\n",
    "\n",
    "        # Handle JPMorgan specific calculations if needed\n",
    "        if bank_name == \"JPMorgan Chase\":\n",
    "            entities = self._normalize_jpmorgan_entities(entities)\n",
    "        else:\n",
    "            # For UBS or other banks, clean numeric values but preserve report_period, bank_name\n",
    "            cleaned_entities = {}\n",
    "            for k, v in entities.items():\n",
    "                if k in [\"report_period\", \"bank_name\"]:\n",
    "                    cleaned_entities[k] = v  # Keep report_period, bank_name as is\n",
    "                else:\n",
    "                    cleaned_value = self._clean_numeric_value(v)\n",
    "                    if cleaned_value is not None:\n",
    "                        cleaned_entities[k] = cleaned_value\n",
    "            entities = cleaned_entities\n",
    "\n",
    "        # Get entity mapping for detected bank\n",
    "        entity_mapping = self.entity_mappings[bank_name]\n",
    "\n",
    "        results = {}\n",
    "        for metric_name in self.target_metrics:\n",
    "            metric_info = self.metrics_info[metric_name]\n",
    "            self.logger.debug(f\"Calculating {metric_name}\")\n",
    "\n",
    "            # Map standardized entity names to bank-specific entity names\n",
    "            required_entities = metric_info[\"standardized_entities\"]\n",
    "            entity_values = []\n",
    "            missing_entities = []\n",
    "\n",
    "            # Map and collect required values for this metric\n",
    "            for std_entity in required_entities:\n",
    "                bank_entity = entity_mapping.get(std_entity)\n",
    "\n",
    "                if bank_entity and bank_entity in entities:\n",
    "                    entity_values.append(entities[bank_entity])\n",
    "                else:\n",
    "                    missing_entities.append(std_entity)\n",
    "                    entity_values.append(None)\n",
    "\n",
    "            # Calculate metric if all required entities are available\n",
    "            if not missing_entities:\n",
    "                try:\n",
    "                    # Apply computation function with entity values\n",
    "                    computed_value = metric_info[\"compute\"](entity_values)\n",
    "\n",
    "                    if computed_value is not None:\n",
    "                        # Format value for display\n",
    "                        formatted_value = metric_info[\"format\"] % computed_value\n",
    "\n",
    "                        # Validate against typical range\n",
    "                        min_val, max_val = metric_info[\"typical_range\"]\n",
    "                        in_range = min_val <= computed_value <= max_val\n",
    "\n",
    "                        # Store result with metadata\n",
    "                        results[metric_name] = {\n",
    "                            \"value\": computed_value,\n",
    "                            \"formatted_value\": formatted_value,\n",
    "                            \"in_typical_range\": in_range,\n",
    "                            \"description\": metric_info[\"description\"],\n",
    "                            \"calculation\": metric_info[\"calculation\"],\n",
    "                            \"components\": dict(zip(required_entities, entity_values)),\n",
    "                        }\n",
    "\n",
    "                        self.logger.info(f\"Calculated {metric_name}: {formatted_value}\")\n",
    "\n",
    "                        if not in_range:\n",
    "                            self.logger.warning(\n",
    "                                f\"{metric_name} value {computed_value} outside typical range [{min_val}, {max_val}]\",\n",
    "                            )\n",
    "                    else:\n",
    "                        self.logger.warning(\n",
    "                            f\"Calculation for {metric_name} returned None\",\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    self.logger.error(\n",
    "                        f\"Error calculating {metric_name}: {e}\",\n",
    "                        exc_info=True,\n",
    "                    )\n",
    "            else:\n",
    "                self.logger.warning(\n",
    "                    f\"Cannot calculate {metric_name}. Missing standardized entities: {missing_entities}\",\n",
    "                )\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Calculated {len(results)} out of {len(self.target_metrics)} metrics\",\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def _find_relevant_pages(self, file_path: str) -> Tuple[List[int], str]:\n",
    "        \"\"\"Find pages containing target metrics tables in PDF.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to PDF file\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing list of relevant page numbers and extracted text\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Finding relevant pages in {file_path}\")\n",
    "\n",
    "        # Target sections and their related metrics keywords\n",
    "        target_sections = [\n",
    "            # JP Morgan Chase format\n",
    "            {\n",
    "                \"CAPITAL AND OTHER SELECTED BALANCE SHEET ITEMS\": [\n",
    "                    \"CET1 capital\",\n",
    "                    \"Risk-weighted assets\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"CONSOLIDATED FINANCIAL HIGHLIGHTS\": [\n",
    "                    \"Provision for credit losses\",\n",
    "                    \"Common shares at period-end\",\n",
    "                    \"Tangible book value per share\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"CREDIT-RELATED INFORMATION, CONTINUED\": [\n",
    "                    \"Total nonaccrual loans\",\n",
    "                    \"Total nonperforming assets\",\n",
    "                    \"Provision for loan losses\",\n",
    "                    \"Ending balance\",\n",
    "                    \"Net charge-offs\",\n",
    "                ],\n",
    "            },\n",
    "            {\"CONSOLIDATED FINANCIAL HIGHLIGHTS, CONTINUED\": [\"Total Loans\"]},\n",
    "            # UBS format\n",
    "            {\n",
    "                \"Our key figures\": [\n",
    "                    \"Common equity tier 1 capital\",\n",
    "                    \"Risk-weighted assets\",\n",
    "                    \"Credit loss expense\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"Group performance\": [\n",
    "                    \"Credit loss (expense)\",\n",
    "                    \"Credit loss expense\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"Banking and traded products exposure\": [\n",
    "                    \"Total credit-impaired exposure, gross\",\n",
    "                    \"Total allowances and provisions for expected credit losses\",\n",
    "                    \"Gross exposure\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"Equity, CET1 capital and returns\": [\n",
    "                    \"Tangible equity attributable to shareholders\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"Return on equity and CET1 capital\": [\n",
    "                    \"Tangible equity attributable to shareholders\",\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"UBS shares\": [\n",
    "                    \"Tangible equity attributable to shareholders\",\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        extracted_text = \"\"\n",
    "        relevant_pages = []\n",
    "\n",
    "        try:\n",
    "            # Open the PDF\n",
    "            reader = pypdf.PdfReader(file_path)\n",
    "            total_pages = len(reader.pages)\n",
    "            self.logger.debug(f\"PDF has {total_pages} pages\")\n",
    "\n",
    "            # Scan through pages to find relevant content\n",
    "            for i, page in enumerate(reader.pages):\n",
    "                page_num = i + 1  # 1-indexed page numbers\n",
    "                text = page.extract_text()\n",
    "                extracted_text += text + \"\\n\\n\"\n",
    "\n",
    "                # Check each section and its metrics\n",
    "                for section_dict in target_sections:\n",
    "                    section_header = list(section_dict.keys())[0]\n",
    "                    metrics = section_dict[section_header]\n",
    "\n",
    "                    # Only mark page as relevant if it contains BOTH:\n",
    "                    # 1. The exact section header\n",
    "                    # 2. At least one of the metrics for that section\n",
    "                    if section_header.lower() in text.lower() and any(\n",
    "                        metric.lower() in text.lower() for metric in metrics\n",
    "                    ):\n",
    "                        self.logger.debug(\n",
    "                            f\"Found section '{section_header}' with relevant metrics on page {page_num}\",\n",
    "                        )\n",
    "                        relevant_pages.append(page_num)\n",
    "                        break\n",
    "\n",
    "            # Sort and deduplicate pages\n",
    "            relevant_pages = sorted(set(relevant_pages))\n",
    "\n",
    "            if not relevant_pages:\n",
    "                self.logger.warning(\"No relevant pages found in the document\")\n",
    "                # If no relevant pages found, include first few pages as fallback\n",
    "                relevant_pages = list(range(1, min(6, total_pages + 1)))\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Found {len(relevant_pages)} relevant pages: {relevant_pages}\",\n",
    "            )\n",
    "            return relevant_pages, extracted_text\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error finding relevant pages: {e}\", exc_info=True)\n",
    "            # Return first 5 pages as fallback\n",
    "            fallback_pages = list(range(1, min(6, total_pages + 1)))\n",
    "            self.logger.warning(f\"Using fallback pages: {fallback_pages}\")\n",
    "            return fallback_pages, extracted_text\n",
    "\n",
    "    def _extract_subset_pdf(\n",
    "        self,\n",
    "        input_path: str,\n",
    "        output_path: str,\n",
    "        pages: List[int],\n",
    "    ) -> bool:\n",
    "        \"\"\"Extract specific pages from PDF to create a smaller document.\n",
    "\n",
    "        Args:\n",
    "            input_path: Path to input PDF\n",
    "            output_path: Path where to save the extracted PDF\n",
    "            pages: List of page numbers to extract (1-indexed)\n",
    "\n",
    "        Returns:\n",
    "            Boolean indicating success\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Extracting pages {pages} from {input_path} to {output_path}\")\n",
    "\n",
    "        try:\n",
    "            reader = pypdf.PdfReader(input_path)\n",
    "            writer = pypdf.PdfWriter()\n",
    "\n",
    "            # Add selected pages to the new document\n",
    "            for page_num in pages:\n",
    "                # Convert to 0-indexed\n",
    "                idx = page_num - 1\n",
    "                if 0 <= idx < len(reader.pages):\n",
    "                    writer.add_page(reader.pages[idx])\n",
    "                else:\n",
    "                    self.logger.warning(f\"Page {page_num} out of range, skipping\")\n",
    "\n",
    "            # Save the new PDF\n",
    "            with open(output_path, \"wb\") as output_file:\n",
    "                writer.write(output_file)\n",
    "\n",
    "            self.logger.info(f\"Successfully created subset PDF with {len(pages)} pages\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating subset PDF: {e}\", exc_info=True)\n",
    "            return False\n",
    "\n",
    "    def _process_document_with_docai(self, file_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process document with Document AI to extract entities.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to PDF file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of extracted entities and their values\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Processing document with Document AI: {file_path}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        extracted_entities = {}\n",
    "\n",
    "        try:\n",
    "            # Read file content\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_content = f.read()\n",
    "\n",
    "            # Create Document AI request\n",
    "            request = documentai.ProcessRequest(\n",
    "                name=self.processor_name,\n",
    "                raw_document=documentai.RawDocument(\n",
    "                    content=file_content,\n",
    "                    mime_type=\"application/pdf\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Process the document\n",
    "            self.logger.debug(\"Sending request to Document AI processor\")\n",
    "            result = self.docai_client.process_document(request=request)\n",
    "            document = result.document\n",
    "\n",
    "            # Extract entities\n",
    "            if hasattr(document, \"entities\") and document.entities:\n",
    "                self.logger.debug(f\"Found {len(document.entities)} entities\")\n",
    "\n",
    "                for entity in document.entities:\n",
    "                    entity_type = entity.type_\n",
    "\n",
    "                    # Extract the normalized value if available, otherwise use text\n",
    "                    if hasattr(entity, \"normalized_value\") and entity.normalized_value:\n",
    "                        try:\n",
    "                            value = float(entity.normalized_value.text)\n",
    "                        except ValueError:\n",
    "                            value = entity.normalized_value.text\n",
    "                    else:\n",
    "                        # Extract text and convert to float if possible\n",
    "                        text = self._get_text(entity.text_anchor, document)\n",
    "                        try:\n",
    "                            # Clean and convert to float\n",
    "                            text = text.replace(\",\", \"\").strip()\n",
    "                            value = float(text)\n",
    "                        except ValueError:\n",
    "                            value = text\n",
    "\n",
    "                    self.logger.debug(f\"Extracted entity: {entity_type} = {value}\")\n",
    "                    extracted_entities[entity_type] = value\n",
    "            else:\n",
    "                self.logger.warning(\"No entities found in the document\")\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.logger.info(\n",
    "                f\"Document AI processing completed in {elapsed_time:.2f} seconds\",\n",
    "            )\n",
    "\n",
    "            return extracted_entities\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error processing document with Document AI: {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "    def _get_text(self, text_anchor, document) -> str:\n",
    "        \"\"\"Extract text from text anchor.\n",
    "\n",
    "        Args:\n",
    "            text_anchor: Document AI text anchor\n",
    "            document: Document AI document\n",
    "\n",
    "        Returns:\n",
    "            Extracted text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text = \"\"\n",
    "            for segment_idx in text_anchor.text_segments:\n",
    "                start_idx = segment_idx.start_index\n",
    "                end_idx = segment_idx.end_index\n",
    "                text += document.text[start_idx:end_idx]\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting text from anchor: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _extract_metrics_from_pdf(\n",
    "        self,\n",
    "        file_path: str,\n",
    "    ) -> Tuple[Dict[str, Dict[str, Any]], Dict[str, Any]]:\n",
    "        \"\"\"Main method to extract banking metrics from a PDF file.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to PDF file\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of extracted metrics with values and metadata\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting metrics extraction from {file_path}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            # Step 1: Find relevant pages in the document\n",
    "            relevant_pages, extracted_text = self._find_relevant_pages(file_path)\n",
    "\n",
    "            # Step 2: Create a smaller PDF with only relevant pages\n",
    "            input_path = Path(file_path)\n",
    "            subset_pdf_path = f\"{self.output_dir}/{input_path.stem}_subset.pdf\"\n",
    "            extracted_entities = []\n",
    "            if self._extract_subset_pdf(file_path, subset_pdf_path, relevant_pages):\n",
    "                # Step 3: Process the subset PDF with Document AI\n",
    "                extracted_entities = self._process_document_with_docai(subset_pdf_path)\n",
    "            else:\n",
    "                raise Exception(\"Failed to create subset PDF\")\n",
    "\n",
    "            # Step 4: Calculate metrics from extracted entities\n",
    "            metrics_results = self._calculate_metrics(extracted_entities)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.logger.info(\n",
    "                f\"Metrics extraction completed in {elapsed_time:.2f} seconds. \"\n",
    "                f\"Found {len(metrics_results)}/{len(self.target_metrics)} metrics.\",\n",
    "            )\n",
    "\n",
    "            return metrics_results, extracted_entities\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\n",
    "                f\"Error extracting metrics from {file_path}: {e}\",\n",
    "                exc_info=True,\n",
    "            )\n",
    "            return {}\n",
    "\n",
    "    def _create_metrics_summary(\n",
    "        self,\n",
    "        metrics: Dict[str, Dict[str, Any]],\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Create a DataFrame summary of metrics results.\n",
    "\n",
    "        Args:\n",
    "            metrics: Dictionary of calculated metrics with their metadata\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with metrics summary information\n",
    "        \"\"\"\n",
    "        summary_data = []\n",
    "        for name, metric_data in metrics.items():\n",
    "            row = {\n",
    "                \"metric\": name,\n",
    "                \"value\": metric_data.get(\"value\"),\n",
    "                \"formatted_value\": metric_data.get(\"formatted_value\", \"N/A\"),\n",
    "                \"in_typical_range\": metric_data.get(\"in_typical_range\", False),\n",
    "                \"description\": metric_data.get(\"description\", \"\"),\n",
    "            }\n",
    "            summary_data.append(row)\n",
    "\n",
    "        return pd.DataFrame(summary_data)\n",
    "\n",
    "    def _save_metrics_results(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        metrics: Dict[str, Dict[str, Any]],\n",
    "        entities: Dict[str, Any] = None,\n",
    "        output_dir: Path = None,\n",
    "    ) -> Dict[str, Path]:\n",
    "        \"\"\"Save extraction results to files.\n",
    "\n",
    "        Args:\n",
    "            file_path: Original file path\n",
    "            metrics: Dictionary of calculated metrics\n",
    "            entities: Dictionary of extracted entities (optional)\n",
    "            output_dir: Custom output directory (optional)\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping file types to their saved paths\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Saving results for {file_path}\")\n",
    "        saved_paths = {}\n",
    "\n",
    "        try:\n",
    "            file_name = Path(file_path).stem\n",
    "\n",
    "            # Use custom output directory if provided, otherwise use the default\n",
    "            if output_dir is None:\n",
    "                output_dir = self.output_dir / file_name\n",
    "\n",
    "            output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "            # Save metrics to JSON with full metadata\n",
    "            metrics_path = f\"{output_dir}/{file_name}_metrics.json\"\n",
    "            with open(metrics_path, \"w\") as f:\n",
    "                json.dump(metrics, f, indent=2)\n",
    "            saved_paths[\"metrics_json\"] = metrics_path\n",
    "\n",
    "            # Create and save metrics summary to CSV\n",
    "            if metrics:\n",
    "                summary_df = self._create_metrics_summary(metrics)\n",
    "                summary_path = f\"{output_dir}/{file_name}_metrics_summary.csv\"\n",
    "                summary_df.to_csv(summary_path, index=False)\n",
    "                saved_paths[\"metrics_summary_csv\"] = summary_path\n",
    "\n",
    "            # Save extracted entities to JSON if provided\n",
    "            if entities:\n",
    "                entities_path = f\"{output_dir}/{file_name}_entities.json\"\n",
    "                with open(entities_path, \"w\") as f:\n",
    "                    json.dump(entities, f, indent=2)\n",
    "                saved_paths[\"entities_json\"] = entities_path\n",
    "\n",
    "            self.logger.info(f\"Results saved to {output_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving results: {e}\", exc_info=True)\n",
    "\n",
    "        return saved_paths\n",
    "\n",
    "    def batch_process(self, input_path: str) -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
    "        \"\"\"Process multiple PDF files in batch.\n",
    "\n",
    "        Args:\n",
    "            input_path: Input dir of PDF files\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping file names to extracted metrics\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        results = {}\n",
    "        file_paths = []\n",
    "        entities = {}\n",
    "\n",
    "        # Process all files\n",
    "        for root, _, files in os.walk(input_path):\n",
    "            file_path = [os.path.join(root, f) for f in files if f.endswith(\".pdf\")]\n",
    "            if file_path:\n",
    "                file_paths.extend(file_path)\n",
    "            else:\n",
    "                self.logger.warning(f\"No PDF files found in {root}\")\n",
    "\n",
    "        self.logger.info(f\"Starting batch processing for {len(file_paths)} files\")\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                self.logger.info(f\"Processing file: {file_path}\")\n",
    "                file_start_time = time.time()\n",
    "\n",
    "                # Extract metrics from file\n",
    "                metrics, entities = self._extract_metrics_from_pdf(file_path)\n",
    "\n",
    "                # Store results\n",
    "                file_name = Path(file_path).stem\n",
    "                results[file_name] = metrics\n",
    "\n",
    "                # Save to database\n",
    "                self._save_to_database(file_name, metrics, entities)\n",
    "\n",
    "                file_elapsed_time = time.time() - file_start_time\n",
    "                self.logger.info(\n",
    "                    f\"Completed processing {file_path} in {file_elapsed_time:.2f} seconds\",\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(\n",
    "                    f\"Error processing file {file_path}: {e}\",\n",
    "                    exc_info=True,\n",
    "                )\n",
    "                file_name = Path(file_path).stem\n",
    "                results[file_name] = {}\n",
    "\n",
    "        # Create enhanced summary from database\n",
    "        self._save_enhanced_summary()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        self.logger.info(f\"Completed batch processing in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "KHBen0doAuAi",
    "outputId": "88c25ada-9ff9-43d9-f085-6c8907e652d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full-report-ubs-group-ag-consolidated-3q24': {'NPL_Ratio': {'value': 0.622780060897806,\n",
       "   'formatted_value': '0.62%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6633.0, 'total_loans': 1065063.0}},\n",
       "  'Coverage_Ratio': {'value': 36.650082918739635,\n",
       "   'formatted_value': '36.65%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2431.0,\n",
       "    'nonperforming_loans': 6633.0}},\n",
       "  'PCL': {'value': 121.0,\n",
       "   'formatted_value': '121 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 121.0}},\n",
       "  'CET1_Ratio': {'value': 14.289235082206472,\n",
       "   'formatted_value': '14.3%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 74213.0, 'risk_weighted_assets': 519363.0}},\n",
       "  'Texas_Ratio': {'value': 8.049073501037533,\n",
       "   'formatted_value': '8.05%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 6633.0,\n",
       "    'tangible_equity': 79976.0,\n",
       "    'loan_loss_allowance': 2431.0}}},\n",
       " 'full-report-ubs-group-ag-consolidated-3q23': {'NPL_Ratio': {'value': 0.504217469301629,\n",
       "   'formatted_value': '0.50%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 5582.0, 'total_loans': 1107062.0}},\n",
       "  'Coverage_Ratio': {'value': 36.599785023289144,\n",
       "   'formatted_value': '36.60%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2043.0,\n",
       "    'nonperforming_loans': 5582.0}},\n",
       "  'PCL': {'value': 306.0,\n",
       "   'formatted_value': '306 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 306.0}},\n",
       "  'CET1_Ratio': {'value': 14.380291715691568,\n",
       "   'formatted_value': '14.4%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 78587.0, 'risk_weighted_assets': 546491.0}},\n",
       "  'Texas_Ratio': {'value': 7.026952175938164,\n",
       "   'formatted_value': '7.03%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 5582.0,\n",
       "    'tangible_equity': 77394.0,\n",
       "    'loan_loss_allowance': 2043.0}}},\n",
       " 'full-report-ubs-group-consolidated-1q24': {'NPL_Ratio': {'value': 0.6447103612594844,\n",
       "   'formatted_value': '0.64%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 7038.0, 'total_loans': 1091653.0}},\n",
       "  'Coverage_Ratio': {'value': 31.869849389030975,\n",
       "   'formatted_value': '31.87%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2243.0,\n",
       "    'nonperforming_loans': 7038.0}},\n",
       "  'PCL': {'value': 106.0,\n",
       "   'formatted_value': '106 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 106.0}},\n",
       "  'CET1_Ratio': {'value': 14.844511309045528,\n",
       "   'formatted_value': '14.8%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 78147.0, 'risk_weighted_assets': 526437.0}},\n",
       "  'Texas_Ratio': {'value': 8.706516898411598,\n",
       "   'formatted_value': '8.71%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 7038.0,\n",
       "    'tangible_equity': 78593.0,\n",
       "    'loan_loss_allowance': 2243.0}}},\n",
       " 'full-report-ubs-group-ag-consolidated-2q23': {'NPL_Ratio': {'value': 0.3776655519709846,\n",
       "   'formatted_value': '0.38%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 2497.0, 'total_loans': 661167.0}},\n",
       "  'Coverage_Ratio': {'value': 44.893872647176615,\n",
       "   'formatted_value': '44.89%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 1121.0,\n",
       "    'nonperforming_loans': 2497.0}},\n",
       "  'PCL': {'value': 740.0,\n",
       "   'formatted_value': '740 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 740.0}},\n",
       "  'CET1_Ratio': {'value': 14.419253938624118,\n",
       "   'formatted_value': '14.4%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 80258.0, 'risk_weighted_assets': 556603.0}},\n",
       "  'Texas_Ratio': {'value': 4.838959730243014,\n",
       "   'formatted_value': '4.84%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 2497.0,\n",
       "    'tangible_equity': 50481.0,\n",
       "    'loan_loss_allowance': 1121.0}}},\n",
       " '88617d8a-a183-45a7-acd9-eea77b439879': {'NPL_Ratio': {'value': 0.6107737116616588,\n",
       "   'formatted_value': '0.61%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6895.0, 'total_loans': 1128896.0}},\n",
       "  'Coverage_Ratio': {'value': 290.8339376359681,\n",
       "   'formatted_value': '290.83%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 20053.0,\n",
       "    'nonperforming_loans': 6895.0}},\n",
       "  'PCL': {'value': 2275.0,\n",
       "   'formatted_value': '2275 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 2275.0}},\n",
       "  'CET1_Ratio': {'value': 13.76606501520593,\n",
       "   'formatted_value': '13.8%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 227142.0,\n",
       "    'risk_weighted_assets': 1650014.0}},\n",
       "  'Texas_Ratio': {'value': 3.038119591224081,\n",
       "   'formatted_value': '3.04%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 7418.0,\n",
       "    'tangible_equity': 224111.187,\n",
       "    'loan_loss_allowance': 20053.0}}},\n",
       " 'full-report-ubs-group-ag-consolidated-4q23': {'NPL_Ratio': {'value': 0.5397853236380531,\n",
       "   'formatted_value': '0.54%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6367.0, 'total_loans': 1179543.0}},\n",
       "  'Coverage_Ratio': {'value': 35.49552379456573,\n",
       "   'formatted_value': '35.50%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2260.0,\n",
       "    'nonperforming_loans': 6367.0}},\n",
       "  'PCL': {'value': 136.0,\n",
       "   'formatted_value': '136 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 136.0}},\n",
       "  'CET1_Ratio': {'value': 14.503618448138628,\n",
       "   'formatted_value': '14.5%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 79263.0, 'risk_weighted_assets': 546505.0}},\n",
       "  'Texas_Ratio': {'value': 7.761794465439473,\n",
       "   'formatted_value': '7.76%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 6367.0,\n",
       "    'tangible_equity': 79770.0,\n",
       "    'loan_loss_allowance': 2260.0}}},\n",
       " '393bfa53-d214-4230-8539-860a409b2107': {'NPL_Ratio': {'value': 0.5740199487198668,\n",
       "   'formatted_value': '0.57%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 7520.0, 'total_loans': 1310059.0}},\n",
       "  'Coverage_Ratio': {'value': 291.8351063829787,\n",
       "   'formatted_value': '291.84%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 21946.0,\n",
       "    'nonperforming_loans': 7520.0}},\n",
       "  'PCL': {'value': 1384.0,\n",
       "   'formatted_value': '1384 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 1384.0}},\n",
       "  'CET1_Ratio': {'value': 14.280273056890788,\n",
       "   'formatted_value': '14.3%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 241825.0,\n",
       "    'risk_weighted_assets': 1693420.0}},\n",
       "  'Texas_Ratio': {'value': 3.1378842933821085,\n",
       "   'formatted_value': '3.14%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 8131.0,\n",
       "    'tangible_equity': 237177.64,\n",
       "    'loan_loss_allowance': 21946.0}}},\n",
       " '6bca0e4a-703c-4fff-8e70-f026f015fee5': {'NPL_Ratio': {'value': 0.6026069935246801,\n",
       "   'formatted_value': '0.60%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 8075.0, 'total_loans': 1340011.0}},\n",
       "  'Coverage_Ratio': {'value': 296.58204334365325,\n",
       "   'formatted_value': '296.58%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 23949.0,\n",
       "    'nonperforming_loans': 8075.0}},\n",
       "  'PCL': {'value': 3111.0,\n",
       "   'formatted_value': '3111 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 3111.0}},\n",
       "  'CET1_Ratio': {'value': 15.303467649121469,\n",
       "   'formatted_value': '15.3%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 272988.0,\n",
       "    'risk_weighted_assets': 1783831.0}},\n",
       "  'Texas_Ratio': {'value': 2.920783141174712,\n",
       "   'formatted_value': '2.92%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 8628.0,\n",
       "    'tangible_equity': 271451.226,\n",
       "    'loan_loss_allowance': 23949.0}}},\n",
       " '16d9371e-30e9-4898-abf6-d1f7c86fd311': {'NPL_Ratio': {'value': 0.5225480582546276,\n",
       "   'formatted_value': '0.52%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6917.0, 'total_loans': 1323706.0}},\n",
       "  'Coverage_Ratio': {'value': 324.1289576405956,\n",
       "   'formatted_value': '324.13%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 22420.0,\n",
       "    'nonperforming_loans': 6917.0}},\n",
       "  'PCL': {'value': 2762.0,\n",
       "   'formatted_value': '2762 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 2762.0}},\n",
       "  'CET1_Ratio': {'value': 14.955828844006563,\n",
       "   'formatted_value': '15.0%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 250606.0,\n",
       "    'risk_weighted_assets': 1675641.0}},\n",
       "  'Texas_Ratio': {'value': 2.8132209133176316,\n",
       "   'formatted_value': '2.81%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 7597.0,\n",
       "    'tangible_equity': 247626.33599999998,\n",
       "    'loan_loss_allowance': 22420.0}}},\n",
       " '9387d4e9-a7dc-4613-822d-6848965485ee': {'NPL_Ratio': {'value': 0.5862023677169491,\n",
       "   'formatted_value': '0.59%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 7677.0, 'total_loans': 1309616.0}},\n",
       "  'Coverage_Ratio': {'value': 291.1423733229126,\n",
       "   'formatted_value': '291.14%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 22351.0,\n",
       "    'nonperforming_loans': 7677.0}},\n",
       "  'PCL': {'value': 1884.0,\n",
       "   'formatted_value': '1884 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 1884.0}},\n",
       "  'CET1_Ratio': {'value': 15.01345311345148,\n",
       "   'formatted_value': '15.0%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 257569.0,\n",
       "    'risk_weighted_assets': 1715588.0}},\n",
       "  'Texas_Ratio': {'value': 2.9914589990882945,\n",
       "   'formatted_value': '2.99%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 8265.0,\n",
       "    'tangible_equity': 253935.58800000002,\n",
       "    'loan_loss_allowance': 22351.0}}},\n",
       " '42092156-03a0-428c-9692-d7e844b063a1': {'NPL_Ratio': {'value': 0.6549019724211195,\n",
       "   'formatted_value': '0.65%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 8828.0, 'total_loans': 1347988.0}},\n",
       "  'Coverage_Ratio': {'value': 275.77027639329407,\n",
       "   'formatted_value': '275.77%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 24345.0,\n",
       "    'nonperforming_loans': 8828.0}},\n",
       "  'PCL': {'value': 2631.0,\n",
       "   'formatted_value': '2631 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 2631.0}},\n",
       "  'CET1_Ratio': {'value': 15.66162054556317,\n",
       "   'formatted_value': '15.7%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 275515.0,\n",
       "    'risk_weighted_assets': 1759173.0}},\n",
       "  'Texas_Ratio': {'value': 3.1330142071791385,\n",
       "   'formatted_value': '3.13%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 9291.0,\n",
       "    'tangible_equity': 272206.48,\n",
       "    'loan_loss_allowance': 24345.0}}},\n",
       " 'full-report-ubs-group-ag-consolidated-1q23': {'NPL_Ratio': {'value': 0.3776655519709846,\n",
       "   'formatted_value': '0.38%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 2497.0, 'total_loans': 661167.0}},\n",
       "  'Coverage_Ratio': {'value': 44.893872647176615,\n",
       "   'formatted_value': '44.89%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 1121.0,\n",
       "    'nonperforming_loans': 2497.0}},\n",
       "  'PCL': {'value': 38.0,\n",
       "   'formatted_value': '38 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 38.0}},\n",
       "  'CET1_Ratio': {'value': 13.862463470745507,\n",
       "   'formatted_value': '13.9%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 44590.0, 'risk_weighted_assets': 321660.0}},\n",
       "  'Texas_Ratio': {'value': 4.838959730243014,\n",
       "   'formatted_value': '4.84%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 2497.0,\n",
       "    'tangible_equity': 50481.0,\n",
       "    'loan_loss_allowance': 1121.0}}},\n",
       " 'full-report-ubs-group-consolidated-2q24': {'NPL_Ratio': {'value': 0.5956426485831773,\n",
       "   'formatted_value': '0.60%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6275.0, 'total_loans': 1053484.0}},\n",
       "  'Coverage_Ratio': {'value': 35.984063745019924,\n",
       "   'formatted_value': '35.98%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2258.0,\n",
       "    'nonperforming_loans': 6275.0}},\n",
       "  'PCL': {'value': 95.0,\n",
       "   'formatted_value': '95 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 95.0}},\n",
       "  'CET1_Ratio': {'value': 14.882200181471166,\n",
       "   'formatted_value': '14.9%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 76104.0, 'risk_weighted_assets': 511376.0}},\n",
       "  'Texas_Ratio': {'value': 7.980617591697614,\n",
       "   'formatted_value': '7.98%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 6275.0,\n",
       "    'tangible_equity': 76370.0,\n",
       "    'loan_loss_allowance': 2258.0}}},\n",
       " 'full-report-ubs-group-ag-consolidated-4q24': {'NPL_Ratio': {'value': 0.6622635503592708,\n",
       "   'formatted_value': '0.66%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 6637.0, 'total_loans': 1002169.0}},\n",
       "  'Coverage_Ratio': {'value': 37.773090251619706,\n",
       "   'formatted_value': '37.77%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 2507.0,\n",
       "    'nonperforming_loans': 6637.0}},\n",
       "  'PCL': {'value': 229.0,\n",
       "   'formatted_value': '229 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 229.0}},\n",
       "  'CET1_Ratio': {'value': 14.31525781384769,\n",
       "   'formatted_value': '14.3%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 71367.0, 'risk_weighted_assets': 498538.0}},\n",
       "  'Texas_Ratio': {'value': 8.224389397638136,\n",
       "   'formatted_value': '8.22%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 6637.0,\n",
       "    'tangible_equity': 78192.0,\n",
       "    'loan_loss_allowance': 2507.0}}},\n",
       " 'c9585f9b-75cc-4a49-b1ea-cec4423c87a7': {'NPL_Ratio': {'value': 0.559431845540506,\n",
       "   'formatted_value': '0.56%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 7273.0, 'total_loans': 1300069.0}},\n",
       "  'Coverage_Ratio': {'value': 302.21366698748795,\n",
       "   'formatted_value': '302.21%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 21980.0,\n",
       "    'nonperforming_loans': 7273.0}},\n",
       "  'PCL': {'value': 2899.0,\n",
       "   'formatted_value': '2899 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 2899.0}},\n",
       "  'CET1_Ratio': {'value': 13.785057108453454,\n",
       "   'formatted_value': '13.8%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 235832.0,\n",
       "    'risk_weighted_assets': 1710780.0}},\n",
       "  'Texas_Ratio': {'value': 3.0836731780116238,\n",
       "   'formatted_value': '3.08%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 7838.0,\n",
       "    'tangible_equity': 232197.39,\n",
       "    'loan_loss_allowance': 21980.0}}},\n",
       " '0c34d80d-4a60-46ad-9bb9-cfdb3a51c12d': {'NPL_Ratio': {'value': 0.5899144393124859,\n",
       "   'formatted_value': '0.59%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Non-Performing Loan Ratio measures the percentage of non-performing loans in the total loan portfolio.',\n",
       "   'calculation': 'Non-performing loans / Total loans * 100',\n",
       "   'components': {'nonperforming_loans': 7791.0, 'total_loans': 1320700.0}},\n",
       "  'Coverage_Ratio': {'value': 295.09690668720316,\n",
       "   'formatted_value': '295.10%',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Coverage Ratio shows how much of the non-performing loans are covered by allowances.',\n",
       "   'calculation': 'Loan loss allowance / Non-performing loans * 100',\n",
       "   'components': {'loan_loss_allowance': 22991.0,\n",
       "    'nonperforming_loans': 7791.0}},\n",
       "  'PCL': {'value': 3052.0,\n",
       "   'formatted_value': '3052 million',\n",
       "   'in_typical_range': True,\n",
       "   'description': 'Provision for Credit Losses for the current period.',\n",
       "   'calculation': 'Direct value from Provision for Credit Losses line item',\n",
       "   'components': {'provision_for_credit_losses': 3052.0}},\n",
       "  'CET1_Ratio': {'value': 15.333782872555629,\n",
       "   'formatted_value': '15.3%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Common Equity Tier 1 Capital Ratio, a measure of bank's core capital.\",\n",
       "   'calculation': 'Common Equity Tier 1 Capital / Risk Weighted Assets * 100',\n",
       "   'components': {'cet1_capital': 267195.0,\n",
       "    'risk_weighted_assets': 1742525.0}},\n",
       "  'Texas_Ratio': {'value': 2.935549711586162,\n",
       "   'formatted_value': '2.94%',\n",
       "   'in_typical_range': True,\n",
       "   'description': \"Texas Ratio measures a bank's credit problems relative to its capital resources.\",\n",
       "   'calculation': 'Non-performing assets / (Tangible equity + Loan loss allowance) * 100',\n",
       "   'components': {'nonperforming_assets': 8423.0,\n",
       "    'tangible_equity': 263939.92699999997,\n",
       "    'loan_loss_allowance': 22991.0}}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize extractor with batch processing capabilities\n",
    "metrics_calculator = BankingMetricsCalculator(\n",
    "    db=db,\n",
    "    project_id=DOCUMENT_AI_PROJECT_ID,\n",
    "    location=DOCUMENT_AI_LOCATION,\n",
    "    processor_id=DOCUMENT_AI_PROCESSOR_ID,\n",
    "    output_dir=DOCUMENT_AI_OUTPUT_DIR,\n",
    "    processor_version_id=DOCUMENT_AI_PROCESSOR_VERSION  # Optional\n",
    ")\n",
    "\n",
    "metrics_calculator.batch_process(QUARTERLY_REPORTS_DOWNLOAD_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJGaTOaoMtYG"
   },
   "source": [
    "## üí¨üìä Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OFgsgprPyK3"
   },
   "source": [
    "### üì¶ Dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "U6ruvWfWP1rC"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CreditMetric:\n",
    "    \"\"\"Credit risk metric definition with associated keywords and indicators\"\"\"\n",
    "\n",
    "    name: str\n",
    "    description: str\n",
    "    keywords: Set[str]\n",
    "    indicators: Set[str]\n",
    "    typical_range: tuple  # (min, max)\n",
    "    format_string: str = \"{:.2f}%\"  # Default format\n",
    "    is_percentage: bool = True\n",
    "\n",
    "    def format_value(self, value: float) -> str:\n",
    "        \"\"\"Format a metric value according to its defined format\"\"\"\n",
    "        if self.is_percentage and \"%\" not in self.format_string:\n",
    "            return f\"{self.format_string.format(value)}%\"\n",
    "        return self.format_string.format(value)\n",
    "\n",
    "    def is_in_range(self, value: float) -> bool:\n",
    "        \"\"\"Check if value is within typical range\"\"\"\n",
    "        min_val, max_val = self.typical_range\n",
    "        return min_val <= value <= max_val\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SentimentScore:\n",
    "    \"\"\"Sentiment analysis scores with enhanced properties\"\"\"\n",
    "\n",
    "    positive: float\n",
    "    negative: float\n",
    "    neutral: float\n",
    "    compound: float = field(default=0.0)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Calculate compound score if not provided\"\"\"\n",
    "        if self.compound == 0.0:\n",
    "            self.compound = self.positive - self.negative\n",
    "\n",
    "    @property\n",
    "    def dominant_sentiment(self) -> str:\n",
    "        \"\"\"Determine the dominant sentiment\"\"\"\n",
    "        scores = {\n",
    "            \"positive\": self.positive,\n",
    "            \"negative\": self.negative,\n",
    "            \"neutral\": self.neutral,\n",
    "        }\n",
    "        return max(scores.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    @property\n",
    "    def sentiment_label(self) -> str:\n",
    "        \"\"\"Get a human-readable sentiment label\"\"\"\n",
    "        if self.compound >= 0.05:\n",
    "            return \"Positive\"\n",
    "        if self.compound <= -0.05:\n",
    "            return \"Negative\"\n",
    "        return \"Neutral\"\n",
    "\n",
    "    @property\n",
    "    def sentiment_color(self) -> str:\n",
    "        \"\"\"Get a color code for visualization\"\"\"\n",
    "        if self.compound >= 0.05:\n",
    "            return \"#1f77b4\"  # Blue\n",
    "        if self.compound <= -0.05:\n",
    "            return \"#d62728\"  # Red\n",
    "        return \"#7f7f7f\"  # Gray\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Topic:\n",
    "    \"\"\"Topic model result with keywords and metadata\"\"\"\n",
    "\n",
    "    id: int\n",
    "    name: str\n",
    "    keywords: List[str]\n",
    "    category: str\n",
    "    probability: float = 0.0\n",
    "\n",
    "    @property\n",
    "    def is_credit_risk_related(self) -> bool:\n",
    "        \"\"\"Check if topic is related to credit risk\"\"\"\n",
    "        return self.category == \"CREDIT_RISK\"\n",
    "\n",
    "    @property\n",
    "    def primary_keyword(self) -> str:\n",
    "        \"\"\"Get the most representative keyword\"\"\"\n",
    "        return self.keywords[0] if self.keywords else \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CreditRiskMention:\n",
    "    \"\"\"Mention of a credit risk metric with context and sentiment\"\"\"\n",
    "\n",
    "    metric_type: str  # 'CET1', 'NPL_RATIO', etc.\n",
    "    mention_text: str\n",
    "    mention_count: int = 1\n",
    "    sentiment: Optional[SentimentScore] = None\n",
    "    keyword_used: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class QAPair:\n",
    "    \"\"\"Question-answer pair with metadata\"\"\"\n",
    "\n",
    "    question: str\n",
    "    answer: str\n",
    "    answer_speaker: str\n",
    "    answer_role: str\n",
    "    preprocessed: Dict[str, Dict[str, str]] = field(default_factory=dict)\n",
    "\n",
    "    def add_preprocessed(self, text_type: str, level: str, text: str):\n",
    "        \"\"\"Add preprocessed text\"\"\"\n",
    "        if text_type not in self.preprocessed:\n",
    "            self.preprocessed[text_type] = {}\n",
    "        self.preprocessed[text_type][level] = text\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnalystConversation:\n",
    "    \"\"\"Complete analyst conversation with Q&A pairs and analysis results\"\"\"\n",
    "\n",
    "    id: Optional[int] = None\n",
    "    analyst_name: str = \"\"\n",
    "    analyst_company: str = \"\"\n",
    "    qa_pairs: List[QAPair] = field(default_factory=list)\n",
    "    topic: Optional[Topic] = None\n",
    "    conversation_sentiment: Optional[SentimentScore] = None\n",
    "    speaker_sentiments: Dict[str, SentimentScore] = field(default_factory=dict)\n",
    "    credit_risk_mentions: List[CreditRiskMention] = field(default_factory=list)\n",
    "    vector_id: Optional[int] = None\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def combined_text(self) -> str:\n",
    "        \"\"\"Get combined text of all Q&A pairs\"\"\"\n",
    "        return \" \".join([f\"{qa.question} {qa.answer}\" for qa in self.qa_pairs])\n",
    "\n",
    "    @property\n",
    "    def has_credit_risk_mention(self) -> bool:\n",
    "        \"\"\"Check if conversation mentions credit risk\"\"\"\n",
    "        return len(self.credit_risk_mentions) > 0\n",
    "\n",
    "    @property\n",
    "    def speakers(self) -> List[str]:\n",
    "        \"\"\"Get unique list of speakers\"\"\"\n",
    "        return list(set([qa.answer_speaker for qa in self.qa_pairs]))\n",
    "\n",
    "    @property\n",
    "    def speaker_roles(self) -> Dict[str, str]:\n",
    "        \"\"\"Get mapping of speakers to roles\"\"\"\n",
    "        return {qa.answer_speaker: qa.answer_role for qa in self.qa_pairs}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReportAnalysis:\n",
    "    \"\"\"Analysis results for a quarterly earnings report\"\"\"\n",
    "\n",
    "    bank_name: str\n",
    "    year: int\n",
    "    quarter: str\n",
    "    credit_risk_score: Dict\n",
    "    report_date: Optional[datetime] = None\n",
    "    conversations: List[AnalystConversation] = field(default_factory=list)\n",
    "    topics: Dict[int, Topic] = field(default_factory=dict)\n",
    "    credit_metrics: Dict[str, float] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def period_label(self) -> str:\n",
    "        \"\"\"Get formatted period label\"\"\"\n",
    "        return f\"{self.year} {self.quarter}\"\n",
    "\n",
    "    @property\n",
    "    def overall_sentiment(self) -> SentimentScore:\n",
    "        \"\"\"Calculate overall sentiment across all conversations\"\"\"\n",
    "        if not self.conversations:\n",
    "            return SentimentScore(0.33, 0.33, 0.34, 0.0)\n",
    "\n",
    "        pos = sum(\n",
    "            c.conversation_sentiment.positive\n",
    "            for c in self.conversations\n",
    "            if c.conversation_sentiment\n",
    "        )\n",
    "        neg = sum(\n",
    "            c.conversation_sentiment.negative\n",
    "            for c in self.conversations\n",
    "            if c.conversation_sentiment\n",
    "        )\n",
    "        neu = sum(\n",
    "            c.conversation_sentiment.neutral\n",
    "            for c in self.conversations\n",
    "            if c.conversation_sentiment\n",
    "        )\n",
    "        count = sum(1 for c in self.conversations if c.conversation_sentiment)\n",
    "\n",
    "        if count == 0:\n",
    "            return SentimentScore(0.33, 0.33, 0.34, 0.0)\n",
    "\n",
    "        return SentimentScore(\n",
    "            positive=pos / count,\n",
    "            negative=neg / count,\n",
    "            neutral=neu / count,\n",
    "        )\n",
    "\n",
    "    def topic_mention_counts(self) -> Dict[str, int]:\n",
    "        \"\"\"Count mentions by topic\"\"\"\n",
    "        topic_counts = {}\n",
    "        for conv in self.conversations:\n",
    "            if conv.topic and conv.topic.name:\n",
    "                topic_counts[conv.topic.name] = topic_counts.get(conv.topic.name, 0) + 1\n",
    "        return topic_counts\n",
    "\n",
    "    def credit_metric_mentions(self) -> Dict[str, List[CreditRiskMention]]:\n",
    "        \"\"\"Group credit risk mentions by metric type\"\"\"\n",
    "        result = {}\n",
    "        for conv in self.conversations:\n",
    "            for mention in conv.credit_risk_mentions:\n",
    "                if mention.metric_type not in result:\n",
    "                    result[mention.metric_type] = []\n",
    "                result[mention.metric_type].append(mention)\n",
    "        return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComparativeAnalysis:\n",
    "    \"\"\"Cross-bank comparison analysis\"\"\"\n",
    "\n",
    "    period: str  # e.g., \"2024 Q1\"\n",
    "    banks: List[str]\n",
    "    topic_sentiments: Dict[str, Dict[str, float]] = field(default_factory=dict)\n",
    "    metric_values: Dict[str, Dict[str, float]] = field(default_factory=dict)\n",
    "\n",
    "    def add_topic_sentiment(self, bank: str, topic: str, sentiment: float):\n",
    "        \"\"\"Add sentiment data for a bank-topic pair\"\"\"\n",
    "        if topic not in self.topic_sentiments:\n",
    "            self.topic_sentiments[topic] = {}\n",
    "        self.topic_sentiments[topic][bank] = sentiment\n",
    "\n",
    "    def add_metric_value(self, bank: str, metric: str, value: float):\n",
    "        \"\"\"Add metric value for a bank-metric pair\"\"\"\n",
    "        if metric not in self.metric_values:\n",
    "            self.metric_values[metric] = {}\n",
    "        self.metric_values[metric][bank] = value\n",
    "\n",
    "    def sentiment_gap(self, bank1: str, bank2: str, topic: str) -> float:\n",
    "        \"\"\"Calculate sentiment gap between two banks for a topic\"\"\"\n",
    "        if topic not in self.topic_sentiments:\n",
    "            return 0.0\n",
    "\n",
    "        bank1_sentiment = self.topic_sentiments[topic].get(bank1, 0.0)\n",
    "        bank2_sentiment = self.topic_sentiments[topic].get(bank2, 0.0)\n",
    "\n",
    "        return bank1_sentiment - bank2_sentiment\n",
    "\n",
    "\n",
    "# Define our key credit metrics\n",
    "CREDIT_METRICS = {\n",
    "    \"CET1_RATIO\": CreditMetric(\n",
    "        name=\"CET1 Ratio\",\n",
    "        description=\"Common Equity Tier 1 Capital Ratio measures a bank's core equity capital against its risk-weighted assets.\",\n",
    "        keywords={\n",
    "            \"cet1\",\n",
    "            \"common equity tier 1\",\n",
    "            \"capital ratio\",\n",
    "            \"capital adequacy\",\n",
    "            \"tier 1\",\n",
    "            \"capital buffer\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"regulatory\",\n",
    "            \"requirements\",\n",
    "            \"target\",\n",
    "            \"minimum\",\n",
    "            \"comfortable\",\n",
    "            \"strong\",\n",
    "        },\n",
    "        typical_range=(10.0, 18.0),\n",
    "        format_string=\"{:.1f}\",\n",
    "    ),\n",
    "    \"NPL_RATIO\": CreditMetric(\n",
    "        name=\"NPL Ratio\",\n",
    "        description=\"Non-Performing Loan Ratio measures the percentage of loans that are in default or close to default.\",\n",
    "        keywords={\n",
    "            \"npl\",\n",
    "            \"non-performing loan\",\n",
    "            \"bad loan\",\n",
    "            \"impaired\",\n",
    "            \"delinquent\",\n",
    "            \"problem loan\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"increase\",\n",
    "            \"decrease\",\n",
    "            \"deteriorate\",\n",
    "            \"improve\",\n",
    "            \"stable\",\n",
    "            \"worsening\",\n",
    "        },\n",
    "        typical_range=(0.5, 5.0),\n",
    "        format_string=\"{:.2f}\",\n",
    "    ),\n",
    "    \"COVERAGE_RATIO\": CreditMetric(\n",
    "        name=\"Coverage Ratio\",\n",
    "        description=\"Loan Loss Coverage Ratio shows how much of the non-performing loans are covered by loan loss reserves.\",\n",
    "        keywords={\n",
    "            \"coverage\",\n",
    "            \"provision coverage\",\n",
    "            \"loan loss reserve\",\n",
    "            \"allowance\",\n",
    "            \"reserve\",\n",
    "        },\n",
    "        indicators={\"adequate\", \"sufficient\", \"conservative\", \"aggressive\", \"prudent\"},\n",
    "        typical_range=(60.0, 150.0),\n",
    "        format_string=\"{:.1f}\",\n",
    "    ),\n",
    "    \"TEXAS_RATIO\": CreditMetric(\n",
    "        name=\"Texas Ratio\",\n",
    "        description=\"Texas Ratio measures credit problems relative to a bank's capital. Higher values indicate greater risk.\",\n",
    "        keywords={\n",
    "            \"texas ratio\",\n",
    "            \"problem assets\",\n",
    "            \"capital basis\",\n",
    "            \"troubled\",\n",
    "            \"risk measure\",\n",
    "        },\n",
    "        indicators={\"concerning\", \"worrying\", \"manageable\", \"acceptable\", \"elevated\"},\n",
    "        typical_range=(0.0, 100.0),\n",
    "        format_string=\"{:.1f}\",\n",
    "    ),\n",
    "    \"PCL\": CreditMetric(\n",
    "        name=\"Provision for Credit Losses\",\n",
    "        description=\"PCL represents the amount set aside to cover expected loan losses.\",\n",
    "        keywords={\n",
    "            \"pcl\",\n",
    "            \"provision\",\n",
    "            \"loan loss\",\n",
    "            \"credit loss\",\n",
    "            \"impairment charge\",\n",
    "            \"charge-off\",\n",
    "        },\n",
    "        indicators={\"build\", \"release\", \"increase\", \"decrease\", \"reserve\", \"provision\"},\n",
    "        typical_range=(0.0, 5000.0),\n",
    "        format_string=\"${:.0f}M\",\n",
    "        is_percentage=False,\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RiskCategory:\n",
    "    name: str\n",
    "    metrics: Set[str]\n",
    "    keywords: Set[str]\n",
    "    indicators: Set[str]\n",
    "\n",
    "\n",
    "RISK_CATEGORIES = {\n",
    "    # Credit Risk: Deterioration in credit quality\n",
    "    \"CREDIT_RISK\": RiskCategory(\n",
    "        name=\"Credit Risk\",\n",
    "        metrics={\n",
    "            \"NPL\",\n",
    "            \"LLP\",\n",
    "            \"ECL\",\n",
    "            \"PD\",\n",
    "            \"LGD\",\n",
    "            \"EAD\",\n",
    "            \"CCR\",\n",
    "            \"CVA\",\n",
    "            \"CET1\",\n",
    "            \"Tier 1\",\n",
    "            \"RWA\",\n",
    "            \"IFRS 9\",\n",
    "            \"Stage 2\",\n",
    "            \"Stage 3\",\n",
    "            \"Coverage Ratio\",\n",
    "            \"Cost of Risk\",\n",
    "            \"Risk Density\",\n",
    "        },\n",
    "        keywords={\n",
    "            \"default\",\n",
    "            \"provision\",\n",
    "            \"exposure\",\n",
    "            \"collateral\",\n",
    "            \"lending\",\n",
    "            \"credit quality\",\n",
    "            \"restructuring\",\n",
    "            \"forbearance\",\n",
    "            \"writeoff\",\n",
    "            \"delinquency\",\n",
    "            \"impairment\",\n",
    "            \"recovery\",\n",
    "            \"bankruptcy\",\n",
    "            \"loan book\",\n",
    "            \"portfolio quality\",\n",
    "            \"underwriting\",\n",
    "            \"concentration\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"deteriorating\",\n",
    "            \"increasing\",\n",
    "            \"elevated\",\n",
    "            \"concerning\",\n",
    "            \"stress\",\n",
    "            \"weakness\",\n",
    "            \"adverse\",\n",
    "            \"negative\",\n",
    "            \"challenged\",\n",
    "            \"pressure\",\n",
    "            \"downgrade\",\n",
    "            \"migration\",\n",
    "            \"strain\",\n",
    "            \"heightened\",\n",
    "        },\n",
    "    ),\n",
    "    # Market Risk: Market volatility and uncertainty\n",
    "    \"MARKET_RISK\": RiskCategory(\n",
    "        name=\"Market Risk\",\n",
    "        metrics={\n",
    "            \"VaR\",\n",
    "            \"SVaR\",\n",
    "            \"IRC\",\n",
    "            \"CRM\",\n",
    "            \"DV01\",\n",
    "            \"PV01\",\n",
    "            \"Beta\",\n",
    "            \"Delta\",\n",
    "            \"Gamma\",\n",
    "            \"Vega\",\n",
    "            \"Theta\",\n",
    "            \"RNIV\",\n",
    "            \"NII\",\n",
    "            \"EVE\",\n",
    "            \"P&L\",\n",
    "            \"MTM\",\n",
    "        },\n",
    "        keywords={\n",
    "            \"trading\",\n",
    "            \"volatility\",\n",
    "            \"market value\",\n",
    "            \"hedging\",\n",
    "            \"position\",\n",
    "            \"liquidity\",\n",
    "            \"spread\",\n",
    "            \"interest rate\",\n",
    "            \"fx\",\n",
    "            \"commodity\",\n",
    "            \"equity\",\n",
    "            \"derivative\",\n",
    "            \"securities\",\n",
    "            \"bond\",\n",
    "            \"yield\",\n",
    "            \"trading book\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"volatile\",\n",
    "            \"unstable\",\n",
    "            \"uncertain\",\n",
    "            \"fluctuating\",\n",
    "            \"adverse\",\n",
    "            \"turbulent\",\n",
    "            \"stressed\",\n",
    "            \"illiquid\",\n",
    "            \"disruption\",\n",
    "            \"decline\",\n",
    "            \"sharp\",\n",
    "            \"rapid\",\n",
    "            \"extreme\",\n",
    "            \"severe\",\n",
    "        },\n",
    "    ),\n",
    "    # Liquidity Risk: Funding and liquidity pressures\n",
    "    \"LIQUIDITY_RISK\": RiskCategory(\n",
    "        name=\"Liquidity Risk\",\n",
    "        metrics={\n",
    "            \"LCR\",\n",
    "            \"NSFR\",\n",
    "            \"LDR\",\n",
    "            \"HQLA\",\n",
    "            \"CFP\",\n",
    "            \"MLR\",\n",
    "            \"Survival Period\",\n",
    "            \"Funding Gap\",\n",
    "            \"Buffer\",\n",
    "            \"Liquidity Coverage\",\n",
    "            \"Asset Encumbrance\",\n",
    "        },\n",
    "        keywords={\n",
    "            \"funding\",\n",
    "            \"deposit\",\n",
    "            \"withdrawal\",\n",
    "            \"cash flow\",\n",
    "            \"maturity\",\n",
    "            \"refinancing\",\n",
    "            \"encumbrance\",\n",
    "            \"contingency\",\n",
    "            \"intraday\",\n",
    "            \"wholesale\",\n",
    "            \"retail\",\n",
    "            \"interbank\",\n",
    "            \"funding mix\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"stress\",\n",
    "            \"shortage\",\n",
    "            \"constraint\",\n",
    "            \"tightening\",\n",
    "            \"outflow\",\n",
    "            \"drain\",\n",
    "            \"pressure\",\n",
    "            \"strain\",\n",
    "            \"mismatch\",\n",
    "            \"gap\",\n",
    "            \"dry up\",\n",
    "        },\n",
    "    ),\n",
    "    # Group Risk: Structural and interconnection concerns\n",
    "    \"GROUP_RISK\": RiskCategory(\n",
    "        name=\"Group Risk\",\n",
    "        metrics={\n",
    "            \"RoE\",\n",
    "            \"RoA\",\n",
    "            \"CIR\",\n",
    "            \"Capital Ratio\",\n",
    "            \"Leverage Ratio\",\n",
    "            \"MREL\",\n",
    "            \"TLAC\",\n",
    "            \"Combined Buffer\",\n",
    "            \"Group Solvency\",\n",
    "            \"Consolidated Capital\",\n",
    "        },\n",
    "        keywords={\n",
    "            \"solvency\",\n",
    "            \"capital adequacy\",\n",
    "            \"risk appetite\",\n",
    "            \"risk strategy\",\n",
    "            \"group structure\",\n",
    "            \"consolidation\",\n",
    "            \"interconnectedness\",\n",
    "            \"intragroup\",\n",
    "            \"cross-border\",\n",
    "            \"governance\",\n",
    "            \"group exposure\",\n",
    "            \"contagion\",\n",
    "            \"concentration\",\n",
    "            \"diversification\",\n",
    "        },\n",
    "        indicators={\n",
    "            \"systemic\",\n",
    "            \"material\",\n",
    "            \"significant\",\n",
    "            \"strategic\",\n",
    "            \"group-wide\",\n",
    "            \"consolidated\",\n",
    "            \"aggregate\",\n",
    "            \"cumulative\",\n",
    "            \"interconnected\",\n",
    "            \"concentrated\",\n",
    "            \"correlated\",\n",
    "        },\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydmU8qP4WQn8"
   },
   "source": [
    "### üìù Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PbwoeR_5WBIr"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, db: GSIBDatabase):\n",
    "        \"\"\"\n",
    "        Initialize text preprocessor with database connection\n",
    "\n",
    "        Args:\n",
    "            db: DB Manager\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(f\"Initializing {__name__}\")\n",
    "\n",
    "        self.db = db\n",
    "\n",
    "        # Download required NLTK data\n",
    "        self._download_nltk_data()\n",
    "\n",
    "        # Load spaCy model\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.logger.info(\"Loaded spaCy model: en_core_web_sm\")\n",
    "        except OSError:\n",
    "            self.logger.info(\"Downloading spaCy model: en_core_web_sm\")\n",
    "            import subprocess\n",
    "\n",
    "            subprocess.run(\n",
    "                [\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "            )\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        # Set up stopwords and financial terms\n",
    "        self._setup_stopwords_and_terms()\n",
    "\n",
    "    def _download_nltk_data(self):\n",
    "        \"\"\"Download required NLTK data\"\"\"\n",
    "        try:\n",
    "            nltk.download(\"punkt\", quiet=True)\n",
    "            nltk.download(\"punkt_tab\", quiet=True)\n",
    "            nltk.download(\"stopwords\", quiet=True)\n",
    "            self.logger.info(\"NLTK data downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error downloading NLTK data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _setup_stopwords_and_terms(self):\n",
    "        \"\"\"Set up stopwords and financial terms\"\"\"\n",
    "        # Base stopwords\n",
    "        self.base_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "        # Earnings call specific stopwords\n",
    "        self.earnings_call_stopwords = {\n",
    "            \"guidance\",\n",
    "            \"forward-looking\",\n",
    "            \"please\",\n",
    "            \"thank\",\n",
    "            \"you\",\n",
    "            \"statements\",\n",
    "            \"morning\",\n",
    "            \"good\",\n",
    "            \"call\",\n",
    "            \"question\",\n",
    "            \"questions\",\n",
    "            \"answer\",\n",
    "            \"answers\",\n",
    "            \"afternoon\",\n",
    "            \"basically\",\n",
    "            \"okay\",\n",
    "            \"thanks\",\n",
    "            \"sorry\",\n",
    "            \"hi\",\n",
    "            \"hello\",\n",
    "            \"think\",\n",
    "            \"hear\",\n",
    "            \"heard\",\n",
    "            \"like\",\n",
    "            \"liked\",\n",
    "        }\n",
    "\n",
    "        # Financial terms to preserve\n",
    "        self.financial_terms = {\n",
    "            \"equity\",\n",
    "            \"capital\",\n",
    "            \"increase\",\n",
    "            \"liabilities\",\n",
    "            \"interest\",\n",
    "            \"earnings\",\n",
    "            \"decrease\",\n",
    "            \"return\",\n",
    "            \"tier\",\n",
    "            \"stock\",\n",
    "            \"assets\",\n",
    "            \"loss\",\n",
    "            \"eps\",\n",
    "            \"trading\",\n",
    "            \"mortgage\",\n",
    "            \"investment\",\n",
    "            \"regulatory\",\n",
    "            \"compliance\",\n",
    "            \"volatility\",\n",
    "            \"expenses\",\n",
    "            \"growth\",\n",
    "            \"market\",\n",
    "            \"debt\",\n",
    "            \"decline\",\n",
    "            \"rate\",\n",
    "            \"dividend\",\n",
    "            \"revenue\",\n",
    "            \"margin\",\n",
    "            \"risk\",\n",
    "            \"deposit\",\n",
    "            \"profit\",\n",
    "            \"liability\",\n",
    "            \"asset\",\n",
    "            \"liquidity\",\n",
    "            \"credit\",\n",
    "            \"ratio\",\n",
    "            \"cash flow\",\n",
    "            \"loan\",\n",
    "            \"ai\",\n",
    "            # Add credit risk specific terms\n",
    "            \"npl\",\n",
    "            \"cet1\",\n",
    "            \"provision\",\n",
    "            \"coverage\",\n",
    "            \"texas\",\n",
    "            \"nonperforming\",\n",
    "            \"impaired\",\n",
    "            \"delinquent\",\n",
    "            \"reserve\",\n",
    "            \"allowance\",\n",
    "            \"charge-off\",\n",
    "        }\n",
    "\n",
    "        # Combine all stopwords, excluding financial terms\n",
    "        self.all_stopwords = (\n",
    "            self.base_stopwords.union(self.earnings_call_stopwords)\n",
    "            - self.financial_terms\n",
    "        )\n",
    "\n",
    "        # Common financial abbreviations mapping\n",
    "        self.abbreviations = {\n",
    "            \"CEO\": \"chief executive officer\",\n",
    "            \"CFO\": \"chief financial officer\",\n",
    "            \"CRO\": \"chief risk officer\",\n",
    "            \"ROE\": \"return on equity\",\n",
    "            \"ROI\": \"return on investment\",\n",
    "            \"YOY\": \"year over year\",\n",
    "            \"QOQ\": \"quarter over quarter\",\n",
    "            \"FY\": \"fiscal year\",\n",
    "            \"IBC\": \"investment banking\",\n",
    "            \"M&A\": \"mergers and acquisitions\",\n",
    "            \"DCM\": \"debt capital markets\",\n",
    "            \"NPL\": \"non performing loan\",\n",
    "            \"CET1\": \"common equity tier one\",\n",
    "            \"PCL\": \"provision for credit losses\",\n",
    "            \"LLP\": \"loan loss provision\",\n",
    "        }\n",
    "\n",
    "    def expand_abbreviations(self, text: str) -> str:\n",
    "        \"\"\"Expand common financial abbreviations\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        for abbr, full in self.abbreviations.items():\n",
    "            text = re.sub(rf\"\\b{abbr}\\b\", full, text, flags=re.IGNORECASE)\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text: str, remove_numbers: bool = True) -> str:\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Expand abbreviations\n",
    "        text = self.expand_abbreviations(text)\n",
    "\n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "        # Call artifacts\n",
    "        text = re.sub(r\"\\(ph\\)|\\{.*?\\}\", \" \", text)\n",
    "\n",
    "        # Remove numbers if specified\n",
    "        if remove_numbers:\n",
    "            text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(\n",
    "        self,\n",
    "        text: str,\n",
    "        custom_stopwords: Optional[Set[str]] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Remove stopwords with optional custom additions\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        # Combine stopwords\n",
    "        stopwords_to_use = self.all_stopwords\n",
    "        if custom_stopwords:\n",
    "            stopwords_to_use = stopwords_to_use.union(set(custom_stopwords))\n",
    "\n",
    "        # Tokenize and remove stopwords\n",
    "        words = word_tokenize(text)\n",
    "        filtered_words = [\n",
    "            word for word in words if word.lower() not in stopwords_to_use\n",
    "        ]\n",
    "\n",
    "        return \" \".join(filtered_words)\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text into words\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def lemmatize_with_spacy(self, text: str) -> str:\n",
    "        \"\"\"Lemmatize text using spaCy\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "\n",
    "        try:\n",
    "            doc = self.nlp(text)\n",
    "\n",
    "            # Extract lemmatized tokens, preserving entities and financial terms\n",
    "            processed_tokens = []\n",
    "            for token in doc:\n",
    "                if (\n",
    "                    token.text.lower() in self.financial_terms\n",
    "                    or token.ent_type_\n",
    "                    in {\"ORG\", \"MONEY\", \"PERCENT\", \"DATE\", \"CARDINAL\"}\n",
    "                    or (len(token.text) > 2 and token.ent_type_ != \"PERSON\")\n",
    "                ):  # Filter single/double character tokens\n",
    "                    processed_tokens.append(token.lemma_)\n",
    "\n",
    "            return \" \".join(processed_tokens)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in lemmatization: {str(e)}\")\n",
    "            return text\n",
    "\n",
    "    def preprocess_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        custom_stopwords: Optional[Set[str]] = None,\n",
    "        remove_numbers: bool = True,\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for earnings call text\n",
    "\n",
    "        Args:\n",
    "            text: Raw text to preprocess\n",
    "            custom_stopwords: Optional additional stopwords\n",
    "            remove_numbers: Whether to remove numeric values\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with different preprocessing levels\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return {\"raw\": \"\", \"cleaned\": \"\", \"tokenized\": \"\", \"lemmatized\": \"\"}\n",
    "\n",
    "        try:\n",
    "            # Store raw text\n",
    "            result = {\"raw\": text}\n",
    "\n",
    "            # Clean text\n",
    "            cleaned_text = self.clean_text(text, remove_numbers=remove_numbers)\n",
    "            result[\"cleaned\"] = cleaned_text\n",
    "\n",
    "            # Remove stopwords\n",
    "            no_stopwords = self.remove_stopwords(\n",
    "                cleaned_text,\n",
    "                custom_stopwords=custom_stopwords,\n",
    "            )\n",
    "\n",
    "            # Tokenize\n",
    "            tokens = self.tokenize(no_stopwords)\n",
    "            result[\"tokenized\"] = \" \".join(tokens)\n",
    "\n",
    "            # Lemmatize\n",
    "            lemmatized = self.lemmatize_with_spacy(no_stopwords)\n",
    "            result[\"lemmatized\"] = lemmatized\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error preprocessing text: {str(e)}\")\n",
    "            return {\"raw\": text, \"cleaned\": text, \"tokenized\": text, \"lemmatized\": text}\n",
    "\n",
    "    def process_qa_pairs_from_db(self, limit: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Pre-process raw QA pairs directly from the qa_pairs table\n",
    "\n",
    "        This method retrieves raw QA pairs and applies preprocessing steps\n",
    "        (tokenized, lemmatized) on both question and answer texts.\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of QA pairs to process\n",
    "        \"\"\"\n",
    "        # Get QA pairs needing processing\n",
    "        raw_qa_pairs = self.db.get_unprocessed_qa_pairs(limit)\n",
    "        if not raw_qa_pairs:\n",
    "            self.logger.info(\"No QA pairs found for preprocessing\")\n",
    "            return\n",
    "\n",
    "        self.logger.info(f\"Found {len(raw_qa_pairs)} QA pairs for preprocessing\")\n",
    "\n",
    "        def process_entries(conn):\n",
    "            # Process each QA pair\n",
    "            for qa_pair in tqdm(raw_qa_pairs, desc=\"Processing QA pairs\"):\n",
    "                qa_pair_id = qa_pair[\"id\"]\n",
    "                question_text = qa_pair[\"question\"]\n",
    "                answer_text = qa_pair[\"answer\"]\n",
    "\n",
    "                # Skip if texts are empty\n",
    "                if not question_text or not answer_text:\n",
    "                    self.logger.warning(\n",
    "                        f\"Skipping QA pair with empty text: {qa_pair_id}\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Preprocess question text with the full pipeline\n",
    "                question_results = self.preprocess_text(question_text)\n",
    "\n",
    "                # Preprocess answer text with the full pipeline\n",
    "                answer_results = self.preprocess_text(answer_text)\n",
    "\n",
    "                # Create and preprocess combined text\n",
    "                combined_text = f\"{question_text} {answer_text}\"\n",
    "                combined_results = self.preprocess_text(combined_text)\n",
    "\n",
    "                # Store all preprocessed texts\n",
    "                self._store_preprocessed_text(\n",
    "                    qa_pair_id,\n",
    "                    question_results,\n",
    "                    answer_results,\n",
    "                    combined_results,\n",
    "                )\n",
    "\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            # Execute the function within a transaction\n",
    "            self.db.execute_in_transaction(process_entries)\n",
    "            self.logger.info(\"Successfully completed preprocessing\")\n",
    "            # Checkpoint after successful processing\n",
    "            self.db.checkpoint_wal()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing QA pairs: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _store_preprocessed_text(\n",
    "        self,\n",
    "        qa_pair_id,\n",
    "        question_results,\n",
    "        answer_results,\n",
    "        combined_results,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Store preprocessed text in the database\n",
    "\n",
    "        Args:\n",
    "            qa_pair_id: ID of the QA pair\n",
    "            question_results: Dict with question preprocessing results\n",
    "            answer_results: Dict with answer preprocessing results\n",
    "            combined_results: Dict with combined preprocessing results\n",
    "        \"\"\"\n",
    "\n",
    "        def store_text(conn):\n",
    "            # We're only storing tokenized and lemmatized versions\n",
    "            preprocessing_levels = [\"tokenized\", \"lemmatized\"]\n",
    "\n",
    "            # Store question preprocessing results with metadata\n",
    "            for level in preprocessing_levels:\n",
    "                if level in question_results:\n",
    "                    text = question_results[level]\n",
    "                    metadata = {\n",
    "                        \"token_count\": len(text.split()),\n",
    "                        \"source\": \"raw_question\",\n",
    "                    }\n",
    "                    self.db.add_preprocessed_text(\n",
    "                        qa_pair_id,\n",
    "                        \"question\",\n",
    "                        level,\n",
    "                        text,\n",
    "                        metadata,\n",
    "                    )\n",
    "\n",
    "            # Store answer preprocessing results with metadata\n",
    "            for level in preprocessing_levels:\n",
    "                if level in answer_results:\n",
    "                    text = answer_results[level]\n",
    "                    metadata = {\n",
    "                        \"token_count\": len(text.split()),\n",
    "                        \"source\": \"raw_answer\",\n",
    "                    }\n",
    "                    self.db.add_preprocessed_text(\n",
    "                        qa_pair_id,\n",
    "                        \"answer\",\n",
    "                        level,\n",
    "                        text,\n",
    "                        metadata,\n",
    "                    )\n",
    "\n",
    "            # Store combined preprocessing results with metadata\n",
    "            for level in preprocessing_levels:\n",
    "                if level in combined_results:\n",
    "                    text = combined_results[level]\n",
    "                    metadata = {\n",
    "                        \"token_count\": len(text.split()),\n",
    "                        \"source\": \"combined_qa\",\n",
    "                    }\n",
    "                    self.db.add_preprocessed_text(\n",
    "                        qa_pair_id,\n",
    "                        \"combined\",\n",
    "                        level,\n",
    "                        text,\n",
    "                        metadata,\n",
    "                    )\n",
    "\n",
    "            return True\n",
    "\n",
    "        try:\n",
    "            # Execute the function within a transaction\n",
    "            self.db.execute_in_transaction(store_text)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing preprocessed text: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL8bbAleXKRA"
   },
   "source": [
    "### üî§ Text Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nS_ynGi-7XWd"
   },
   "outputs": [],
   "source": [
    "class TextEmbedder:\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "        Initialize text embedder with transformer model\n",
    "\n",
    "        Args:\n",
    "            model: Transformer model\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Model-specific configurations\n",
    "        model_configs = {\n",
    "            \"ProsusAI/finbert\": {\n",
    "                \"dimension\": 768,\n",
    "                \"normalize\": False,\n",
    "                \"pooling\": \"cls\",\n",
    "            },\n",
    "            \"yiyanghkust/finbert-tone\": {\n",
    "                \"dimension\": 768,\n",
    "                \"normalize\": False,\n",
    "                \"pooling\": \"cls\",\n",
    "            },\n",
    "            \"BAAI/bge-large-en-v1.5\": {\n",
    "                \"dimension\": 1024,\n",
    "                \"normalize\": True,\n",
    "                \"pooling\": \"mean\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.config = model_configs.get(\n",
    "            model_name,\n",
    "            {\"dimension\": 1024, \"normalize\": True, \"pooling\": \"mean\", \"batch_size\": 32},\n",
    "        )\n",
    "\n",
    "        # Initialize model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Generate embedding for a single text\n",
    "\n",
    "        Args:\n",
    "            text: Input text to embed\n",
    "\n",
    "        Returns:\n",
    "            Embedding as a list of floats\n",
    "        \"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.config[\"dimension\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "\n",
    "        # Apply pooling strategy\n",
    "        if self.config[\"pooling\"] == \"cls\":\n",
    "            embeddings = outputs.last_hidden_state[:, 0]\n",
    "        else:  # mean pooling\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = (\n",
    "                attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            )\n",
    "            embeddings = torch.sum(\n",
    "                token_embeddings * input_mask_expanded,\n",
    "                1,\n",
    "            ) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        # Normalize if required\n",
    "        if self.config[\"normalize\"]:\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "        return embeddings[0].cpu().numpy().tolist()\n",
    "\n",
    "    def __call__(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Allow direct calling of the object\n",
    "\n",
    "        Args:\n",
    "            text: Input text to embed\n",
    "\n",
    "        Returns:\n",
    "            Embedding as a list of floats\n",
    "        \"\"\"\n",
    "        return self.get_embedding(text)\n",
    "\n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a batch of texts\"\"\"\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.config[\"dimension\"]):\n",
    "            batch = texts[i : i + self.config[\"dimension\"]]\n",
    "            batch_embeddings = [self.get_embedding(text) for text in batch]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "        return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZBL9YzA7Hy4"
   },
   "source": [
    "### üíπ Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ng57mbPE7J29"
   },
   "outputs": [],
   "source": [
    "class TopicAnalyzer:\n",
    "    \"\"\"BERTopic based topic clustering for G-SIB quarterly announcement analysis\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db: GSIBDatabase,\n",
    "        embedder: TextEmbedder,\n",
    "        preprocessor=None,\n",
    "        sentiment_analyzer=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize topic analyzer with BERTopic model\n",
    "\n",
    "        Args:\n",
    "            db: GSIBDatabase instance\n",
    "            embedder: TextEmbedder instance\n",
    "            preprocessor: TextPreprocessor instance\n",
    "            sentiment_analyzer: SentimentAnalyser instance\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(f\"Initializing {__name__}\")\n",
    "\n",
    "        # Store database and components\n",
    "        self.db = db\n",
    "        self.embedder = embedder\n",
    "        self.preprocessor = preprocessor\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "\n",
    "        # Configure HDBSCAN parameters for more lenient clustering\n",
    "        hdbscan_params = {\n",
    "            \"min_cluster_size\": 2,  # Minimum size of clusters\n",
    "            \"min_samples\": 1,  # More lenient cluster assignment\n",
    "            \"metric\": \"euclidean\",\n",
    "            \"cluster_selection_method\": \"eom\",\n",
    "            \"prediction_data\": True,  # Enable prediction for new documents\n",
    "        }\n",
    "\n",
    "        # Configure UMAP parameters for better separation\n",
    "        umap_params = {\n",
    "            \"n_neighbors\": 3,  # Smaller local neighborhood\n",
    "            \"n_components\": 5,  # Reduced dimensions\n",
    "            \"min_dist\": 0.0,  # Allow tighter clusters\n",
    "            \"metric\": \"cosine\",  # Better for text embeddings\n",
    "        }\n",
    "\n",
    "        # Initialize BERTopic model\n",
    "        self.model = BERTopic(\n",
    "            embedding_model=embedder.model,  # Use the prepared embedding model\n",
    "            language=\"english\",\n",
    "            calculate_probabilities=True,\n",
    "            verbose=False,\n",
    "            # UMAP configuration\n",
    "            umap_model=UMAP(**umap_params),\n",
    "            # HDBSCAN configuration\n",
    "            hdbscan_model=HDBSCAN(**hdbscan_params),\n",
    "            # Additional parameters\n",
    "            nr_topics=\"auto\",  # Let model determine optimal number\n",
    "            top_n_words=20,  # More words per topic\n",
    "        )\n",
    "\n",
    "        # Generate risk category embeddings for similarity comparison\n",
    "        self._generate_risk_embeddings()\n",
    "\n",
    "    def _generate_risk_embeddings(self):\n",
    "        \"\"\"Generate embeddings for risk categories for similarity comparison\"\"\"\n",
    "        self.risk_embeddings = {}\n",
    "\n",
    "        for category_id, category in RISK_CATEGORIES.items():\n",
    "            # Create a rich context description for the category\n",
    "            context = (\n",
    "                f\"{category.name}: \"\n",
    "                f\"Key metrics: {', '.join(sorted(list(category.metrics))[:10])}. \"\n",
    "                f\"Keywords: {', '.join(sorted(list(category.keywords))[:10])}. \"\n",
    "                f\"Indicators: {', '.join(sorted(list(category.indicators))[:10])}\"\n",
    "            )\n",
    "\n",
    "            # Generate embedding\n",
    "            embedding = self.embedder.embed_batch([context])[0]\n",
    "\n",
    "            # Store embedding\n",
    "            self.risk_embeddings[category_id] = embedding\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Generated embeddings for {len(self.risk_embeddings)} risk categories\",\n",
    "        )\n",
    "\n",
    "    def analyze_topics(self, limit: Optional[int] = None, new_only: bool = True):\n",
    "        \"\"\"\n",
    "        Analyze topics in conversations\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations to analyze\n",
    "            new_only: Only analyze conversations without existing topic analysis\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping conversation IDs to topic information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get conversations to analyze using the database method\n",
    "            conversations = self.db.get_conversations_to_analyze(\n",
    "                limit=limit,\n",
    "                new_only=new_only,\n",
    "            )\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Found {len(conversations)} conversations for topic clustering\",\n",
    "            )\n",
    "\n",
    "            # Get texts for each conversation\n",
    "            texts = []\n",
    "            conv_ids = []\n",
    "            conv_metadata = []\n",
    "\n",
    "            for conversation in tqdm(\n",
    "                conversations,\n",
    "                desc=\"Clustering conversation topics\",\n",
    "            ):\n",
    "                conversation_id = conversation[\"id\"]\n",
    "\n",
    "                # Get lemmatized text using preprocessor\n",
    "                text = self.db.get_conversation_text(\n",
    "                    conversation_id,\n",
    "                    level=\"lemmatized\",\n",
    "                )\n",
    "\n",
    "                if not text:\n",
    "                    self.logger.warning(\n",
    "                        f\"No text found for conversation {conversation_id}\",\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                texts.append(text)\n",
    "                conv_ids.append(conversation_id)\n",
    "\n",
    "                # Get metadata for this conversation\n",
    "                metadata = self.db.get_conversation_metadata(conversation_id)\n",
    "                if metadata:\n",
    "                    conv_metadata.append(\n",
    "                        {\n",
    "                            \"bank\": metadata[\"bank_name\"],\n",
    "                            \"year\": metadata[\"year\"],\n",
    "                            \"quarter\": metadata[\"quarter\"],\n",
    "                            \"analyst_name\": metadata[\"analyst_name\"],\n",
    "                            \"analyst_company\": metadata[\"analyst_company\"],\n",
    "                        },\n",
    "                    )\n",
    "                else:\n",
    "                    conv_metadata.append({})\n",
    "\n",
    "            if not texts:\n",
    "                self.logger.warning(\"No texts found for topic analysis\")\n",
    "                return {}\n",
    "\n",
    "            self.logger.info(f\"Running topic modeling on {len(texts)} conversations\")\n",
    "\n",
    "            # Run BERTopic\n",
    "            topics, probs = self.model.fit_transform(texts)\n",
    "\n",
    "            # Process results\n",
    "            topic_info = self.model.get_topic_info()\n",
    "\n",
    "            # Store results using data classes\n",
    "            results = {}\n",
    "            for i, (conv_id, topic_id, prob) in enumerate(zip(conv_ids, topics, probs)):\n",
    "                # Skip outlier topic\n",
    "                if topic_id == -1:\n",
    "                    continue\n",
    "\n",
    "                # Get top words for this topic\n",
    "                topic_words = self.model.get_topic(topic_id)\n",
    "                if not topic_words:\n",
    "                    continue\n",
    "\n",
    "                # Create a topic embedding for risk category classification\n",
    "                topic_text = \" \".join([word for word, _ in topic_words[:10]])\n",
    "                topic_embedding = self.embedder.embed_batch([topic_text])[0]\n",
    "\n",
    "                # Determine topic category\n",
    "                topic_category = self._determine_topic_category(\n",
    "                    topic_embedding,\n",
    "                    topic_words,\n",
    "                )\n",
    "\n",
    "                # Generate topic name\n",
    "                topic_name = self._generate_topic_name(topic_words, topic_category)\n",
    "\n",
    "                # Create Topic dataclass instance\n",
    "                topic_obj = Topic(\n",
    "                    id=int(topic_id),\n",
    "                    name=topic_name,\n",
    "                    keywords=[word for word, _ in topic_words[:10]],\n",
    "                    category=topic_category,\n",
    "                    probability=float(prob)\n",
    "                    if isinstance(prob, (int, float))\n",
    "                    else float(max(prob)),\n",
    "                )\n",
    "\n",
    "                # Store in database\n",
    "                self.db.update_conversation_topic(\n",
    "                    conversation_id=conv_id,\n",
    "                    topic_id=topic_obj.id,\n",
    "                    topic_probability=topic_obj.probability,\n",
    "                )\n",
    "\n",
    "                # Add or update topic in topics table\n",
    "                self.db.add_or_update_topic(\n",
    "                    topic_id=topic_obj.id,\n",
    "                    name=topic_obj.name,\n",
    "                    keywords=topic_obj.keywords,\n",
    "                    category=topic_obj.category,\n",
    "                )\n",
    "\n",
    "                # Add sentiment analysis if available\n",
    "                if self.sentiment_analyzer and topic_category.startswith(\n",
    "                    \"CREDIT_RISK:\",\n",
    "                ):\n",
    "                    # Get sample text for sentiment analysis\n",
    "                    topic_sample = \" \".join([word for word, _ in topic_words[:5]])\n",
    "                    sentiment = self.sentiment_analyzer.get_sentiment_for_text(\n",
    "                        topic_sample,\n",
    "                    )\n",
    "\n",
    "                    # Add to results\n",
    "                    results[conv_id] = {\n",
    "                        \"topic\": topic_obj,\n",
    "                        \"sentiment\": sentiment,\n",
    "                        \"metadata\": conv_metadata[i] if i < len(conv_metadata) else {},\n",
    "                    }\n",
    "                else:\n",
    "                    results[conv_id] = {\n",
    "                        \"topic\": topic_obj,\n",
    "                        \"metadata\": conv_metadata[i] if i < len(conv_metadata) else {},\n",
    "                    }\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Completed topic analysis for {len(results)} conversations\",\n",
    "            )\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing topics: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _determine_topic_category(self, topic_embedding, topic_words):\n",
    "        \"\"\"Determine topic category using embedding similarity with risk categories\"\"\"\n",
    "        # Calculate similarity with risk categories\n",
    "        similarities = {\n",
    "            risk: cosine_similarity([topic_embedding], [embedding])[0][0]\n",
    "            for risk, embedding in self.risk_embeddings.items()\n",
    "        }\n",
    "\n",
    "        # Find best match\n",
    "        best_match = max(similarities, key=similarities.get)\n",
    "        confidence = similarities[best_match]\n",
    "\n",
    "        # If confidence is high enough, use this category\n",
    "        if confidence > 0.5:\n",
    "            # For credit risk, check if we can identify subcategory\n",
    "            if best_match == \"CREDIT_RISK\":\n",
    "                # Extract metrics from CREDIT_METRICS\n",
    "                subcategory_scores = {}\n",
    "                words = [word.lower() for word, _ in topic_words]\n",
    "\n",
    "                for metric_key, metric in CREDIT_METRICS.items():\n",
    "                    metric_score = 0\n",
    "                    # Check keywords\n",
    "                    for keyword in metric.keywords:\n",
    "                        for word in words:\n",
    "                            if keyword.lower() in word or word in keyword.lower():\n",
    "                                metric_score += 1\n",
    "\n",
    "                    # Check indicators\n",
    "                    for indicator in metric.indicators:\n",
    "                        for word in words:\n",
    "                            if indicator.lower() in word or word in indicator.lower():\n",
    "                                metric_score += 0.5\n",
    "\n",
    "                    if metric_score > 0:\n",
    "                        subcategory_scores[metric.name] = metric_score\n",
    "\n",
    "                # If subcategory found, add it to result\n",
    "                if subcategory_scores:\n",
    "                    subcategory = max(subcategory_scores, key=subcategory_scores.get)\n",
    "                    return f\"CREDIT_RISK:{subcategory}\"\n",
    "\n",
    "            return best_match\n",
    "\n",
    "        # Fall back to keyword matching for low confidence cases\n",
    "        return self._determine_topic_category_by_keywords(topic_words)\n",
    "\n",
    "    def _determine_topic_category_by_keywords(self, topic_words):\n",
    "        \"\"\"Determine topic category using keyword matching\"\"\"\n",
    "        # Extract words from topic\n",
    "        words = [word.lower() for word, _ in topic_words]\n",
    "\n",
    "        # Calculate risk category scores\n",
    "        category_scores = {}\n",
    "\n",
    "        # Check against risk categories\n",
    "        for category_id, category in RISK_CATEGORIES.items():\n",
    "            # Check for metrics\n",
    "            metric_matches = sum(\n",
    "                1\n",
    "                for word in words\n",
    "                for metric in category.metrics\n",
    "                if metric.lower() in word or word in metric.lower()\n",
    "            )\n",
    "\n",
    "            # Check for keywords\n",
    "            keyword_matches = sum(\n",
    "                1\n",
    "                for word in words\n",
    "                for keyword in category.keywords\n",
    "                if keyword.lower() in word or word in keyword.lower()\n",
    "            )\n",
    "\n",
    "            # Check for indicators\n",
    "            indicator_matches = sum(\n",
    "                1\n",
    "                for word in words\n",
    "                for indicator in category.indicators\n",
    "                if indicator.lower() in word or word in indicator.lower()\n",
    "            )\n",
    "\n",
    "            # Calculate weighted score (metrics are most important)\n",
    "            score = (\n",
    "                (metric_matches * 1.5)\n",
    "                + (keyword_matches * 1.0)\n",
    "                + (indicator_matches * 0.5)\n",
    "            )\n",
    "\n",
    "            if score > 0:\n",
    "                category_scores[category_id] = score\n",
    "\n",
    "        # Find category with highest score\n",
    "        if category_scores:\n",
    "            max_category = max(category_scores, key=category_scores.get)\n",
    "            if category_scores[max_category] > 1.0:\n",
    "                return max_category\n",
    "\n",
    "        return \"OTHER\"  # Default category if no clear match\n",
    "\n",
    "    def _generate_topic_name(\n",
    "        self,\n",
    "        topic_words: List[Tuple[str, float]],\n",
    "        category: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Generate a descriptive name for a topic based on top words and category\"\"\"\n",
    "        # Get top 3 words\n",
    "        top_words = [word for word, _ in topic_words[:3]]\n",
    "\n",
    "        # Check if it's a credit risk topic\n",
    "        if category.startswith(\"CREDIT_RISK:\"):\n",
    "            subcategory = category.split(\":\")[1]\n",
    "            return f\"Credit Risk: {subcategory} ({', '.join(top_words)})\"\n",
    "\n",
    "        # For other categories\n",
    "        if \":\" in category:\n",
    "            main_category, subcategory = category.split(\":\")\n",
    "            return f\"{main_category}: {subcategory} ({', '.join(top_words)})\"\n",
    "\n",
    "        return f\"{category}: {', '.join(top_words)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGPvgl-h65TH"
   },
   "source": [
    "### üòäüòê‚òπÔ∏è Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rsWvhF4l67C1"
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyser:\n",
    "    \"\"\"FinBERT based sentiment analysis for G-SIB earnings call QA pairs\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db: GSIBDatabase,\n",
    "        model_name=\"yiyanghkust/finbert-tone\",\n",
    "        preprocessor=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment analyzer with FinBERT or similar financial NLP model\n",
    "\n",
    "        Args:\n",
    "            db: GSIBDatabase instance\n",
    "            model_name: HuggingFace model name (default: yiyanghkust/finbert-tone)\n",
    "            preprocessor: TextPreprocessor instance\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(f\"Initializing {__name__} with model {model_name}\")\n",
    "\n",
    "        # Store database and components\n",
    "        self.db = db\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "        # Initialize model and tokenizer\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "            self.model.to(device)\n",
    "\n",
    "            # Cache for sentiment analysis to avoid repeated computations\n",
    "            self.sentiment_cache = {}\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_qa_pairs(self, limit: Optional[int] = None) -> Dict[int, Dict]:\n",
    "        \"\"\"\n",
    "        Analyze sentiment for all QA pairs in the database\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of QA pairs to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping conversation IDs to sentiment analysis results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get conversations that need sentiment analysis\n",
    "            conversations = self.db.get_conversations_for_sentiment_analysis(\n",
    "                limit=limit,\n",
    "            )\n",
    "            if not conversations:\n",
    "                self.logger.info(\"No conversations found for sentiment analysis\")\n",
    "                return {}\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Found {len(conversations)} conversations for sentiment analysis\",\n",
    "            )\n",
    "\n",
    "            # Analyze sentiment for each conversation\n",
    "            results = {}\n",
    "            for conversation in tqdm(conversations, desc=\"Analyzing sentiment\"):\n",
    "                conversation_id = conversation[\"conversation_id\"]\n",
    "\n",
    "                # Get metadata\n",
    "                metadata = {\n",
    "                    \"bank\": conversation.get(\"bank_name\", \"\"),\n",
    "                    \"year\": conversation.get(\"year\", \"\"),\n",
    "                    \"quarter\": conversation.get(\"quarter\", \"\"),\n",
    "                    \"analyst_name\": conversation.get(\"analyst_name\", \"\"),\n",
    "                    \"analyst_company\": conversation.get(\"analyst_company\", \"\"),\n",
    "                }\n",
    "\n",
    "                # Get QA pairs for this conversation\n",
    "                qa_pairs = self.db.get_qa_pairs(conversation_id)\n",
    "\n",
    "                if not qa_pairs:\n",
    "                    self.logger.warning(\n",
    "                        f\"No QA pairs found for conversation {conversation_id}\",\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # Get full conversation text for overall sentiment using preprocessor\n",
    "                conversation_text = self.db.get_conversation_text(\n",
    "                    conversation_id,\n",
    "                    level=\"lemmatized\",\n",
    "                )\n",
    "                if not conversation_text:\n",
    "                    # Fallback to combining all QA pairs\n",
    "                    conversation_text = \" \".join(\n",
    "                        [f\"{qa['question']} {qa['answer']}\" for qa in qa_pairs],\n",
    "                    )\n",
    "\n",
    "                # Get overall conversation sentiment\n",
    "                conversation_sentiment_scores = self._get_sentiment(conversation_text)\n",
    "                conversation_sentiment = SentimentScore(\n",
    "                    positive=conversation_sentiment_scores[\"positive\"],\n",
    "                    negative=conversation_sentiment_scores[\"negative\"],\n",
    "                    neutral=conversation_sentiment_scores[\"neutral\"],\n",
    "                    compound=conversation_sentiment_scores[\"compound\"],\n",
    "                )\n",
    "\n",
    "                # Analyze individual QA pairs\n",
    "                qa_sentiments = []\n",
    "                for qa_pair in qa_pairs:\n",
    "                    # Get individual sentiments\n",
    "                    question_sentiment_scores = self._get_sentiment(qa_pair[\"question\"])\n",
    "                    answer_sentiment_scores = self._get_sentiment(qa_pair[\"answer\"])\n",
    "\n",
    "                    # Create SentimentScore objects\n",
    "                    question_sentiment = SentimentScore(\n",
    "                        positive=question_sentiment_scores[\"positive\"],\n",
    "                        negative=question_sentiment_scores[\"negative\"],\n",
    "                        neutral=question_sentiment_scores[\"neutral\"],\n",
    "                        compound=question_sentiment_scores[\"compound\"],\n",
    "                    )\n",
    "\n",
    "                    answer_sentiment = SentimentScore(\n",
    "                        positive=answer_sentiment_scores[\"positive\"],\n",
    "                        negative=answer_sentiment_scores[\"negative\"],\n",
    "                        neutral=answer_sentiment_scores[\"neutral\"],\n",
    "                        compound=answer_sentiment_scores[\"compound\"],\n",
    "                    )\n",
    "\n",
    "                    # Calculate combined sentiment\n",
    "                    combined_sentiment = SentimentScore(\n",
    "                        positive=(\n",
    "                            question_sentiment.positive + answer_sentiment.positive\n",
    "                        )\n",
    "                        / 2,\n",
    "                        negative=(\n",
    "                            question_sentiment.negative + answer_sentiment.negative\n",
    "                        )\n",
    "                        / 2,\n",
    "                        neutral=(question_sentiment.neutral + answer_sentiment.neutral)\n",
    "                        / 2,\n",
    "                        compound=(\n",
    "                            question_sentiment.compound + answer_sentiment.compound\n",
    "                        )\n",
    "                        / 2,\n",
    "                    )\n",
    "\n",
    "                    # Store QA pair sentiment\n",
    "                    qa_sentiments.append(\n",
    "                        {\n",
    "                            \"qa_pair_id\": qa_pair[\"id\"],\n",
    "                            \"question_sentiment\": question_sentiment,\n",
    "                            \"answer_sentiment\": answer_sentiment,\n",
    "                            \"combined_sentiment\": combined_sentiment,\n",
    "                            \"speaker\": qa_pair[\"answer_speaker\"],\n",
    "                            \"role\": qa_pair[\"answer_role\"],\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "                # Get all unique speakers\n",
    "                speakers = self.db.get_all_conversation_speakers(conversation_id)\n",
    "\n",
    "                # Calculate speaker-level sentiment\n",
    "                speaker_sentiments = self._calculate_speaker_sentiments(qa_sentiments)\n",
    "\n",
    "                # Store results\n",
    "                results[conversation_id] = {\n",
    "                    \"qa_pairs\": qa_sentiments,\n",
    "                    \"conversation\": conversation_sentiment,\n",
    "                    \"speakers\": speaker_sentiments,\n",
    "                    \"metadata\": metadata,\n",
    "                }\n",
    "\n",
    "                # Store results in database\n",
    "                self._store_sentiment_results(\n",
    "                    conversation_id,\n",
    "                    qa_sentiments,\n",
    "                    conversation_sentiment,\n",
    "                    speaker_sentiments,\n",
    "                )\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing sentiment: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _get_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get sentiment scores for a single text using FinBERT\n",
    "\n",
    "        Args:\n",
    "            text: Input text to analyze\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with positive, negative, neutral, and compound sentiment scores\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        cache_key = hash(text[:1000])  # Use first 1000 chars to create key\n",
    "        if cache_key in self.sentiment_cache:\n",
    "            return self.sentiment_cache[cache_key]\n",
    "\n",
    "        try:\n",
    "            # Truncate text if too long (FinBERT has 512 token limit)\n",
    "            if len(text) > 5000:\n",
    "                text = text[:5000]\n",
    "\n",
    "            # Tokenize and get sentiment\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                scores = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "            result = {\n",
    "                \"positive\": float(scores[0][0]),\n",
    "                \"negative\": float(scores[0][1]),\n",
    "                \"neutral\": float(scores[0][2]),\n",
    "                \"compound\": float(scores[0][0] - scores[0][1]),  # positive - negative\n",
    "            }\n",
    "\n",
    "            # Cache result\n",
    "            self.sentiment_cache[cache_key] = result\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting sentiment: {str(e)}\")\n",
    "            return {\n",
    "                \"positive\": 0.33,\n",
    "                \"negative\": 0.33,\n",
    "                \"neutral\": 0.34,\n",
    "                \"compound\": 0.0,\n",
    "            }\n",
    "\n",
    "    def _calculate_speaker_sentiments(\n",
    "        self,\n",
    "        qa_sentiments: List[Dict],\n",
    "    ) -> Dict[str, Dict]:\n",
    "        \"\"\"\n",
    "        Calculate sentiment scores for each speaker\n",
    "\n",
    "        Args:\n",
    "            qa_sentiments: List of QA pair sentiment results\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping speaker names to sentiment scores\n",
    "        \"\"\"\n",
    "        speaker_sentiments = {}\n",
    "\n",
    "        # Group sentiments by speaker\n",
    "        for qa in qa_sentiments:\n",
    "            speaker = qa[\"speaker\"]\n",
    "            role = qa[\"role\"]\n",
    "            key = f\"{speaker}|{role}\"\n",
    "\n",
    "            if key not in speaker_sentiments:\n",
    "                speaker_sentiments[key] = {\n",
    "                    \"answer_texts\": [],\n",
    "                    \"name\": speaker,\n",
    "                    \"role\": role,\n",
    "                }\n",
    "\n",
    "            # Get the answer text from the QA pair\n",
    "            qa_pair_id = qa[\"qa_pair_id\"]\n",
    "            answer = self.db.get_qa_answer(qa_pair_id)\n",
    "\n",
    "            if answer:\n",
    "                speaker_sentiments[key][\"answer_texts\"].append(answer)\n",
    "\n",
    "        # Calculate sentiment for each speaker using full text\n",
    "        for key, data in speaker_sentiments.items():\n",
    "            if data[\"answer_texts\"]:\n",
    "                # Combine all answers from this speaker\n",
    "                full_text = \" \".join(data[\"answer_texts\"])\n",
    "\n",
    "                # Get sentiment for combined text\n",
    "                sentiment_scores = self._get_sentiment(full_text)\n",
    "                sentiment = SentimentScore(\n",
    "                    positive=sentiment_scores[\"positive\"],\n",
    "                    negative=sentiment_scores[\"negative\"],\n",
    "                    neutral=sentiment_scores[\"neutral\"],\n",
    "                    compound=sentiment_scores[\"compound\"],\n",
    "                )\n",
    "\n",
    "                speaker_sentiments[key][\"sentiment\"] = sentiment\n",
    "                del speaker_sentiments[key][\"answer_texts\"]  # Remove texts after use\n",
    "            else:\n",
    "                # Default sentiment if no texts\n",
    "                speaker_sentiments[key][\"sentiment\"] = SentimentScore(\n",
    "                    0.33,\n",
    "                    0.33,\n",
    "                    0.34,\n",
    "                    0.0,\n",
    "                )\n",
    "\n",
    "        return speaker_sentiments\n",
    "\n",
    "    def _store_sentiment_results(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        qa_sentiments: List[Dict],\n",
    "        conversation_sentiment: SentimentScore,\n",
    "        speaker_sentiments: Dict[str, Dict],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Store sentiment analysis results in the database\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "            qa_sentiments: List of QA pair sentiment results\n",
    "            conversation_sentiment: Conversation-level sentiment scores\n",
    "            speaker_sentiments: Speaker-level sentiment scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Store conversation-level sentiment\n",
    "            self.db.add_conversation_sentiment(\n",
    "                conversation_id=conversation_id,\n",
    "                positive_score=conversation_sentiment.positive,\n",
    "                negative_score=conversation_sentiment.negative,\n",
    "                neutral_score=conversation_sentiment.neutral,\n",
    "                compound_score=conversation_sentiment.compound,\n",
    "            )\n",
    "\n",
    "            # Store speaker-level sentiment\n",
    "            for key, speaker_data in speaker_sentiments.items():\n",
    "                self.db.add_speaker_sentiment(\n",
    "                    conversation_id=conversation_id,\n",
    "                    speaker_name=speaker_data[\"name\"],\n",
    "                    speaker_role=speaker_data[\"role\"],\n",
    "                    positive_score=speaker_data[\"sentiment\"].positive,\n",
    "                    negative_score=speaker_data[\"sentiment\"].negative,\n",
    "                    neutral_score=speaker_data[\"sentiment\"].neutral,\n",
    "                    compound_score=speaker_data[\"sentiment\"].compound,\n",
    "                )\n",
    "\n",
    "            # Update credit risk mentions with sentiment (if present)\n",
    "            mentions = self.db.get_credit_risk_mentions([conversation_id])\n",
    "\n",
    "            for mention in mentions:\n",
    "                # Get sentiment for the mention context\n",
    "                if mention[\"mention_context\"]:\n",
    "                    contexts = json.loads(mention[\"mention_context\"])\n",
    "                    if contexts:\n",
    "                        # Get sentiment for first context\n",
    "                        context_sentiment = self._get_sentiment(contexts[0])\n",
    "                        self.db.update_credit_risk_mention_sentiment(\n",
    "                            mention_id=mention[\"id\"],\n",
    "                            positive_score=context_sentiment[\"positive\"],\n",
    "                            negative_score=context_sentiment[\"negative\"],\n",
    "                            neutral_score=context_sentiment[\"neutral\"],\n",
    "                            compound_score=context_sentiment[\"compound\"],\n",
    "                        )\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Stored sentiment results for conversation {conversation_id}\",\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error storing sentiment results: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_sentiment_for_text(self, text: str) -> SentimentScore:\n",
    "        \"\"\"\n",
    "        Get sentiment score for any text\n",
    "\n",
    "        Args:\n",
    "            text: Text to analyze\n",
    "\n",
    "        Returns:\n",
    "            SentimentScore object with sentiment analysis\n",
    "        \"\"\"\n",
    "        scores = self._get_sentiment(text)\n",
    "        return SentimentScore(\n",
    "            positive=scores[\"positive\"],\n",
    "            negative=scores[\"negative\"],\n",
    "            neutral=scores[\"neutral\"],\n",
    "            compound=scores[\"compound\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuJVVlvCKypv"
   },
   "source": [
    "### ‚öñÔ∏è Credit-Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_d-rmOkNK1Fj"
   },
   "outputs": [],
   "source": [
    "class CreditRiskAnalyser:\n",
    "    \"\"\"Credit risk analysis for G-SIB earnings call QA pairs\"\"\"\n",
    "\n",
    "    def __init__(self, db: GSIBDatabase):\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(f\"Initializing {__name__}\")\n",
    "\n",
    "        # Store database and components\n",
    "        self.db = db\n",
    "        self._populate_credit_risk_keywords()\n",
    "\n",
    "    def _populate_credit_risk_keywords(self):\n",
    "        \"\"\"Populate database with credit risk keywords\"\"\"\n",
    "        conn = db.begin_transaction()\n",
    "        try:\n",
    "            self.logger.info(\"Populating credit risk keywords\")\n",
    "            for metric_key, metric in CREDIT_METRICS.items():\n",
    "                for keyword in metric.keywords:\n",
    "                    self.db.add_credit_risk_keyword(keyword, metric_key, 1.0)\n",
    "                for indicator in metric.indicators:\n",
    "                    self.db.add_credit_risk_keyword(indicator, metric_key, 0.7)\n",
    "            db.commit_transaction(conn)\n",
    "            self.logger.info(\"Credit risk keywords populated\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error populating credit risk keywords: {str(e)}\")\n",
    "            db.rollback_transaction(conn)\n",
    "            raise\n",
    "\n",
    "    def _get_risk_level(self, score: float) -> str:\n",
    "        \"\"\"Convert numeric score to risk level category with more granularity\"\"\"\n",
    "        if score < 15:\n",
    "            return \"Very Low\"\n",
    "        if score < 30:\n",
    "            return \"Low\"\n",
    "        if score < 45:\n",
    "            return \"Moderately Low\"\n",
    "        if score < 60:\n",
    "            return \"Moderate\"\n",
    "        if score < 75:\n",
    "            return \"Moderately High\"\n",
    "        if score < 85:\n",
    "            return \"High\"\n",
    "        return \"Very High\"\n",
    "\n",
    "    def run_credit_risk_analysis(self, limit=None):\n",
    "        \"\"\"\n",
    "        Run credit risk analysis\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations to analyze\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting credit risk analysis{' (limited to ' + str(limit) + ' conversations)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Get all conversations\n",
    "        conversations = self.db.get_all_conversations()\n",
    "\n",
    "        if limit:\n",
    "            conversations = conversations[:limit]\n",
    "\n",
    "        # Analyze credit risk mentions\n",
    "        results = {}\n",
    "        for conversation in tqdm(conversations, desc=\"Analyzing credit risk mentions\"):\n",
    "            conversation_id = conversation[\"id\"]\n",
    "\n",
    "            # Run credit risk analysis\n",
    "            mentions = self.analyze_credit_risk_mentions(conversation_id)\n",
    "            self.db.checkpoint_wal()\n",
    "\n",
    "            if mentions:\n",
    "                results[conversation_id] = mentions\n",
    "                self.logger.debug(\n",
    "                    f\"Found {len(mentions)} credit risk mentions in conversation {conversation_id}\",\n",
    "                )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_credit_risk_mentions(self, conversation_id: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze credit risk metric mentions in a conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with credit risk mention analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get QA pairs for this conversation\n",
    "            qa_pairs = self.db.get_qa_pairs(conversation_id)\n",
    "\n",
    "            if not qa_pairs:\n",
    "                return {}\n",
    "\n",
    "            # Prepare text data\n",
    "            qa_texts = [\n",
    "                {\"id\": qa[\"id\"], \"text\": f\"{qa['question']} {qa['answer']}\"}\n",
    "                for qa in qa_pairs\n",
    "            ]\n",
    "\n",
    "            # Find credit risk metric mentions\n",
    "            mentions = {}\n",
    "            for metric_key, metric in CREDIT_METRICS.items():\n",
    "                # Search for metric keywords in each QA pair\n",
    "                mention_count = 0\n",
    "                mention_contexts = []\n",
    "                used_keywords = set()\n",
    "\n",
    "                for qa in qa_texts:\n",
    "                    text = qa[\"text\"].lower()\n",
    "\n",
    "                    # Check each keyword\n",
    "                    for keyword in metric.keywords:\n",
    "                        # Look for exact keyword matches with word boundaries\n",
    "                        pattern = r\"\\b\" + re.escape(keyword) + r\"\\b\"\n",
    "                        matches = list(re.finditer(pattern, text))\n",
    "\n",
    "                        if matches:\n",
    "                            mention_count += len(matches)\n",
    "                            used_keywords.add(keyword)\n",
    "\n",
    "                            # Extract context (50 chars before and after)\n",
    "                            for match in matches[\n",
    "                                :2\n",
    "                            ]:  # Limit to first 2 matches per keyword\n",
    "                                start_pos = match.start()\n",
    "                                context_start = max(0, start_pos - 50)\n",
    "                                context_end = min(\n",
    "                                    len(text),\n",
    "                                    start_pos + len(keyword) + 50,\n",
    "                                )\n",
    "                                context = text[context_start:context_end]\n",
    "                                mention_contexts.append(context)\n",
    "\n",
    "                if mention_count > 0:\n",
    "                    # Create CreditRiskMention instance\n",
    "                    mention = CreditRiskMention(\n",
    "                        metric_type=metric_key,\n",
    "                        mention_text=mention_contexts[0] if mention_contexts else \"\",\n",
    "                        mention_count=mention_count,\n",
    "                        keyword_used=list(used_keywords)[0] if used_keywords else None,\n",
    "                    )\n",
    "\n",
    "                    mentions[metric_key] = {\n",
    "                        \"mention\": mention,\n",
    "                        \"contexts\": mention_contexts[:3],  # Store up to 3 contexts\n",
    "                    }\n",
    "\n",
    "                    # Store in database using our new method\n",
    "                    keyword_id = list(used_keywords)[0] if used_keywords else metric_key\n",
    "                    self.db.add_credit_risk_mention(\n",
    "                        conversation_id=conversation_id,\n",
    "                        keyword_id=keyword_id,\n",
    "                        mention_count=mention_count,\n",
    "                        mention_contexts=mention_contexts[:3],\n",
    "                        positive_score=0.33,  # Placeholder sentiment scores\n",
    "                        negative_score=0.33,\n",
    "                        neutral_score=0.34,\n",
    "                        compound_score=0.0,\n",
    "                    )\n",
    "\n",
    "            return mentions\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing credit risk mentions: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def calculate_credit_risk_score(\n",
    "        self,\n",
    "        bank_name: str,\n",
    "        year: int,\n",
    "        quarter: str,\n",
    "    ) -> Dict:\n",
    "        \"\"\"Calculate credit risk score for a bank's quarterly report with improved differentiation\"\"\"\n",
    "        # Get metric values from database\n",
    "        report_id = self.db.get_report_id(bank_name, year, quarter)\n",
    "        if not report_id:\n",
    "            return {}\n",
    "        metrics = self.db.get_metrics_by_report(report_id)\n",
    "\n",
    "        # Get mentions of credit risk from conversations\n",
    "        conversations = self.db.get_bank_conversations(bank_name, year, quarter)\n",
    "        credit_mentions = self.db.get_credit_risk_mentions(conversations)\n",
    "\n",
    "        # Get bank asset size for scaling\n",
    "        bank_assets = self._get_bank_assets(bank_name)\n",
    "\n",
    "        # Calculate base scores WITHOUT size factor\n",
    "        metric_scores = {\n",
    "            \"NPL_RATIO\": self._score_npl_ratio(\n",
    "                self.db.get_metric_value(metrics, \"NPL_RATIO\"),\n",
    "            ),\n",
    "            \"COVERAGE_RATIO\": self._score_coverage_ratio(\n",
    "                self.db.get_metric_value(metrics, \"COVERAGE_RATIO\"),\n",
    "            ),\n",
    "            \"PCL\": self._score_pcl(self.db.get_metric_value(metrics, \"PCL\"), bank_name),\n",
    "            \"CET1_RATIO\": self._score_cet1_ratio(\n",
    "                self.db.get_metric_value(metrics, \"CET1_RATIO\"),\n",
    "            ),\n",
    "            \"TEXAS_RATIO\": self._score_texas_ratio(\n",
    "                self.db.get_metric_value(metrics, \"TEXAS_RATIO\"),\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Add sentiment risk from mentions\n",
    "        sentiment_risk = self._calculate_risk_from_mentions(credit_mentions)\n",
    "\n",
    "        # Adjust weights based on bank size\n",
    "        weights = self._get_adjusted_weights(bank_assets)\n",
    "\n",
    "        # Calculate final weighted score\n",
    "        risk_score = (\n",
    "            weights[\"NPL_RATIO\"] * metric_scores[\"NPL_RATIO\"]\n",
    "            + weights[\"COVERAGE_RATIO\"] * metric_scores[\"COVERAGE_RATIO\"]\n",
    "            + weights[\"PCL\"] * metric_scores[\"PCL\"]\n",
    "            + weights[\"CET1_RATIO\"] * metric_scores[\"CET1_RATIO\"]\n",
    "            + weights[\"TEXAS_RATIO\"] * metric_scores[\"TEXAS_RATIO\"]\n",
    "            # Add sentiment adjustment\n",
    "            + (sentiment_risk * 1.2)  # Increased impact of sentiment analysis\n",
    "        )\n",
    "\n",
    "        # Apply volatility multiplier if available\n",
    "        volatility_multiplier = self._calculate_volatility_multiplier(\n",
    "            bank_name,\n",
    "            year,\n",
    "            quarter,\n",
    "        )\n",
    "        risk_score *= volatility_multiplier\n",
    "\n",
    "        return {\n",
    "            \"bank\": bank_name,\n",
    "            \"period\": f\"{year} {quarter}\",\n",
    "            \"asset_size\": f\"${bank_assets / 1e12:.2f}T\",\n",
    "            \"risk_score\": min(100, max(0, risk_score)),\n",
    "            \"risk_level\": self._get_risk_level(risk_score),\n",
    "            \"metric_scores\": metric_scores,\n",
    "            \"sentiment_adjustment\": sentiment_risk,\n",
    "            \"weights\": weights,\n",
    "            \"high_risk_mentions\": self._extract_high_risk_mentions(credit_mentions),\n",
    "        }\n",
    "\n",
    "    def _score_npl_ratio(self, value: float) -> float:\n",
    "        \"\"\"Higher NPL = Higher Risk\"\"\"\n",
    "        if value <= 1.0:  # Excellent\n",
    "            return 10\n",
    "        if value <= 2.0:  # Good\n",
    "            return 25\n",
    "        if value <= 3.5:  # Average\n",
    "            return 50\n",
    "        if value <= 5.0:  # Concerning\n",
    "            return 75\n",
    "        # High Risk\n",
    "        return 90 + min(10, (value - 5.0) * 2)  # Can go over 100\n",
    "\n",
    "    def _score_coverage_ratio(self, value: float) -> float:\n",
    "        \"\"\"Lower Coverage = Higher Risk (inverse relationship)\"\"\"\n",
    "        if value >= 120:  # Excellent\n",
    "            return 10\n",
    "        if value >= 100:  # Good\n",
    "            return 25\n",
    "        if value >= 80:  # Average\n",
    "            return 50\n",
    "        if value >= 60:  # Concerning\n",
    "            return 75\n",
    "        # High Risk\n",
    "        return 90 + min(10, (60 - value) / 6)  # Can go over 100\n",
    "\n",
    "    def _score_cet1_ratio(self, value: float) -> float:\n",
    "        \"\"\"Lower CET1 = Higher Risk (inverse relationship)\"\"\"\n",
    "        if value >= 15.0:  # Excellent\n",
    "            return 10\n",
    "        if value >= 13.0:  # Good\n",
    "            return 25\n",
    "        if value >= 11.0:  # Average\n",
    "            return 50\n",
    "        if value >= 9.0:  # Concerning\n",
    "            return 75\n",
    "        # High Risk\n",
    "        return 90 + min(10, (9.0 - value) * 5)  # Can go over 100\n",
    "\n",
    "    def _score_texas_ratio(self, value: float) -> float:\n",
    "        \"\"\"Higher Texas Ratio = Higher Risk\"\"\"\n",
    "        if value <= 20:  # Excellent\n",
    "            return 10\n",
    "        if value <= 40:  # Good\n",
    "            return 25\n",
    "        if value <= 60:  # Average\n",
    "            return 50\n",
    "        if value <= 80:  # Concerning\n",
    "            return 75\n",
    "        # High Risk\n",
    "        return 90 + min(10, (value - 80) / 2)  # Can go over 100\n",
    "\n",
    "    def _get_bank_assets(self, bank_name: str) -> float:\n",
    "        \"\"\"Get total assets for a bank.\"\"\"\n",
    "        bank_assets = {\n",
    "            \"JPMorgan Chase\": 3.875e12,  # $3.875 trillion as of 2023\n",
    "            \"HSBC\": 2.919e12,  # $2.919 trillion as of 2023\n",
    "            \"Bank of America\": 3.180e12,  # $3.180 trillion as of 2023\n",
    "            \"UBS\": 1.717e12,  # $1.717 trillion as of 2023\n",
    "            \"Morgan Stanley\": 1.193e12,  # $1.193 trillion as of 2023\n",
    "            \"Credit Suisse\": 0.727e12,  # $727 billion before its acquisition by UBS in 2023\n",
    "            \"Goldman Sachs\": 1.642e12,  # $1.642 trillion as of 2023\n",
    "        }\n",
    "\n",
    "        return bank_assets.get(bank_name, 1.0e12)  # Default to $1 trillion\n",
    "\n",
    "    def _calculate_size_factor(self, bank_assets: float) -> float:\n",
    "        \"\"\"Calculate a multiplier based on bank size to increase score differentiation\"\"\"\n",
    "        if bank_assets >= 3.0e12:  # $3T+ (mega banks)\n",
    "            return 1.15  # 15% higher score for mega banks\n",
    "        if bank_assets >= 2.0e12:  # $2-3T\n",
    "            return 1.10  # 10% higher score for very large banks\n",
    "        if bank_assets >= 1.0e12:  # $1-2T\n",
    "            return 1.0  # Baseline\n",
    "        # <$1T\n",
    "        return 0.90  # 10% lower score for smaller banks\n",
    "\n",
    "    def _score_pcl(self, value: float, bank_name: str) -> float:\n",
    "        \"\"\"Higher PCL (relative to bank size) = Higher Risk\"\"\"\n",
    "        # Get bank assets to normalize PCL\n",
    "        bank_assets = self._get_bank_assets(bank_name)\n",
    "        if not bank_assets:\n",
    "            return 50  # Default to average if asset data unavailable\n",
    "\n",
    "        # Calculate PCL as percentage of assets\n",
    "        pcl_percentage = (value / bank_assets) * 100\n",
    "\n",
    "        if pcl_percentage <= 0.05:  # Excellent\n",
    "            return 10\n",
    "        if pcl_percentage <= 0.10:  # Good\n",
    "            return 25\n",
    "        if pcl_percentage <= 0.20:  # Average\n",
    "            return 50\n",
    "        if pcl_percentage <= 0.40:  # Concerning\n",
    "            return 75\n",
    "        # High Risk\n",
    "        return 90 + min(10, (pcl_percentage - 0.40) * 25)  # Can go over 100\n",
    "\n",
    "    def _get_adjusted_weights(self, bank_assets: float) -> Dict[str, float]:\n",
    "        \"\"\"Return adjusted metric weights based on bank size\"\"\"\n",
    "        # Base weights add up to 1.0\n",
    "        if bank_assets >= 3.0e12:  # Mega banks (JPM, BofA)\n",
    "            return {\n",
    "                \"NPL_RATIO\": 0.30,  # Higher weight on asset quality\n",
    "                \"COVERAGE_RATIO\": 0.15,\n",
    "                \"PCL\": 0.20,  # Higher weight on provisions\n",
    "                \"CET1_RATIO\": 0.20,  # Capital still important\n",
    "                \"TEXAS_RATIO\": 0.15,\n",
    "            }\n",
    "        if bank_assets >= 1.5e12:  # Large banks (GS, MS)\n",
    "            return {\n",
    "                \"NPL_RATIO\": 0.25,\n",
    "                \"COVERAGE_RATIO\": 0.15,\n",
    "                \"PCL\": 0.15,\n",
    "                \"CET1_RATIO\": 0.30,  # Higher capital requirements for investment banks\n",
    "                \"TEXAS_RATIO\": 0.15,\n",
    "            }\n",
    "        # Medium sized banks\n",
    "        return {\n",
    "            \"NPL_RATIO\": 0.20,\n",
    "            \"COVERAGE_RATIO\": 0.20,  # More focused on provisions\n",
    "            \"PCL\": 0.20,\n",
    "            \"CET1_RATIO\": 0.15,  # Less focus on capital\n",
    "            \"TEXAS_RATIO\": 0.25,  # Higher focus on overall health\n",
    "        }\n",
    "\n",
    "    def _calculate_volatility_multiplier(\n",
    "        self,\n",
    "        bank_name: str,\n",
    "        year: int,\n",
    "        quarter: str,\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate multiplier based on metric volatility from previous quarters\"\"\"\n",
    "        try:\n",
    "            # Get current and previous quarter metrics\n",
    "            current_id = self.db.get_report_id(bank_name, year, quarter)\n",
    "            current_metrics = self.db.get_metrics_by_report(current_id)\n",
    "\n",
    "            # Determine previous quarter\n",
    "            prev_year, prev_quarter = self._get_previous_quarter(year, quarter)\n",
    "            prev_id = self.db.get_report_id(bank_name, prev_year, prev_quarter)\n",
    "            prev_metrics = self.db.get_metrics_by_report(prev_id)\n",
    "\n",
    "            if not prev_metrics:\n",
    "                return 1.0  # No previous data\n",
    "\n",
    "            # Calculate percentage change in key metrics\n",
    "            key_metrics = [\"NPL_RATIO\", \"CET1_RATIO\", \"TEXAS_RATIO\"]\n",
    "            changes = []\n",
    "\n",
    "            for metric in key_metrics:\n",
    "                current_value = self.db.get_metric_value(current_metrics, metric)\n",
    "                prev_value = self.db.get_metric_value(prev_metrics, metric)\n",
    "\n",
    "                if current_value and prev_value > 0:  # Ensure we have valid values\n",
    "                    pct_change = abs((current_value - prev_value) / prev_value)\n",
    "                    changes.append(pct_change)\n",
    "\n",
    "            if not changes:\n",
    "                return 1.0\n",
    "\n",
    "            # Average volatility\n",
    "            avg_change = sum(changes) / len(changes)\n",
    "\n",
    "            # Higher volatility = higher multiplier\n",
    "            if avg_change < 0.05:  # <5% change\n",
    "                return 1.0\n",
    "            if avg_change < 0.10:  # 5-10% change\n",
    "                return 1.05\n",
    "            if avg_change < 0.20:  # 10-20% change\n",
    "                return 1.10\n",
    "            # >20% change\n",
    "            return 1.15\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error calculating volatility: {str(e)}\")\n",
    "            return 1.0\n",
    "\n",
    "    def _get_previous_quarter(self, year: int, quarter: str) -> Tuple[int, str]:\n",
    "        \"\"\"Get previous quarter and year\"\"\"\n",
    "        q_map = {\"Q1\": 4, \"Q2\": 1, \"Q3\": 2, \"Q4\": 3}\n",
    "        prev_q_num = q_map[quarter]\n",
    "        prev_year = year - 1 if quarter == \"Q1\" else year\n",
    "        prev_quarter = f\"Q{prev_q_num}\"\n",
    "        return prev_year, prev_quarter\n",
    "\n",
    "    def _calculate_risk_from_mentions(self, mentions: List[Dict]) -> float:\n",
    "        \"\"\"Calculate risk adjustment based on sentiment with higher impact\"\"\"\n",
    "        if not mentions:\n",
    "            return 0\n",
    "\n",
    "        # Aggregate sentiment scores with greater weight for negative mentions\n",
    "        negative_scores = []\n",
    "        for mention in mentions:\n",
    "            neg_score = mention.get(\"negative_score\", 0)\n",
    "            compound_score = mention.get(\"compound_score\", 0)\n",
    "\n",
    "            # Higher weight for more concerning sentiment\n",
    "            if compound_score < -0.5:  # Very negative\n",
    "                negative_scores.append(neg_score * 2.0)\n",
    "            elif compound_score < -0.3:  # Moderately negative\n",
    "                negative_scores.append(neg_score * 1.5)\n",
    "            else:\n",
    "                negative_scores.append(neg_score)\n",
    "\n",
    "        # Average negative sentiment (0-1 scale)\n",
    "        avg_negative = (\n",
    "            sum(negative_scores) / len(negative_scores) if negative_scores else 0\n",
    "        )\n",
    "\n",
    "        # Convert to 0-15 scale for adjustment (increased from 0-10)\n",
    "        return avg_negative * 15\n",
    "\n",
    "    def _extract_high_risk_mentions(self, mentions: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract the highest risk mentions as evidence\"\"\"\n",
    "        if not mentions:\n",
    "            return []\n",
    "\n",
    "        # Sort mentions by negative sentiment\n",
    "        sorted_mentions = sorted(\n",
    "            mentions,\n",
    "            key=lambda m: m.get(\"negative_score\", 0),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        # Return top 3 most negative mentions\n",
    "        high_risk_mentions = []\n",
    "        for mention in sorted_mentions[:3]:\n",
    "            contexts = json.loads(mention.get(\"mention_context\", \"[]\"))\n",
    "            if contexts:\n",
    "                high_risk_mentions.append(\n",
    "                    {\n",
    "                        \"metric\": mention.get(\"keyword_id\", \"\"),\n",
    "                        \"context\": contexts[0],\n",
    "                        \"negative_score\": mention.get(\"negative_score\", 0),\n",
    "                        \"compound_score\": mention.get(\"compound_score\", 0),\n",
    "                    },\n",
    "                )\n",
    "\n",
    "        return high_risk_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGAuTWWYddjJ"
   },
   "source": [
    "### ‚ö° FAISS Vector Store (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kgnrAUMq4Cc8"
   },
   "outputs": [],
   "source": [
    "class FAISSManager:\n",
    "    \"\"\"Manager class for FAISS vector indices with SQLite integration\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_manager: GSIBDatabase,\n",
    "        model_name: str = \"FinLang/finance-embeddings-investopedia\",\n",
    "        reranker_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "        index_name: str = \"main\",\n",
    "        device: str = \"cpu\",\n",
    "        lazy_load: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize FAISS manager with database connection and embedding models\n",
    "\n",
    "        Args:\n",
    "            db_manager: Instance of GSIBDatabase\n",
    "            model_name: HuggingFace model name for embeddings\n",
    "            reranker_name: Model name for cross-encoder reranker\n",
    "            index_name: Name for the FAISS index\n",
    "            device: Device to use for models ('cpu' or 'cuda')\n",
    "            lazy_load: Only load the index when needed (default: True)\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Store config\n",
    "        self.db_manager = db_manager\n",
    "        self.model_name = model_name\n",
    "        self.reranker_name = reranker_name\n",
    "        self.index_name = index_name\n",
    "        self.device = device\n",
    "        self.lazy_load = lazy_load\n",
    "\n",
    "        # Load required models\n",
    "        self._load_models()\n",
    "\n",
    "        # Initialize index as None, will be loaded on first use if lazy_load is True\n",
    "        self.index = None\n",
    "        if not lazy_load:\n",
    "            self.index = self._load_or_create_index()\n",
    "\n",
    "    def _load_models(self) -> None:\n",
    "        \"\"\"Load embedding model, reranker, and spaCy model\"\"\"\n",
    "        try:\n",
    "            # Load embedding model\n",
    "            self.model = SentenceTransformer(self.model_name, device=self.device)\n",
    "            self.embedding_dim = self.model.get_sentence_embedding_dimension()\n",
    "            self.logger.info(\n",
    "                f\"Loaded embedding model: {self.model_name} ({self.embedding_dim} dimensions)\",\n",
    "            )\n",
    "\n",
    "            # Load reranker model\n",
    "            self.reranker = CrossEncoder(self.reranker_name, device=self.device)\n",
    "            self.logger.info(f\"Loaded reranker model: {self.reranker_name}\")\n",
    "\n",
    "            # Load spaCy for tokenization\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            self.logger.info(\"Loaded spaCy model: en_core_web_sm\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _ensure_index_loaded(self):\n",
    "        \"\"\"Ensure the FAISS index is loaded before use\"\"\"\n",
    "        if self.index is None:\n",
    "            self.index = self._load_or_create_index()\n",
    "\n",
    "    def _load_or_create_index(self) -> faiss.IndexIDMap2:\n",
    "        \"\"\"\n",
    "        Load existing FAISS index from database or create a new one\n",
    "\n",
    "        Returns:\n",
    "            FAISS index with vectors loaded from database if they exist\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Check if index exists\n",
    "            cursor.execute(\n",
    "                \"SELECT id, embedding_dim, model_name, config FROM faiss_indices WHERE name = ?\",\n",
    "                (self.index_name,),\n",
    "            )\n",
    "            index_info = cursor.fetchone()\n",
    "\n",
    "            if index_info:\n",
    "                # Index exists in database, now load the actual vectors\n",
    "                self.logger.info(f\"Loading existing FAISS index: {self.index_name}\")\n",
    "\n",
    "                # Create the index structure\n",
    "                M = 32\n",
    "                base_index = faiss.IndexHNSWFlat(self.embedding_dim, M)\n",
    "                base_index.hnsw.efConstruction = 200\n",
    "                base_index.hnsw.efSearch = 50\n",
    "                index = faiss.IndexIDMap2(base_index)\n",
    "\n",
    "                # Load vectors from database\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT id, vector_data\n",
    "                    FROM vector_embeddings\n",
    "                    WHERE embedding_model = ? AND preprocessing_level = 'lemmatized'\n",
    "                    \"\"\",\n",
    "                    (self.model_name,),\n",
    "                )\n",
    "\n",
    "                vectors = []\n",
    "                ids = []\n",
    "                for row in cursor.fetchall():\n",
    "                    vector_data = pickle.loads(row[\"vector_data\"])\n",
    "                    vectors.append(vector_data)\n",
    "                    ids.append(\n",
    "                        row[\"id\"]\n",
    "                    )  # Using the vector_embeddings.id as the FAISS id\n",
    "\n",
    "                if vectors:\n",
    "                    # Add vectors to index in batch\n",
    "                    vectors_array = np.array(vectors, dtype=np.float32)\n",
    "                    ids_array = np.array(ids, dtype=np.int64)\n",
    "                    index.add_with_ids(vectors_array, ids_array)\n",
    "                    self.logger.info(f\"Loaded {len(vectors)} vectors into FAISS index\")\n",
    "\n",
    "                return index\n",
    "            # Create new index and register in database\n",
    "            self.logger.info(f\"Creating new FAISS index: {self.index_name}\")\n",
    "            index = self._create_hnsw_index()\n",
    "\n",
    "            # Register index in database\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                    INSERT INTO faiss_indices (name, embedding_dim, model_name, config)\n",
    "                    VALUES (?, ?, ?, ?)\n",
    "                    \"\"\",\n",
    "                (\n",
    "                    self.index_name,\n",
    "                    self.embedding_dim,\n",
    "                    self.model_name,\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"type\": \"hnsw\",\n",
    "                            \"M\": 32,\n",
    "                            \"efConstruction\": 200,\n",
    "                            \"efSearch\": 50,\n",
    "                        }\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "            conn.commit()\n",
    "            return index\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading/creating FAISS index: {str(e)}\")\n",
    "            # Create a new index as fallback\n",
    "            return self._create_hnsw_index()\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def _create_hnsw_index(self) -> faiss.IndexIDMap2:\n",
    "        \"\"\"Create a new HNSW index with optimized parameters\"\"\"\n",
    "        # Initialize HNSW index\n",
    "        M = 32  # Number of neighbors per node\n",
    "        base_index = faiss.IndexHNSWFlat(self.embedding_dim, M)\n",
    "\n",
    "        # Set HNSW parameters\n",
    "        base_index.hnsw.efConstruction = 200  # Construction-time accuracy\n",
    "        base_index.hnsw.efSearch = 50  # Search-time accuracy\n",
    "\n",
    "        # Wrap with IDMap for mapping to database IDs\n",
    "        index = faiss.IndexIDMap2(base_index)\n",
    "        self.logger.info(f\"Created new HNSW index with dim={self.embedding_dim}, M={M}\")\n",
    "        return index\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "\n",
    "        Returns:\n",
    "            Numpy array of embeddings\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            return np.array([], dtype=np.float32)\n",
    "\n",
    "        # Generate embeddings\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "        # Convert to float32 and normalize\n",
    "        embeddings = np.array(embeddings, dtype=np.float32)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "    def _get_combined_qa_to_index(self, limit: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get QA pairs that need to be indexed\n",
    "\n",
    "        Retrieves preprocessed text entries of type 'combined' that haven't been indexed yet.\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of QA pairs to retrieve\n",
    "\n",
    "        Returns:\n",
    "            List of preprocessed text entries with metadata\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Find combined preprocessed texts not yet indexed\n",
    "            query = \"\"\"\n",
    "            SELECT\n",
    "                pt.id as preprocessed_text_id,\n",
    "                pt.processed_text as combined,\n",
    "                pt.preprocessing_level,\n",
    "                qp.id as qa_pair_id,\n",
    "                ac.id as conversation_id,\n",
    "                ac.analyst_name,\n",
    "                ac.analyst_company,\n",
    "                r.year,\n",
    "                r.quarter,\n",
    "                b.bank_name\n",
    "            FROM preprocessed_text pt\n",
    "            JOIN qa_pairs qp ON pt.qa_pair_id = qp.id\n",
    "            JOIN analyst_conversations ac ON qp.conversation_id = ac.id\n",
    "            JOIN reports r ON ac.report_id = r.id\n",
    "            JOIN banks b ON r.bank_id = b.id\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM vector_embeddings ve\n",
    "                WHERE pt.id = ve.preprocessed_text_id\n",
    "                AND ve.embedding_model = ?\n",
    "            )\n",
    "            AND pt.text_type = 'combined'\n",
    "            AND pt.preprocessing_level = 'lemmatized'\n",
    "            \"\"\"\n",
    "\n",
    "            if limit:\n",
    "                query += f\" LIMIT {limit}\"\n",
    "\n",
    "            cursor.execute(query, (self.model_name,))\n",
    "            entries = cursor.fetchall()\n",
    "            return self.db_manager.cursor_to_dict(cursor, entries)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting QA pairs to index: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def _process_qa_batch(\n",
    "        self, batch: List[Dict]\n",
    "    ) -> Tuple[List[str], List[int], List[Dict]]:\n",
    "        \"\"\"\n",
    "        Process a batch of combined texts for indexing.\n",
    "\n",
    "        Args:\n",
    "            batch: List of preprocessed text entries with metadata\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (text_list, preprocessed_text_ids, metadata_list)\n",
    "        \"\"\"\n",
    "        combined_texts = []\n",
    "        preprocessed_text_ids = []\n",
    "        metadata_list = []\n",
    "\n",
    "        for entry in batch:\n",
    "            combined_texts.append(entry[\"combined\"])\n",
    "            preprocessed_text_ids.append(entry[\"preprocessed_text_id\"])\n",
    "\n",
    "            # Create metadata\n",
    "            metadata = {\n",
    "                \"preprocessed_text_id\": entry[\"preprocessed_text_id\"],\n",
    "                \"qa_pair_id\": entry[\"qa_pair_id\"],\n",
    "                \"conversation_id\": entry[\"conversation_id\"],\n",
    "                \"analyst_name\": entry[\"analyst_name\"],\n",
    "                \"analyst_company\": entry[\"analyst_company\"],\n",
    "                \"bank\": entry[\"bank_name\"],\n",
    "                \"year\": entry[\"year\"],\n",
    "                \"quarter\": entry[\"quarter\"],\n",
    "                \"preprocessing_level\": entry[\"preprocessing_level\"],\n",
    "            }\n",
    "            metadata_list.append(metadata)\n",
    "\n",
    "        return combined_texts, preprocessed_text_ids, metadata_list\n",
    "\n",
    "    def _add_vectors_to_index(self, embeddings: np.ndarray, db_ids: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Add vectors to FAISS index\n",
    "\n",
    "        Args:\n",
    "            embeddings: Numpy array of embeddings\n",
    "            db_ids: Numpy array of database IDs\n",
    "\n",
    "        Returns:\n",
    "            Success flag\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.index.add_with_ids(embeddings, db_ids)\n",
    "            self.logger.info(f\"Added {len(db_ids)} vectors to FAISS index\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error adding vectors to FAISS index: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _store_embeddings_in_db(\n",
    "        self, preprocessed_text_ids, embeddings, metadata_list, qa_texts\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Store embeddings in the database\n",
    "\n",
    "        Args:\n",
    "            preprocessed_text_ids: List of preprocessed text IDs\n",
    "            embeddings: List of embeddings\n",
    "            metadata_list: List of metadata dictionaries\n",
    "            qa_texts: List of text chunks\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            for i, (preprocessed_text_id, embedding, metadata, text) in enumerate(\n",
    "                zip(\n",
    "                    preprocessed_text_ids,\n",
    "                    embeddings,\n",
    "                    metadata_list,\n",
    "                    qa_texts,\n",
    "                )\n",
    "            ):\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    INSERT INTO vector_embeddings\n",
    "                    (preprocessed_text_id, embedding_model, preprocessing_level, vector_data, chunk_text, metadata)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?)\n",
    "                    ON CONFLICT(preprocessed_text_id, embedding_model, preprocessing_level) DO UPDATE SET\n",
    "                        vector_data = excluded.vector_data,\n",
    "                        chunk_text = excluded.chunk_text,\n",
    "                        metadata = excluded.metadata\n",
    "                    \"\"\",\n",
    "                    (\n",
    "                        preprocessed_text_id,\n",
    "                        self.model_name,\n",
    "                        metadata[\"preprocessing_level\"],\n",
    "                        pickle.dumps(embedding),\n",
    "                        text,\n",
    "                        json.dumps(metadata),\n",
    "                    ),\n",
    "                )\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error storing embeddings in database: {str(e)}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def _update_faiss_index_info(self) -> None:\n",
    "        \"\"\"Update FAISS index information in database\"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                UPDATE faiss_indices\n",
    "                SET embedding_dim = ?, config = ?\n",
    "                WHERE name = ?\n",
    "                \"\"\",\n",
    "                (\n",
    "                    self.embedding_dim,\n",
    "                    json.dumps(\n",
    "                        {\n",
    "                            \"type\": \"hnsw\",\n",
    "                            \"M\": 32,\n",
    "                            \"efConstruction\": 200,\n",
    "                            \"efSearch\": 50,\n",
    "                            \"total_vectors\": self.index.ntotal,\n",
    "                        },\n",
    "                    ),\n",
    "                    self.index_name,\n",
    "                ),\n",
    "            )\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            conn.rollback()\n",
    "            self.logger.error(f\"Error updating FAISS index info: {str(e)}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def index_combined_qa(self, limit: Optional[int] = None) -> int:\n",
    "        \"\"\"\n",
    "        Index all Combined qa pairs from the database that aren't already indexed\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of QA pairs to index\n",
    "\n",
    "        Returns:\n",
    "            Number of QA pairs indexed\n",
    "        \"\"\"\n",
    "        self._ensure_index_loaded()\n",
    "\n",
    "        # Get QA pairs to index\n",
    "        qa_pairs = self._get_combined_qa_to_index(limit)\n",
    "\n",
    "        if not qa_pairs:\n",
    "            self.logger.info(\"No new QA pairs to index\")\n",
    "            return 0\n",
    "\n",
    "        self.logger.info(f\"Found {len(qa_pairs)} Combined QA's to index\")\n",
    "\n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 50\n",
    "        total_indexed = 0\n",
    "\n",
    "        for i in range(0, len(qa_pairs), batch_size):\n",
    "            batch = qa_pairs[i : i + batch_size]\n",
    "            self.logger.info(\n",
    "                f\"Processing batch {i // batch_size + 1}/{(len(qa_pairs) - 1) // batch_size + 1}\",\n",
    "            )\n",
    "\n",
    "            # Process QA pairs\n",
    "            qa_texts, qa_ids, metadata_list = self._process_qa_batch(batch)\n",
    "\n",
    "            # Generate embeddings\n",
    "            embeddings = self.generate_embeddings(qa_texts)\n",
    "\n",
    "            # Add to FAISS index\n",
    "            db_ids = np.array([int(qa_id) for qa_id in qa_ids], dtype=np.int64)\n",
    "            if not self._add_vectors_to_index(embeddings, db_ids):\n",
    "                continue\n",
    "\n",
    "            # Store in database\n",
    "            self._store_embeddings_in_db(qa_ids, embeddings, metadata_list, qa_texts)\n",
    "            total_indexed += len(batch)\n",
    "\n",
    "        # Update FAISS index info\n",
    "        self._update_faiss_index_info()\n",
    "        self.logger.info(f\"Indexed {total_indexed} QA pairs\")\n",
    "        return total_indexed\n",
    "\n",
    "    def _fetch_search_results(self, vector_ids: List[int]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch search results from database\n",
    "\n",
    "        Args:\n",
    "            vector_ids: List of vector IDs from FAISS search\n",
    "\n",
    "        Returns:\n",
    "            List of search results with metadata\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            placeholders = \",\".join([\"?\"] * len(vector_ids))\n",
    "            sql = f\"\"\"\n",
    "            SELECT\n",
    "                ve.id as vector_embedding_id,\n",
    "                ve.preprocessed_text_id,\n",
    "                ve.chunk_text,\n",
    "                ve.metadata,\n",
    "                pt.qa_pair_id,\n",
    "                qp.question,\n",
    "                qp.answer,\n",
    "                qp.answer_speaker,\n",
    "                qp.answer_role\n",
    "            FROM vector_embeddings ve\n",
    "            JOIN preprocessed_text pt ON ve.preprocessed_text_id = pt.id\n",
    "            JOIN qa_pairs qp ON pt.qa_pair_id = qp.id\n",
    "            WHERE ve.id IN ({placeholders})\n",
    "            \"\"\"\n",
    "\n",
    "            cursor.execute(sql, vector_ids)\n",
    "            results = cursor.fetchall()\n",
    "            return self.db_manager.cursor_to_dict(cursor, results)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching search results: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def _prepare_faiss_results(\n",
    "        self,\n",
    "        results: List[Dict],\n",
    "        vector_ids: List[int],\n",
    "        distances: np.ndarray,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prepare FAISS search results\n",
    "\n",
    "        Args:\n",
    "            results: Raw search results from database\n",
    "            vector_ids: List of vector IDs from FAISS search\n",
    "            distances: Distances from FAISS search\n",
    "\n",
    "        Returns:\n",
    "            List of formatted search results\n",
    "        \"\"\"\n",
    "        faiss_results = []\n",
    "\n",
    "        for row in results:\n",
    "            # Extract vector_id position in the original results\n",
    "            try:\n",
    "                position = vector_ids.index(row[\"vector_embedding_id\"])\n",
    "                distance = float(distances[0][position])\n",
    "                similarity = 1.0 - distance  # Convert distance to similarity\n",
    "            except ValueError:\n",
    "                similarity = 0.0\n",
    "\n",
    "            # Parse metadata\n",
    "            try:\n",
    "                metadata = json.loads(row[\"metadata\"]) if row[\"metadata\"] else {}\n",
    "            except json.JSONDecodeError:\n",
    "                metadata = {}\n",
    "\n",
    "            # Prepare result\n",
    "            result = {\n",
    "                \"vector_embedding_id\": row[\"vector_embedding_id\"],\n",
    "                \"preprocessed_text_id\": row[\"preprocessed_text_id\"],\n",
    "                \"qa_pair_id\": row[\"qa_pair_id\"],\n",
    "                \"question\": row[\"question\"],\n",
    "                \"answer\": row[\"answer\"],\n",
    "                \"answer_speaker\": row[\"answer_speaker\"],\n",
    "                \"answer_role\": row[\"answer_role\"],\n",
    "                \"text\": row[\"chunk_text\"] or f\"{row['question']} {row['answer']}\",\n",
    "                \"vector_score\": similarity,\n",
    "                \"metadata\": metadata,\n",
    "            }\n",
    "            faiss_results.append(result)\n",
    "\n",
    "        return faiss_results\n",
    "\n",
    "    def _search_risk_metric(self, doc_id_to_text, query_keywords, top_n=5):\n",
    "        \"\"\"\n",
    "        Apply BM25 search using risk-specific keywords\n",
    "\n",
    "        Args:\n",
    "            doc_id_to_text: Dictionary mapping document IDs to text\n",
    "            query_keywords: List of keywords for the risk type\n",
    "            top_n: Number of top results to return\n",
    "\n",
    "        Returns:\n",
    "            List of (document, score) tuples\n",
    "        \"\"\"\n",
    "        # Tokenize documents with spaCy\n",
    "        tokenized_corpus = [\n",
    "            [token.text.lower() for token in self.nlp(text)]\n",
    "            for text in doc_id_to_text.values()\n",
    "        ]\n",
    "\n",
    "        # Build BM25 index\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "        # Create search query from keywords\n",
    "        search_query = \" \".join(query_keywords)\n",
    "        tokenized_query = [token.text.lower() for token in self.nlp(search_query)]\n",
    "\n",
    "        # Get BM25 scores\n",
    "        scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "        # Rank documents based on scores\n",
    "        documents = list(doc_id_to_text.values())\n",
    "        ranked_indices = sorted(\n",
    "            range(len(scores)), key=lambda i: scores[i], reverse=True\n",
    "        )[:top_n]\n",
    "\n",
    "        # Return ranked documents with scores\n",
    "        return [(documents[idx], scores[idx]) for idx in ranked_indices]\n",
    "\n",
    "    def _create_filtered_index(\n",
    "        self, preprocessed_text_ids: List[int]\n",
    "    ) -> faiss.IndexIDMap2:\n",
    "        \"\"\"\n",
    "        Create a temporary FAISS index containing only vectors that match filters\n",
    "\n",
    "        Args:\n",
    "            preprocessed_text_ids: List of preprocessed text IDs to include\n",
    "\n",
    "        Returns:\n",
    "            Filtered FAISS index\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Create a new HNSW index\n",
    "            base_index = faiss.IndexHNSWFlat(self.embedding_dim, 32)\n",
    "            base_index.hnsw.efConstruction = 200\n",
    "            base_index.hnsw.efSearch = 50\n",
    "            filtered_index = faiss.IndexIDMap2(base_index)\n",
    "\n",
    "            # Get embeddings for the vector IDs\n",
    "            placeholders = \",\".join([\"?\"] * len(preprocessed_text_ids))\n",
    "            sql = f\"\"\"\n",
    "            SELECT id, preprocessed_text_id, vector_data\n",
    "            FROM vector_embeddings\n",
    "            WHERE preprocessed_text_id IN ({placeholders})\n",
    "            AND embedding_model = ?\n",
    "            AND preprocessing_level = 'lemmatized'\n",
    "            \"\"\"\n",
    "\n",
    "            cursor.execute(sql, preprocessed_text_ids + [self.model_name])\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if not results:\n",
    "                return filtered_index\n",
    "\n",
    "            # Add vectors to the filtered index\n",
    "            for row in results:\n",
    "                vector_id = row[\"id\"]\n",
    "                vector_data = pickle.loads(row[\"vector_data\"])\n",
    "\n",
    "                # Make sure vector is properly shaped and normalized\n",
    "                vector_data = vector_data.reshape(1, -1)\n",
    "                faiss.normalize_L2(vector_data)\n",
    "\n",
    "                # Add to index\n",
    "                filtered_index.add_with_ids(\n",
    "                    vector_data, np.array([vector_id], dtype=np.int64)\n",
    "                )\n",
    "\n",
    "            return filtered_index\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating filtered index: {str(e)}\")\n",
    "            # Return empty index as fallback\n",
    "            base_index = faiss.IndexHNSWFlat(self.embedding_dim, 32)\n",
    "            return faiss.IndexIDMap2(base_index)\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def _apply_reranking(self, query: str, results: List[Dict], k: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Apply cross-encoder reranking to search results\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            results: List of search results\n",
    "            k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            Reranked results\n",
    "        \"\"\"\n",
    "        # Create query-document pairs for reranking\n",
    "        query_doc_pairs = [(query, result[\"text\"]) for result in results]\n",
    "\n",
    "        try:\n",
    "            # Get reranking scores\n",
    "            rerank_scores = self.reranker.predict(query_doc_pairs)\n",
    "\n",
    "            # Add scores to results\n",
    "            for i, score in enumerate(rerank_scores):\n",
    "                results[i][\"rerank_score\"] = float(score)\n",
    "\n",
    "            # Sort by reranker score\n",
    "            reranked_results = sorted(\n",
    "                results,\n",
    "                key=lambda x: x.get(\"rerank_score\", 0),\n",
    "                reverse=True,\n",
    "            )\n",
    "\n",
    "            return reranked_results[:k]\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Reranking failed: {str(e)}\")\n",
    "            # Fall back to vector scores\n",
    "            return sorted(\n",
    "                results,\n",
    "                key=lambda x: x.get(\"vector_score\", 0),\n",
    "                reverse=True,\n",
    "            )[:k]\n",
    "\n",
    "    def _enhance_with_bm25(self, faiss_results, bm25_results, doc_id_to_text):\n",
    "        \"\"\"\n",
    "        Enhance FAISS results with BM25 scores\n",
    "\n",
    "        Args:\n",
    "            faiss_results: Results from FAISS search\n",
    "            bm25_results: Results from BM25 search with keyword focus\n",
    "            doc_id_to_text: Dictionary mapping IDs to texts\n",
    "\n",
    "        Returns:\n",
    "            Enhanced results with combined scores\n",
    "        \"\"\"\n",
    "        # Create reverse mapping from text to result\n",
    "        text_to_result = {result[\"text\"]: result for result in faiss_results}\n",
    "\n",
    "        # Get max BM25 score for normalization\n",
    "        max_bm25 = max([score for _, score in bm25_results] or [1.0])\n",
    "\n",
    "        # Create mapping from text to normalized BM25 score\n",
    "        bm25_scores = {text: score / max_bm25 for text, score in bm25_results}\n",
    "\n",
    "        # Enhance results with BM25 scores\n",
    "        for result in faiss_results:\n",
    "            text = result[\"text\"]\n",
    "            # Add BM25 score if available\n",
    "            result[\"bm25_score\"] = bm25_scores.get(text, 0.0)\n",
    "\n",
    "            # Calculate combined score (70% vector, 30% BM25)\n",
    "            vector_score = result.get(\"vector_score\", 0.0)\n",
    "            bm25_score = result.get(\"bm25_score\", 0.0)\n",
    "            result[\"combined_score\"] = (0.7 * vector_score) + (0.3 * bm25_score)\n",
    "\n",
    "        return faiss_results\n",
    "\n",
    "    def _filtered_hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 10,\n",
    "        filter_conditions: Dict[str, Any] = None,\n",
    "        rerank: bool = True,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Optimized search that applies filtering at the database level first\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            filter_conditions: Conditions to filter results\n",
    "            rerank: Whether to apply cross-encoder reranking\n",
    "\n",
    "        Returns:\n",
    "            List of filtered search results\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # First, get the preprocessed_text_ids that match the filter conditions\n",
    "            sql_conditions = []\n",
    "            sql_params = []\n",
    "\n",
    "            for key, value in filter_conditions.items():\n",
    "                sql_conditions.append(f\"json_extract(ve.metadata, '$.{key}') = ?\")\n",
    "                sql_params.append(value)\n",
    "\n",
    "            where_clause = \" AND \".join(sql_conditions)\n",
    "\n",
    "            # Query to get vector_ids that match filter conditions\n",
    "            sql = f\"\"\"\n",
    "            SELECT ve.id as vector_embedding_id, ve.preprocessed_text_id\n",
    "            FROM vector_embeddings ve\n",
    "            WHERE ve.embedding_model = ?\n",
    "            AND ve.preprocessing_level = 'lemmatized'\n",
    "            AND {where_clause}\n",
    "            \"\"\"\n",
    "\n",
    "            cursor.execute(sql, [self.model_name] + sql_params)\n",
    "            filtered_results = cursor.fetchall()\n",
    "\n",
    "            vector_embedding_ids = [\n",
    "                row[\"vector_embedding_id\"] for row in filtered_results\n",
    "            ]\n",
    "            preprocessed_text_ids = [\n",
    "                row[\"preprocessed_text_id\"] for row in filtered_results\n",
    "            ]\n",
    "\n",
    "            if not vector_embedding_ids:\n",
    "                self.logger.info(\n",
    "                    f\"No vectors found for filter conditions: {filter_conditions}\"\n",
    "                )\n",
    "                return []\n",
    "\n",
    "            # Create a filtered FAISS index for the search\n",
    "            filtered_index = self._create_filtered_index(preprocessed_text_ids)\n",
    "            if filtered_index.ntotal == 0:\n",
    "                self.logger.info(\"Filtered index is empty\")\n",
    "                return []\n",
    "\n",
    "            # Perform semantic search on the filtered index\n",
    "            query_embedding = self.generate_embeddings([query])[0]\n",
    "            query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "            query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "            n_search = min(k * 2, filtered_index.ntotal)\n",
    "            distances, indices = filtered_index.search(query_embedding, k=n_search)\n",
    "            vector_ids = indices[0].tolist()\n",
    "\n",
    "            # Get results from database\n",
    "            results = self._fetch_search_results(vector_ids)\n",
    "            faiss_results = self._prepare_faiss_results(results, vector_ids, distances)\n",
    "\n",
    "            # Apply reranking if enabled\n",
    "            if rerank and faiss_results:\n",
    "                return self._apply_reranking(query, faiss_results, k)\n",
    "\n",
    "            # Return sorted results\n",
    "            return sorted(\n",
    "                faiss_results, key=lambda x: x.get(\"vector_score\", 0), reverse=True\n",
    "            )[:k]\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in filtered search: {str(e)}\")\n",
    "            return []\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def hybrid_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        k: int = 10,\n",
    "        filter_conditions: Optional[Dict[str, Any]] = None,\n",
    "        rerank: bool = True,\n",
    "        query_keywords: Optional[List[str]] = None,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Perform hybrid search using semantic, BM25 ranking and risk-specific filtering\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            filter_conditions: Conditions to filter results (bank, year, quarter, etc.)\n",
    "            rerank: Whether to apply cross-encoder reranking\n",
    "            query_keywords: List of keywords for risk-specific filtering\n",
    "\n",
    "        Returns:\n",
    "            List of search results with metadata\n",
    "        \"\"\"\n",
    "        self._ensure_index_loaded()\n",
    "\n",
    "        try:\n",
    "            # Apply early filtering if possible\n",
    "            if filter_conditions:\n",
    "                initial_results = self._filtered_hybrid_search(\n",
    "                    query, k * 2, filter_conditions, False\n",
    "                )\n",
    "\n",
    "                if not initial_results:\n",
    "                    self.logger.info(\n",
    "                        f\"No results found for filter conditions: {filter_conditions}\"\n",
    "                    )\n",
    "                    return []\n",
    "\n",
    "                # Apply keyword-based filtering if keywords provided\n",
    "                if query_keywords and initial_results:\n",
    "                    # Create document dictionary for BM25\n",
    "                    doc_id_to_text = {\n",
    "                        i: result[\"text\"] for i, result in enumerate(initial_results)\n",
    "                    }\n",
    "\n",
    "                    # Apply BM25 search with specific keywords\n",
    "                    bm25_results = self._search_risk_metric(\n",
    "                        doc_id_to_text, query_keywords, top_n=len(initial_results)\n",
    "                    )\n",
    "\n",
    "                    # Enhance initial results with BM25 scores\n",
    "                    enhanced_results = self._enhance_with_bm25(\n",
    "                        initial_results, bm25_results, doc_id_to_text\n",
    "                    )\n",
    "\n",
    "                    # Apply reranking if enabled\n",
    "                    if rerank:\n",
    "                        final_results = self._apply_reranking(\n",
    "                            query, enhanced_results, k\n",
    "                        )\n",
    "                    else:\n",
    "                        # Sort by combined score\n",
    "                        final_results = sorted(\n",
    "                            enhanced_results,\n",
    "                            key=lambda x: x.get(\n",
    "                                \"combined_score\", x.get(\"vector_score\", 0)\n",
    "                            ),\n",
    "                            reverse=True,\n",
    "                        )[:k]\n",
    "\n",
    "                    return final_results\n",
    "\n",
    "                # No keywords provided, just apply reranking\n",
    "                if rerank:\n",
    "                    return self._apply_reranking(query, initial_results, k)\n",
    "                return sorted(\n",
    "                    initial_results,\n",
    "                    key=lambda x: x.get(\"vector_score\", 0),\n",
    "                    reverse=True,\n",
    "                )[:k]\n",
    "\n",
    "            # No filter conditions, do regular search\n",
    "            # Check if index is empty\n",
    "            if self.index.ntotal == 0:\n",
    "                self.logger.warning(\"FAISS index is empty\")\n",
    "                return []\n",
    "\n",
    "            # Semantic search with FAISS\n",
    "            query_embedding = self.generate_embeddings([query])[0]\n",
    "            query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "            query_embedding = np.array([query_embedding]).astype(\"float32\")\n",
    "\n",
    "            n_search = min(k * 3, self.index.ntotal)\n",
    "            distances, indices = self.index.search(query_embedding, k=n_search)\n",
    "            vector_ids = indices[0].tolist()\n",
    "\n",
    "            # No results found\n",
    "            if not vector_ids or vector_ids[0] == -1:\n",
    "                self.logger.info(\"No semantic search results found\")\n",
    "                return []\n",
    "\n",
    "            # Get metadata for retrieved vectors\n",
    "            results = self._fetch_search_results(vector_ids[:n_search])\n",
    "            if not results:\n",
    "                self.logger.warning(\"No results found in database\")\n",
    "                return []\n",
    "\n",
    "            # Convert to list of dicts with scores\n",
    "            faiss_results = self._prepare_faiss_results(results, vector_ids, distances)\n",
    "\n",
    "            # Apply keyword-based scoring if keywords provided\n",
    "            if query_keywords and faiss_results:\n",
    "                doc_id_to_text = {\n",
    "                    i: result[\"text\"] for i, result in enumerate(faiss_results)\n",
    "                }\n",
    "                bm25_results = self._search_risk_metric(\n",
    "                    doc_id_to_text, query_keywords, top_n=len(faiss_results)\n",
    "                )\n",
    "                combined_results = self._enhance_with_bm25(\n",
    "                    faiss_results, bm25_results, doc_id_to_text\n",
    "                )\n",
    "            else:\n",
    "                combined_results = faiss_results\n",
    "\n",
    "            # Apply reranking if enabled\n",
    "            if rerank and combined_results:\n",
    "                return self._apply_reranking(query, combined_results, k)\n",
    "\n",
    "            # Return sorted results\n",
    "            return sorted(\n",
    "                combined_results,\n",
    "                key=lambda x: x.get(\"combined_score\", x.get(\"vector_score\", 0)),\n",
    "                reverse=True,\n",
    "            )[:k]\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in hybrid_search: {str(e)}\")\n",
    "            import traceback\n",
    "\n",
    "            self.logger.error(traceback.format_exc())\n",
    "            return []\n",
    "\n",
    "    def search_similar_conversations(\n",
    "        self,\n",
    "        conversation_id: int,\n",
    "        k: int = 5,\n",
    "        exclude_same_quarter: bool = True,\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find conversations similar to a given conversation\n",
    "\n",
    "        Args:\n",
    "            conversation_id: ID of the conversation to compare\n",
    "            k: Number of similar conversations to return\n",
    "            exclude_same_quarter: Exclude conversations from the same quarter/year\n",
    "\n",
    "        Returns:\n",
    "            List of similar conversations with metadata\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Get QA pairs for the conversation\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT qp.id, qp.question, qp.answer,\n",
    "                       ac.analyst_name, r.year, r.quarter, b.bank_name\n",
    "                FROM qa_pairs qp\n",
    "                JOIN analyst_conversations ac ON qp.conversation_id = ac.id\n",
    "                JOIN reports r ON ac.report_id = r.id\n",
    "                JOIN banks b ON r.bank_id = b.id\n",
    "                WHERE qp.conversation_id = ?\n",
    "                \"\"\",\n",
    "                (conversation_id,),\n",
    "            )\n",
    "            qa_pairs = cursor.fetchall()\n",
    "            qa_dict = self.db_manager.cursor_to_dict(cursor, qa_pairs)\n",
    "\n",
    "            if not qa_dict:\n",
    "                self.logger.warning(\n",
    "                    f\"No QA pairs found for conversation {conversation_id}\",\n",
    "                )\n",
    "                return []\n",
    "\n",
    "            # Combine all QA texts\n",
    "            qa_texts = [f\"{qa['question']} {qa['answer']}\" for qa in qa_dict]\n",
    "            combined_text = \" \".join(qa_texts)\n",
    "\n",
    "            # Get similar conversations\n",
    "            results = self.hybrid_search(\n",
    "                combined_text,\n",
    "                k=k * 2,\n",
    "            )  # Get more for filtering\n",
    "\n",
    "            # Filter results\n",
    "            if exclude_same_quarter and qa_dict:\n",
    "                # Get quarter and year for original conversation\n",
    "                quarter = qa_dict[0][\"quarter\"]\n",
    "                year = qa_dict[0][\"year\"]\n",
    "                bank = qa_dict[0][\"bank_name\"]\n",
    "\n",
    "                # Filter out same quarter/year/bank\n",
    "                filtered_results = []\n",
    "                for result in results:\n",
    "                    metadata = result.get(\"metadata\", {})\n",
    "                    if (\n",
    "                        metadata.get(\"quarter\") != quarter\n",
    "                        or metadata.get(\"year\") != year\n",
    "                        or metadata.get(\"bank\") != bank\n",
    "                    ):\n",
    "                        filtered_results.append(result)\n",
    "\n",
    "                results = filtered_results\n",
    "\n",
    "            # Return top k results\n",
    "            return results[:k]\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error finding similar conversations: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def topic_search(self, topic_keywords: List[str], k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for content related to specific risk topics using keywords\n",
    "\n",
    "        Args:\n",
    "            topic_keywords: List of topic keywords to search for\n",
    "            k: Number of results to return\n",
    "\n",
    "        Returns:\n",
    "            List of search results with metadata\n",
    "        \"\"\"\n",
    "        # Combine keywords into a query\n",
    "        query = \" \".join(topic_keywords)\n",
    "\n",
    "        # Use the hybrid search with the combined keywords\n",
    "        return self.hybrid_search(query, k=k)\n",
    "\n",
    "    def save_index(self, path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Save the FAISS index to disk\n",
    "\n",
    "        Args:\n",
    "            path: Path to save the index\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            faiss.write_index(self.index, path)\n",
    "            self.logger.info(f\"Saved FAISS index to {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving FAISS index: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def load_index(self, path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Load the FAISS index from disk\n",
    "\n",
    "        Args:\n",
    "            path: Path to load the index from\n",
    "\n",
    "        Returns:\n",
    "            True if successful, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.index = faiss.read_index(path)\n",
    "            self.logger.info(\n",
    "                f\"Loaded FAISS index from {path} with {self.index.ntotal} vectors\",\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading FAISS index: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_index_stats(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get statistics about the FAISS index\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with index statistics\n",
    "        \"\"\"\n",
    "        conn = self.db_manager.get_db_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Get index info from database\n",
    "            cursor.execute(\n",
    "                \"SELECT * FROM faiss_indices WHERE name = ?\",\n",
    "                (self.index_name,),\n",
    "            )\n",
    "            index_info = cursor.fetchone()\n",
    "\n",
    "            if not index_info:\n",
    "                return {\n",
    "                    \"name\": self.index_name,\n",
    "                    \"vectors\": self.index.ntotal,\n",
    "                    \"dimension\": self.embedding_dim,\n",
    "                    \"model\": self.model_name,\n",
    "                }\n",
    "\n",
    "            # Get vector counts by bank, year, quarter\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT\n",
    "                    json_extract(ve.metadata, '$.bank') as bank,\n",
    "                    json_extract(ve.metadata, '$.year') as year,\n",
    "                    json_extract(ve.metadata, '$.quarter') as quarter,\n",
    "                    COUNT(*) as count\n",
    "                FROM vector_embeddings ve\n",
    "                WHERE ve.embedding_model = ?\n",
    "                GROUP BY bank, year, quarter\n",
    "                ORDER BY bank, year, quarter\n",
    "                \"\"\",\n",
    "                (self.model_name,),\n",
    "            )\n",
    "            distribution = self.db_manager.cursor_to_dict(cursor, cursor.fetchall())\n",
    "\n",
    "            # Convert from SQLite row to dict\n",
    "            info = dict(index_info)\n",
    "\n",
    "            # Add runtime info\n",
    "            info.update(\n",
    "                {\"vectors_in_memory\": self.index.ntotal, \"distribution\": distribution},\n",
    "            )\n",
    "\n",
    "            return info\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting index stats: {str(e)}\")\n",
    "            return {\"name\": self.index_name, \"error\": str(e)}\n",
    "\n",
    "        finally:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TydSi8JLsOCp"
   },
   "source": [
    "### üè¶üí°üß† Financial Expert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2E0OLV1tQBd_"
   },
   "source": [
    "#### üåêüîç Web Search & API Integrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WrPbUG8tQLhO"
   },
   "outputs": [],
   "source": [
    "class BankNewsSearcher:\n",
    "    \"\"\"Class to search for and analyze bank news articles\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_api_key: str,\n",
    "        brightdata_api_key: str,\n",
    "        google_api_key: Optional[str],\n",
    "        google_cx: Optional[str],\n",
    "        serper_api_key: Optional[str],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize with necessary API keys\n",
    "\n",
    "        Args:\n",
    "            openai_api_key: API key for OpenAI\n",
    "            brightdata_api_key: API key for BrightData Web Unlocker\n",
    "            google_api_key: API key for Google Custom Search (optional)\n",
    "            google_cx: Custom Search Engine ID (optional)\n",
    "            serper_api_key: API key for Serper.dev (optional)\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.OPENAI_API_KEY = openai_api_key\n",
    "        self.BRIGHTDATA_API_KEY = brightdata_api_key\n",
    "        self.GOOGLE_API_KEY = google_api_key\n",
    "        self.GOOGLE_CX = google_cx\n",
    "        self.SERPER_API_KEY = serper_api_key\n",
    "\n",
    "    def generate_fact_check_queries(\n",
    "        self, sentences: List[str], quarter: str, year: str, bank: str, risk_type: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates two highly optimized fact-checking queries using journalistic terms for financial news.\n",
    "\n",
    "        Args:\n",
    "            sentences: List of sentences to generate queries for\n",
    "            quarter: Quarter (e.g., \"Q1\", \"Q2\")\n",
    "            year: Year (e.g., \"2023\")\n",
    "            bank: Bank name\n",
    "            risk_type: Type of risk\n",
    "\n",
    "        Returns:\n",
    "            List of generated queries\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are a financial analyst and fact-checking expert. Convert the following financial insights into **two** short, keyword-rich search queries that match **how journalists report financial news**.\n",
    "### **Example Format**\n",
    "**Input:**\n",
    "\"In Q3 2024, Goldman Sachs' market risk was significantly impacted by increased equity market volatility. The bank adjusted its trading strategy by increasing hedging in derivatives and diversifying across asset classes. Rising interest rates and geopolitical instability led to higher VaR, prompting risk management interventions.\"\n",
    "**Output:**\n",
    "[\"Goldman Sachs Q3 2024 stock market volatility impact\",\n",
    "\"Goldman Sachs Q3 2024 hedging strategy shifts\"]\n",
    "### **Financial Insights:**\n",
    "{json.dumps(sentences)}\n",
    "### **Instructions:**\n",
    "- **Generate exactly two optimized search queries.**\n",
    "- **Use journalistic terms** that align with financial news reporting.\n",
    "- **Prioritize words used in financial headlines** (e.g., \"cash reserves,\" \"liquidity crunch,\" \"capital risk\").\n",
    "- Keep **each query under 7 words**.\n",
    "- **DO NOT** phrase them as questions.\n",
    "- **Avoid overly technical finance jargon** (e.g., instead of \"VaR increase,\" use \"risk exposure rises\").\n",
    "- Format: `\"Bank + Quarter + Year + Key Financial Terms\"`.\n",
    "- **Prioritize terms journalists use** to describe financial risk, capital management, and liquidity.\n",
    "- **Generate two queries per insight.**\n",
    "**Example Output Format:**\n",
    "```json\n",
    "[\"Query 1\", \"Query 2\"]\n",
    "```\n",
    "\"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.OPENAI_API_KEY}\"}\n",
    "        data = {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.1,  # Lower temperature for precision\n",
    "        }\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\", headers=headers, json=data\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                response_json = response.json()\n",
    "                raw_content = (\n",
    "                    response_json.get(\"choices\", [{}])[0]\n",
    "                    .get(\"message\", {})\n",
    "                    .get(\"content\", \"\")\n",
    "                    .strip()\n",
    "                )\n",
    "                # Remove markdown if present\n",
    "                if raw_content.startswith(\"```json\"):\n",
    "                    raw_content = (\n",
    "                        raw_content.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "                    )\n",
    "                return json.loads(raw_content)  # Convert JSON to Python list\n",
    "            except json.JSONDecodeError as e:\n",
    "                self.logger.error(f\"JSON Parsing Error: {e}\\nResponse: {raw_content}\")\n",
    "                return []\n",
    "        else:\n",
    "            self.logger.error(\n",
    "                f\"GPT-4o API Error: {response.status_code} - {response.text}\"\n",
    "            )\n",
    "            return []\n",
    "\n",
    "    def build_search_queries(\n",
    "        self, queries: List[str], bank: str, quarter: str, year: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Build search queries with source restrictions\n",
    "\n",
    "        Args:\n",
    "            queries: List of base queries\n",
    "            bank: Bank name\n",
    "            quarter: Quarter\n",
    "            year: Year\n",
    "\n",
    "        Returns:\n",
    "            List of search queries with source restrictions\n",
    "        \"\"\"\n",
    "        sources = [\n",
    "            # \"site:bloomberg.com\",\n",
    "            \"site:reuters.com\",\n",
    "            # \"site:wsj.com\",\n",
    "            \"site:cnbc.com\",\n",
    "        ]\n",
    "        search_queries = []\n",
    "        for source in sources:\n",
    "            for query in queries:\n",
    "                search_queries.append(f\"{query} {source}\")\n",
    "        return search_queries\n",
    "\n",
    "    def extract_article_summary(self, soup: BeautifulSoup) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extracts the main content from an article page.\n",
    "\n",
    "        Args:\n",
    "            soup: BeautifulSoup object of article page\n",
    "\n",
    "        Returns:\n",
    "            Summary string or None\n",
    "        \"\"\"\n",
    "        # Try different metadata sources for the summary\n",
    "        for tag in [\"description\", \"og:description\", \"twitter:description\"]:\n",
    "            meta_tag = soup.find(\"meta\", {\"name\": tag}) or soup.find(\n",
    "                \"meta\", {\"property\": tag}\n",
    "            )\n",
    "            if meta_tag and \"content\" in meta_tag.attrs:\n",
    "                return meta_tag[\"content\"].strip()\n",
    "\n",
    "        # Try extracting text from article body\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        if paragraphs:\n",
    "            return \" \".join(\n",
    "                [p.get_text().strip() for p in paragraphs[:3]]\n",
    "            )  # Take first 3 paragraphs\n",
    "\n",
    "        return None\n",
    "\n",
    "    def check_article_summary(self, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Checks if an article is accessible and retrieves a summary using BrightData.\n",
    "\n",
    "        Args:\n",
    "            url: URL of article to check\n",
    "\n",
    "        Returns:\n",
    "            Summary of article or error message\n",
    "        \"\"\"\n",
    "        # Fetch with BrightData Web Unlocker\n",
    "        brightdata_url = \"https://api.brightdata.com/request\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.BRIGHTDATA_API_KEY}\",\n",
    "        }\n",
    "        payload = {\n",
    "            \"zone\": \"web_unlocker1\",  # BrightData Web Unlocker\n",
    "            \"url\": url,\n",
    "            \"format\": \"raw\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                brightdata_url, headers=headers, json=payload, timeout=15\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                summary = self.extract_article_summary(soup)\n",
    "                if summary:\n",
    "                    return summary\n",
    "\n",
    "                # If no main summary is found, check meta description\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                meta_description = soup.find(\"meta\", {\"name\": \"description\"})\n",
    "                if meta_description and \"content\" in meta_description.attrs:\n",
    "                    meta_summary = meta_description[\"content\"].strip()\n",
    "                    return meta_summary\n",
    "\n",
    "                return \"No summary available\"\n",
    "            self.logger.warning(f\"BrightData request failed: {response.status_code}\")\n",
    "            return \"Failed to access article\"\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error checking article summary: {str(e)}\")\n",
    "            return \"Error accessing article\"\n",
    "\n",
    "    def search_google_cloud(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Searches Google for financial news using Google Cloud Custom Search API.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "\n",
    "        Returns:\n",
    "            List of article summaries\n",
    "        \"\"\"\n",
    "        if not self.GOOGLE_API_KEY or not self.GOOGLE_CX:\n",
    "            self.logger.warning(\"Google API key or CX not provided\")\n",
    "            return []\n",
    "\n",
    "        article_summaries = []\n",
    "        url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={self.GOOGLE_API_KEY}&cx={self.GOOGLE_CX}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"items\", [])\n",
    "                for result in results[:5]:  # Limit to top 5 results\n",
    "                    summary = self.check_article_summary(result[\"link\"])\n",
    "                    article = {\n",
    "                        \"title\": result[\"title\"],\n",
    "                        \"summary\": summary,\n",
    "                    }\n",
    "                    article_summaries.append(article)\n",
    "            else:\n",
    "                self.logger.warning(\n",
    "                    f\"Google Cloud search failed: {response.status_code}\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in Google Cloud search: {str(e)}\")\n",
    "\n",
    "        return article_summaries\n",
    "\n",
    "    def search_google_serper(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Searches Google for financial news using Serper.dev API.\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "\n",
    "        Returns:\n",
    "            List of article summaries\n",
    "        \"\"\"\n",
    "        if not self.SERPER_API_KEY:\n",
    "            self.logger.warning(\"Serper API key not provided\")\n",
    "            return []\n",
    "\n",
    "        article_summaries = []\n",
    "        url = \"https://google.serper.dev/news\"\n",
    "        headers = {\"X-API-KEY\": self.SERPER_API_KEY}\n",
    "        payload = {\"q\": query, \"num\": 3}\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                results = response.json().get(\"news\", [])\n",
    "                for result in results:\n",
    "                    summary = self.check_article_summary(result[\"link\"])\n",
    "                    article = {\n",
    "                        \"title\": result[\"title\"],\n",
    "                        \"summary\": summary,\n",
    "                    }\n",
    "                    article_summaries.append(article)\n",
    "            else:\n",
    "                self.logger.warning(f\"Serper search failed: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in Serper search: {str(e)}\")\n",
    "\n",
    "        return article_summaries\n",
    "\n",
    "    def search_all_sources(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search all available sources (Google Cloud and Serper)\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "\n",
    "        Returns:\n",
    "            Combined list of article summaries\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "\n",
    "        # Use Google Cloud if API key and CX provided\n",
    "        if self.GOOGLE_API_KEY and self.GOOGLE_CX:\n",
    "            google_results = self.search_google_cloud(query)\n",
    "            all_results.extend(google_results)\n",
    "\n",
    "        # Use Serper if API key provided\n",
    "        if self.SERPER_API_KEY:\n",
    "            serper_results = self.search_google_serper(query)\n",
    "            all_results.extend(serper_results)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def search_with_concurrent_requests(self, queries: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search multiple queries concurrently for faster results\n",
    "\n",
    "        Args:\n",
    "            queries: List of search queries\n",
    "\n",
    "        Returns:\n",
    "            Combined list of article summaries\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "\n",
    "        # Use ThreadPoolExecutor for concurrent requests\n",
    "        with ThreadPoolExecutor(max_workers=min(10, len(queries))) as executor:\n",
    "            future_to_query = {\n",
    "                executor.submit(self.search_all_sources, query): query\n",
    "                for query in queries\n",
    "            }\n",
    "            for future in as_completed(future_to_query):\n",
    "                query = future_to_query[future]\n",
    "                try:\n",
    "                    results = future.result()\n",
    "                    all_results.extend(results)\n",
    "                    self.logger.info(\n",
    "                        f\"Completed search for query: {query} - Found {len(results)} results\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error searching query {query}: {str(e)}\")\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def run_bank_news_search(\n",
    "        self, bank: str, quarter: str, year: str, sentences: List[str], risk_type: str\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Run complete bank news search process\n",
    "\n",
    "        Args:\n",
    "            bank: Bank name\n",
    "            quarter: Quarter\n",
    "            year: Year\n",
    "            sentences: List of sentences to generate queries from\n",
    "            risk_type: Type of risk\n",
    "\n",
    "        Returns:\n",
    "            List of article summaries\n",
    "        \"\"\"\n",
    "        # Step 1: Generate fact-check queries\n",
    "        self.logger.info(\n",
    "            f\"Generating queries for {bank} {quarter} {year} ({risk_type})\"\n",
    "        )\n",
    "        queries = self.generate_fact_check_queries(\n",
    "            sentences, quarter, year, bank, risk_type\n",
    "        )\n",
    "\n",
    "        if not queries:\n",
    "            self.logger.warning(\"Failed to generate queries\")\n",
    "            return []\n",
    "\n",
    "        # Step 2: Build search queries with source restrictions\n",
    "        search_queries = self.build_search_queries(queries, bank, quarter, year)\n",
    "        self.logger.info(f\"Generated {len(search_queries)} search queries\")\n",
    "\n",
    "        # Step 3: Search with concurrent requests\n",
    "        self.logger.info(\"Searching for articles...\")\n",
    "        results = self.search_with_concurrent_requests(search_queries)\n",
    "\n",
    "        self.logger.info(f\"Found {len(results)} total results\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX4LekNvQUWF"
   },
   "source": [
    "#### ü§ñüõ†Ô∏è Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ErxGdpG1ZMvb"
   },
   "outputs": [],
   "source": [
    "class FinancialExpert:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        db: GSIBDatabase,\n",
    "        faiss_manager: FAISSManager,\n",
    "    ):\n",
    "        self.db = db\n",
    "        self.faiss_manager = faiss_manager\n",
    "        self.llm = llm\n",
    "        self.client = OpenAI()\n",
    "        self.searcher = BankNewsSearcher(\n",
    "            openai_api_key=os.getenv('OPENAI_API_KEY'),\n",
    "            brightdata_api_key=os.getenv('BRIGHTDATA_API_KEY'),\n",
    "            google_api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "            google_cx=os.getenv('GOOGLE_CX'),\n",
    "            serper_api_key=os.getenv('SERPER_API_KEY'),\n",
    "        )\n",
    "        self.keywords = {\n",
    "            \"liquidity_risk\": [\n",
    "                \"liquidity score\",\n",
    "                \"liquidity position\",\n",
    "                \"cash reserves\",\n",
    "                \"funding costs\",\n",
    "                \"liquidity coverage ratio (LCR)\",\n",
    "                \"deposit inflows\",\n",
    "                \"deposit outflows\",\n",
    "                \"cash flow\",\n",
    "                \"liquidity risk\",\n",
    "                \"short-term funding\",\n",
    "                \"working capital\",\n",
    "                \"cash holdings\",\n",
    "                \"high-quality liquid assets (HQLA)\",\n",
    "                \"wholesale funding\",\n",
    "                \"liquidity buffers\",\n",
    "                \"repo market\",\n",
    "                \"liquidity crunch\",\n",
    "                \"central bank liquidity support\",\n",
    "                \"stress testing\",\n",
    "                \"liquidity management\",\n",
    "                \"maturity mismatch\",\n",
    "                \"funding liquidity\",\n",
    "                \"interbank lending market\",\n",
    "                \"liquidity shocks\",\n",
    "                \"contingent liquidity risk\",\n",
    "                \"monetary policy impact on liquidity\",\n",
    "            ],\n",
    "            \"credit_risk\": [\n",
    "                \"credit risk score\",\n",
    "                \"non-performing loans (NPL)\",\n",
    "                \"Stage 3 loans\",\n",
    "                \"loan defaults\",\n",
    "                \"credit exposure\",\n",
    "                \"corporate lending\",\n",
    "                \"consumer lending\",\n",
    "                \"creditworthiness\",\n",
    "                \"loan loss provisions\",\n",
    "                \"credit spreads\",\n",
    "                \"default probability\",\n",
    "                \"credit derivatives\",\n",
    "                \"sovereign risk\",\n",
    "                \"counterparty credit risk\",\n",
    "                \"credit risk stress testing\",\n",
    "                \"credit rating downgrades\",\n",
    "                \"exposure at default (EAD)\",\n",
    "                \"probability of default (PD)\",\n",
    "                \"loss given default (LGD)\",\n",
    "                \"credit market deterioration\",\n",
    "            ],\n",
    "            \"market_risk\": [\n",
    "                \"market risk score\",\n",
    "                \"volatility\",\n",
    "                \"equity markets\",\n",
    "                \"bond markets\",\n",
    "                \"interest rate risk\",\n",
    "                \"currency risk\",\n",
    "                \"foreign exchange risk\",\n",
    "                \"commodity prices\",\n",
    "                \"value at risk (VaR)\",\n",
    "                \"derivatives pricing\",\n",
    "                \"yield curve shifts\",\n",
    "                \"credit spread risk\",\n",
    "                \"equity price fluctuations\",\n",
    "                \"market liquidity risk\",\n",
    "                \"macro-economic risk\",\n",
    "                \"systemic market risk\",\n",
    "                \"hedge fund exposure\",\n",
    "                \"interest rate swaps\",\n",
    "                \"global market trends\",\n",
    "                \"inflation risk\",\n",
    "                \"cross-asset correlations\",\n",
    "                \"geopolitical market uncertainty\",\n",
    "                \"financial contagion\",\n",
    "                \"option volatility skew\",\n",
    "            ],\n",
    "            \"group_risk\": [\n",
    "                \"group risk score\",\n",
    "                \"capital allocation\",\n",
    "                \"global risk exposure\",\n",
    "                \"group-level risks\",\n",
    "                \"interconnected risks\",\n",
    "                \"economic downturn\",\n",
    "                \"derivatives exposure\",\n",
    "                \"systemic risk\",\n",
    "                \"parent-subsidiary exposure\",\n",
    "                \"conglomerate risk\",\n",
    "                \"risk concentration\",\n",
    "                \"interbank dependencies\",\n",
    "                \"shadow banking risks\",\n",
    "                \"cross-border regulatory risks\",\n",
    "                \"capital adequacy ratios\",\n",
    "                \"leverage ratios\",\n",
    "                \"financial stability risks\",\n",
    "                \"macroprudential policies\",\n",
    "                \"contagion effects\",\n",
    "                \"operational interdependencies\",\n",
    "                \"group solvency stress tests\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        self.risk_questions_map = {\n",
    "            \"liquidity_risk\": [\n",
    "                lambda bank,\n",
    "                q,\n",
    "                y: f\"What liquidity challenges did {bank} face in {q} {y}, and how were its liquidity score and deposit stability impacted by external market conditions?\",\n",
    "            ],\n",
    "            \"credit_risk\": [\n",
    "                lambda bank,\n",
    "                q,\n",
    "                y: f\"What were the key drivers of changes in {bank}'s credit risk during {q} {y}, and what factors influence its risk profile?\",\n",
    "            ],\n",
    "            \"market_risk\": [\n",
    "                lambda bank,\n",
    "                q,\n",
    "                y: f\"How did {bank} manage market risk in {q} {y}, considering equity volatility, interest rate changes, and exposure to commodity and foreign exchange fluctuations?\",\n",
    "            ],\n",
    "            \"group_risk\": [\n",
    "                lambda bank,\n",
    "                q,\n",
    "                y: f\"What were the key drivers of {bank}'s group risk in {q} {y}, including global market volatility, capital allocation, and other systemic exposures?\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(\"Initializing FinancialExpert\")\n",
    "\n",
    "    def _rate_limited_api_call(self, method, **kwargs):\n",
    "        \"\"\"\n",
    "        Execute API calls with exponential backoff for rate limiting.\n",
    "\n",
    "        Args:\n",
    "            method: The API method to call\n",
    "            **kwargs: Arguments to pass to the method\n",
    "\n",
    "        Returns:\n",
    "            API response or None on failure\n",
    "        \"\"\"\n",
    "        max_retries = 5\n",
    "        base_delay = 1\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                return method(**kwargs)\n",
    "            except Exception as e:\n",
    "                # Check if this is a rate limit error\n",
    "                if (\n",
    "                    \"rate_limit\" in str(e).lower()\n",
    "                    or \"too many requests\" in str(e).lower()\n",
    "                ):\n",
    "                    delay = base_delay * (2**attempt) + random.uniform(0, 1)\n",
    "                    self.logger.warning(\n",
    "                        f\"Rate limit hit. Retrying in {delay:.2f} seconds...\",\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                # For other exceptions, log and return None\n",
    "                self.logger.error(f\"API call failed: {e}\")\n",
    "                return None\n",
    "\n",
    "        self.logger.error(f\"API call failed after {max_retries} retries\")\n",
    "        return None\n",
    "\n",
    "    def _get_last_3_quarters(self, quarter=None, year=None):\n",
    "        \"\"\"\n",
    "        Get the last 3 quarters previous to the given input for financial report analysis purposes.\n",
    "\n",
    "        Args:\n",
    "            quarter (str): Current quarter (e.g., \"Q2\"), None for all quarters\n",
    "            year (int): Current year\n",
    "\n",
    "        Returns:\n",
    "            list: List of (quarter, year) tuples for the last 3 quarters\n",
    "        \"\"\"\n",
    "        if not quarter or not year:\n",
    "            raise ValueError(\"Both quarter and year must be provided.\")\n",
    "\n",
    "        # Mapping of quarters to their respective numbers\n",
    "        quarters_mapping = {\"Q1\": 1, \"Q2\": 2, \"Q3\": 3, \"Q4\": 4}\n",
    "        reverse_quarters_mapping = {v: k for k, v in quarters_mapping.items()}\n",
    "\n",
    "        # Initialize the list with the given quarter and year\n",
    "        quarters_relevant = [(quarter, year)]\n",
    "\n",
    "        while len(quarters_relevant) < 3:\n",
    "            current_quarter, current_year = quarters_relevant[-1]\n",
    "            current_quarter_num = quarters_mapping[current_quarter]\n",
    "\n",
    "            # Move to the previous quarter\n",
    "            if current_quarter_num == 1:\n",
    "                next_quarter_num = 4\n",
    "                next_year = current_year - 1\n",
    "            else:\n",
    "                next_quarter_num = current_quarter_num - 1\n",
    "                next_year = current_year\n",
    "\n",
    "            next_quarter = reverse_quarters_mapping[next_quarter_num]\n",
    "            quarters_relevant.append((next_quarter, next_year))\n",
    "\n",
    "        return quarters_relevant\n",
    "\n",
    "    def _get_historical_context(self, bank, risk_type, current_quarter, current_year):\n",
    "        \"\"\"\n",
    "        Get historical context from previous quarters.\n",
    "\n",
    "        Args:\n",
    "            bank (str): Bank name\n",
    "            risk_type (str): Risk type\n",
    "            current_quarter (str): Current quarter\n",
    "            current_year (int): Current year\n",
    "\n",
    "        Returns:\n",
    "            tuple: (insights_dict, alignment_report_dict)\n",
    "                insights_dict: Dictionary containing previous quarter and one before previous quarter insights\n",
    "                alignment_report_dict: Dictionary containing previous quarter and one before previous quarter alignment reports\n",
    "        \"\"\"\n",
    "        # Get the last 3 quarters including the current one\n",
    "        last_3_quarters = self._get_last_3_quarters(current_quarter, current_year)\n",
    "        # Skip the current quarter (first in the list)\n",
    "        historical_quarters = last_3_quarters[1:]\n",
    "\n",
    "        insights_dict = {\n",
    "            \"previous\": \"No insights available\",\n",
    "            \"one_before_previous\": \"No insights available\",\n",
    "        }\n",
    "\n",
    "        alignment_report_dict = {\n",
    "            \"previous\": \"No alignment data available\",\n",
    "            \"one_before_previous\": \"No alignment data available\",\n",
    "        }\n",
    "\n",
    "        for idx, (quarter, year) in enumerate(historical_quarters):\n",
    "            report_id = self.db.get_report_id(bank, year, quarter)\n",
    "            if not report_id:\n",
    "                continue\n",
    "\n",
    "            metrics = self.db.get_metrics_by_report(report_id)\n",
    "            if len(metrics) == 0:\n",
    "                continue\n",
    "\n",
    "            # Get insights\n",
    "            insight_key = f\"AI_INSIGHT_{risk_type.upper()}\"\n",
    "            insight = self.db.get_metric_formatted_value(metrics, insight_key)\n",
    "            if insight:\n",
    "                if idx == 0:  # Last quarter\n",
    "                    insights_dict[\"previous\"] = insight\n",
    "                elif idx == 1:  # Last but one quarter\n",
    "                    insights_dict[\"one_before_previous\"] = insight\n",
    "\n",
    "            # Get alignment reports\n",
    "            alignment_key = f\"AI_ALIGNMENT_{risk_type.upper()}\"\n",
    "            alignment = self.db.get_metric_formatted_value(metrics, alignment_key)\n",
    "            if alignment:\n",
    "                if idx == 0:  # Last quarter\n",
    "                    alignment_report_dict[\"previous\"] = alignment\n",
    "                elif idx == 1:  # Last but one quarter\n",
    "                    alignment_report_dict[\"one_before_previous\"] = alignment\n",
    "\n",
    "        return insights_dict, alignment_report_dict\n",
    "\n",
    "    def _generate_answers_with_gen_ai(self, bank, quarter, year, risk_type, questions):\n",
    "        \"\"\"\n",
    "        Generate risk assessment answers using RAG with FAISS and GenAI\n",
    "\n",
    "        Args:\n",
    "            bank: Bank name\n",
    "            quarter: Quarter (e.g., 'Q1')\n",
    "            year: Year (e.g., 2023)\n",
    "            risk_type: Risk type (e.g., 'liquidity_risk')\n",
    "            questions: List of questions to answer\n",
    "\n",
    "        Returns:\n",
    "            List of generated reports\n",
    "        \"\"\"\n",
    "\n",
    "        all_reports = []\n",
    "\n",
    "        for question in questions:\n",
    "            # Retrieve relevant context using FAISS with keywords\n",
    "            contexts = self.faiss_manager.hybrid_search(\n",
    "                query=question,\n",
    "                k=5,\n",
    "                filter_conditions={\"bank\": bank, \"quarter\": quarter, \"year\": year},\n",
    "                query_keywords=self.keywords.get(risk_type),\n",
    "            )\n",
    "\n",
    "            for context in contexts:\n",
    "                prompt = f\"\"\"\n",
    "                    You are a senior risk analyst at the Bank of England's Prudential Regulation Authority (PRA), assessing {bank}'s {quarter} {year} earnings call.\n",
    "\n",
    "                    **Analyst Question:**\n",
    "                    \"{context[\"question\"]}\" - {context[\"metadata\"][\"analyst_name\"]}, {context[\"metadata\"][\"analyst_company\"]}\n",
    "\n",
    "                    **Executive Response:**\n",
    "                    \"{context[\"answer\"]}\" - {context[\"answer_speaker\"]}, {context[\"answer_role\"]}\n",
    "\n",
    "                    **Your Task:**\n",
    "                    Analyze the above exchange for credit risk implications and write a focused assessment that answers:\n",
    "                    {question}\n",
    "\n",
    "                    **Instructions:**\n",
    "                    - Write a **formal financial report** summarizing all relevant insights.\n",
    "                    - Structure the response with **an introduction, analysis, and a conclusion**.\n",
    "                    - Avoid bullet points, numbered lists, or itemized formats.\n",
    "                    - Ensure the report is **concise (100-150 words)** but informative.\n",
    "                    - If the context does not provide direct insights, infer logical conclusions.\n",
    "                    - If no relevant information is found, state: \"No insights available based on the given data.\"\n",
    "                    \"\"\"\n",
    "\n",
    "                # Analyze using OpenAI\n",
    "                response = self._rate_limited_api_call(\n",
    "                    self.client.chat.completions.create,\n",
    "                    model=OPENAI_MODEL,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"You are a financial analyst answering strictly based on the provided context.\",\n",
    "                        },\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    max_tokens=500,\n",
    "                    temperature=0.2,\n",
    "                )\n",
    "                answer = response.choices[0].message.content.strip()\n",
    "\n",
    "                # Analyze using Local LLM (Mistral-Small-24B-Instruct)\n",
    "                # answer = self.llm.generate(prompt=prompt, max_tokens=256, temperature=0.2)\n",
    "\n",
    "                if (\n",
    "                    \"no insights available\" not in answer.lower()\n",
    "                    and \"not enough information\" not in answer.lower()\n",
    "                ):\n",
    "                    if answer not in all_reports:  # Deduplication\n",
    "                        all_reports.append(answer)\n",
    "\n",
    "        return all_reports\n",
    "\n",
    "    def _validate_article_relevance(self, summaries, document):\n",
    "        \"\"\"\n",
    "        Evaluates if news article summaries align with the document.\n",
    "        Returns an alignment summary and score.\n",
    "        \"\"\"\n",
    "        if not summaries:\n",
    "            return \"No relevant news articles found to validate alignment.\"\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant evaluating whether financial news summaries create a broader financial context that aligns \"\n",
    "                \"with the document's claims. The summaries do not need to directly confirm the document's points but should provide \"\n",
    "                \"a financial picture that makes the document's statements seem plausible or implausible in a broad way.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The document states:\\n\\n{document}\\n\\nHere are some article summaries:\\n\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        for article in summaries:\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Title: {article['title']}\\nSummary: {article['summary']}\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Based on these summaries, does the overall financial picture align with what is stated in the document? \"\n",
    "                \"If yes, explain briefly why. If no, highlight any contradictions or gaps in information.\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self._rate_limited_api_call(\n",
    "                self.client.chat.completions.create,\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=messages,\n",
    "            )\n",
    "\n",
    "            full_assessment = response.choices[0].message.content.strip()\n",
    "\n",
    "            # üîç Strip potential leaked instructions from full_assessment\n",
    "            if \"You are an AI assistant\" in full_assessment:\n",
    "                full_assessment = full_assessment.split(\"\\n\", 1)[\n",
    "                    -1\n",
    "                ]  # Remove AI-generated setup lines\n",
    "\n",
    "            if \"Your task is\" in full_assessment:\n",
    "                full_assessment = full_assessment.split(\"Your task is\", 1)[-1].strip()\n",
    "\n",
    "            # Get concise summary and score\n",
    "            summary_request = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an AI assistant summarizing a financial alignment assessment. \"\n",
    "                    \"Your task is to provide a short summary (2-3 sentences) and an alignment score from 0 to 100, \"\n",
    "                    \"where 0 means completely misaligned, and 100 means fully aligned.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Here is the full assessment:\\n\\n{full_assessment}\\n\\nSummarize it concisely and provide an alignment score.\",\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            response_2 = self._rate_limited_api_call(\n",
    "                self.client.chat.completions.create,\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=summary_request,\n",
    "            )\n",
    "\n",
    "            return response_2.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error validating article relevance: {e}\")\n",
    "            return \"Error validating alignment.\"\n",
    "\n",
    "    def _analyze_risk(self, report, insights_dict, alignment_report_dict, risk_type):\n",
    "        \"\"\"\n",
    "        Analyzes the risk level based on the insight and alignment report.\n",
    "        Returns risk level and summary formatted in a professional manner.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            quarter = report.get(\"quarter\")\n",
    "            quarter_number = int(\n",
    "                \"\".join(filter(str.isdigit, quarter))\n",
    "            )  # Extracts quarter number\n",
    "            year = report.get(\"year\")\n",
    "            bank = report.get(\"bank_name\")\n",
    "            current_insights = insights_dict.get(\"current\", \"No insights available\")\n",
    "            previous_insights = insights_dict.get(\"previous\", \"No insights available\")\n",
    "            current_alignment = alignment_report_dict.get(\n",
    "                \"current\", \"No alignment data available.\"\n",
    "            )\n",
    "            previous_alignment = alignment_report_dict.get(\n",
    "                \"previous\", \"No alignment data available.\"\n",
    "            )\n",
    "            one_before_previous_insights = insights_dict.get(\n",
    "                \"one_before_previous\", \"No insights available\"\n",
    "            )\n",
    "            one_before_previous_alignment = alignment_report_dict.get(\n",
    "                \"one_before_previous\", \"No alignment data available.\"\n",
    "            )\n",
    "\n",
    "            # Claude/Sonnet suggested\n",
    "            # messages = [\n",
    "            #     {\n",
    "            #         \"role\": \"system\",\n",
    "            #         \"content\": (\n",
    "            #             f\"You are a financial risk assessment expert at the Bank of England's Prudential Regulation Authority (PRA), \"\n",
    "            #             f\"specializing in {risk_type} risk analysis for Global Systemically Important Banks (G-SIBs). \"\n",
    "            #             f\"Your task is to produce a professional, concise risk assessment that follows this exact format:\\n\\n\"\n",
    "            #             f\"1. A structured summary section titled '{bank} {quarter} {year} Insights Summary'\\n\"\n",
    "            #             f\"2. A 'Key Focus Areas' section with 2-3 paragraphs summarizing the bank's primary {risk_type} risk challenges and mitigation strategies\\n\"\n",
    "            #             f\"3. An 'Evolution from Previous Quarters' section showing clear bullet points of the progression across quarters\\n\"\n",
    "            #             f\"4. A 'Key {risk_type.title()} Risk Challenges' section with bullet points for each significant risk factor\\n\"\n",
    "            #             f\"5. A 'Performance Scores' section with objective Alignment and Risk Exposure scores based on the data\\n\\n\"\n",
    "            #             f\"Your assessment must be evidence-based, professional, and formatted exactly like the reference format provided.\"\n",
    "            #         )\n",
    "            #     },\n",
    "            #     {\n",
    "            #         \"role\": \"user\",\n",
    "            #         \"content\": (\n",
    "            #             f\"### Bank: {bank} ({risk_type}) - {quarter} {year}\\n\\n\"\n",
    "            #             f\"üîπ **Current quarter Insights:**\\n{current_insights}\\n\\n\"\n",
    "            #             f\"üîπ **Previous quarter Insights:**\\n{previous_insights}\\n\\n\"\n",
    "            #             f\"üîπ **Last but one quarter Insights:**\\n{one_before_previous_insights}\\n\\n\"\n",
    "            #             f\"üîπ **Previous two quarters Alignment Reports:**\\n\"\n",
    "            #             f\"Last quarter: {previous_alignment}\\n\"\n",
    "            #             f\"Quarter before last: {one_before_previous_alignment}\\n\\n\"\n",
    "            #             f\"Create a professional risk assessment report that looks exactly like this format:\\n\\n\"\n",
    "            #             f\"```\\n\"\n",
    "            #             f\"{bank} {quarter} {year} Insights Summary\\n\\n\"\n",
    "            #             f\"Key Focus Areas\\n\"\n",
    "            #             f\"[2-3 paragraphs about current quarter key challenges and mitigation strategies]\\n\\n\"\n",
    "            #             f\"Evolution from Previous Quarters\\n\"\n",
    "            #             f\"‚Ä¢ [Quarter-2]: [Brief summary]\\n\"\n",
    "            #             f\"‚Ä¢ [Quarter-1]: [Brief summary]\\n\"\n",
    "            #             f\"‚Ä¢ [Current Quarter]: [Brief summary]\\n\\n\"\n",
    "            #             f\"Key {risk_type.title()} Risk Challenges\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 1]\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 2]\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 3]\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 4]\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 5]\\n\"\n",
    "            #             f\"‚Ä¢ [Challenge 6]\\n\\n\"\n",
    "            #             f\"Performance Scores\\n\"\n",
    "            #             f\"‚Ä¢ Alignment Score: [High/Medium/Low]\\n\"\n",
    "            #             f\"‚Ä¢ Risk Exposure Score: [High/Medium/Low]\\n\"\n",
    "            #             f\"```\\n\\n\"\n",
    "            #             f\"Use the insights data to explicitly identify trends across quarters. The Key Focus Areas should \"\n",
    "            #             f\"address both problems and the bank's mitigation strategies. The alignment score measures how well \"\n",
    "            #             f\"external information confirms the bank's statements, while the risk score measures their exposure to {risk_type} risk.\"\n",
    "            #         )\n",
    "            #     }\n",
    "            # ]\n",
    "\n",
    "            # GPT4o suggested\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        f\"You are a financial risk assessment expert at the Bank of England's Prudential Regulation Authority (PRA), \"\n",
    "                        f\"specializing in {risk_type} risk analysis for Global Systemically Important Banks (G-SIBs). \"\n",
    "                        f\"Your task is to produce a professional, concise risk assessment that follows this exact format:\\n\\n\"\n",
    "                        f\"1. A structured summary section titled '{bank} {quarter} {year} Insights Summary'\\n\"\n",
    "                        f\"2. A 'Key Focus Areas' section with 2-3 paragraphs summarizing the bank's primary {risk_type} risk challenges \"\n",
    "                        f\"and explicit mitigation strategies implemented.\\n\"\n",
    "                        f\"3. An 'Evolution from Previous Quarters' section, showing clear bullet points on risk progression, \"\n",
    "                        f\"including whether risks have increased, decreased, or remained stable.\\n\"\n",
    "                        f\"4. A 'Key {risk_type.title()} Risk Challenges' section listing the most significant risk factors.\\n\"\n",
    "                        f\"5. A 'Performance Scores' section that provides an Alignment Score and a Risk Exposure Score.\\n\\n\"\n",
    "                        f\"The Alignment Score measures how well the current quarter‚Äôs financial results align with past alignment reports,\"\n",
    "                        f\"forward-looking statements, and broader financial conditions. It does not indicate accuracy, but rather whether \"\n",
    "                        f\"the overall financial picture supports or contradicts prior expectations.\"\n",
    "                        f\"If the past alignment reports are available try to use the current alignment report to score the Alignment.\"\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        f\"### Bank: {bank} ({risk_type}) - {quarter} {year}\\n\\n\"\n",
    "                        f\"üîπ **Current quarter Insights:**\\n{current_insights}\\n\\n\"\n",
    "                        f\"üîπ **Previous quarter Insights:**\\n{previous_insights}\\n\\n\"\n",
    "                        f\"üîπ **Last but one quarter Insights:**\\n{one_before_previous_insights}\\n\\n\"\n",
    "                        f\"üîπ **Previous two quarters Alignment Reports:**\\n\"\n",
    "                        f\"   - Current quarter: {current_alignment}\\n\"\n",
    "                        f\"   - Last quarter: {previous_alignment}\\n\"\n",
    "                        f\"   - Quarter before last: {one_before_previous_alignment}\\n\\n\"\n",
    "                        f\"### **Professional Risk Assessment Report Format**:\\n\"\n",
    "                        f\"```\\n\"\n",
    "                        f\"{bank} {quarter} {year} Insights Summary\\n\\n\"\n",
    "                        f\"Key Focus Areas\\n\"\n",
    "                        f\"[2-3 paragraphs summarizing the bank's key {risk_type} risk challenges and mitigation strategies]\\n\\n\"\n",
    "                        f\"Evolution from Previous Quarters\\n\"\n",
    "                        f\"‚Ä¢ Q{quarter_number - 2 if quarter_number > 2 else 'N/A'}: [Brief summary]\\n\"\n",
    "                        f\"‚Ä¢ Q{quarter_number - 1 if quarter_number > 1 else 'N/A'}: [Brief summary]\\n\"\n",
    "                        f\"‚Ä¢ Q{quarter_number}: [Brief summary]\\n\"\n",
    "                        f\"‚Ä¢ (Explicitly state if risk exposure has increased, decreased, or remained stable.)\\n\\n\"\n",
    "                        f\"Key {risk_type.title()} Risk Challenges\\n\"\n",
    "                        f\"‚Ä¢ [Challenge 1]\\n\"\n",
    "                        f\"‚Ä¢ [Challenge 2]\\n\"\n",
    "                        f\"‚Ä¢ [Challenge 3]\\n\"\n",
    "                        f\"‚Ä¢ [Challenge 4]\\n\"\n",
    "                        f\"‚Ä¢ [Challenge 5]\\n\\n\"\n",
    "                        f\"Performance Scores\\n\"\n",
    "                        f\"‚Ä¢ Alignment Score: [Very Low / Low / Medium / High / Very High] - (Measures how closely the current quarter‚Äôs \"\n",
    "                        f\"financial picture aligns with past alignment reports and forward-looking statements.)\\n\"\n",
    "                        f\"‚Ä¢ Risk Exposure Score: [Very Low / Low / Medium / High / Very High]\\n\"\n",
    "                        f\"```\\n\\n\"\n",
    "                        f\"### **Important Instructions:**\\n\"\n",
    "                        f\"- Compare risk trends across the last three quarters to highlight evolving patterns.\\n\"\n",
    "                        f\"- Clearly state whether risks have intensified, remained stable, or decreased.\\n\"\n",
    "                        f\"- Explicitly mention any key risk mitigation strategies the bank has adopted.\\n\"\n",
    "                        f\"- Don't need to provide explanation for the **Alignment Score**\\n\"\n",
    "                    ),\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            response = self._rate_limited_api_call(\n",
    "                self.client.chat.completions.create,\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=messages,\n",
    "                temperature=0.1,  # Lower temperature for more consistent formatting\n",
    "                max_tokens=500,\n",
    "            )\n",
    "\n",
    "            return response.choices[0].message.content.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error analyzing risk level: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def _generate_ai_insight_and_summary(self, report, risk_type, refresh):\n",
    "        \"\"\"Generate AI insight for a report and risk type\"\"\"\n",
    "        report_id = report.get(\"id\")\n",
    "        insight_key = f\"AI_INSIGHT_{risk_type.upper()}\"\n",
    "        metrics = self.db.get_metrics_by_report(report_id)\n",
    "        if metrics and insight_key in metrics:\n",
    "            if not refresh:\n",
    "                self.logger.warning(\n",
    "                    f\"AI insight for {risk_type} already exists for the report {report_id}\",\n",
    "                )\n",
    "                return None\n",
    "        self.logger.info(\n",
    "            f\"Updating AI insight for {risk_type} for the report {report_id}\",\n",
    "        )\n",
    "        bank_name = report.get(\"bank_name\")\n",
    "        quarter = report.get(\"quarter\")\n",
    "        year = report.get(\"year\")\n",
    "\n",
    "        # get questions based on risk type\n",
    "        question_templates = self.risk_questions_map.get(risk_type, [])\n",
    "        questions = [\n",
    "            template(bank_name, quarter, year) for template in question_templates\n",
    "        ]\n",
    "\n",
    "        # generate answers based on questions\n",
    "        answers = self._generate_answers_with_gen_ai(\n",
    "            bank_name, quarter, year, risk_type, questions\n",
    "        )\n",
    "\n",
    "        # Combine answers into a single context\n",
    "        combined_answers = \"\\n\\n\".join(answers)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a **senior financial risk analyst** at the **{bank_name}**, responsible for evaluating {risk_type} for **{quarter} {year}**.\n",
    "\n",
    "        ### **Bank:** {bank_name}\n",
    "        ### **Risk Type:** {risk_type}\n",
    "        ### **Reporting Period:** {quarter} {year}\n",
    "\n",
    "        Here are the key financial insights that form the foundation of the report:\n",
    "\n",
    "        ### **Key Financial Insights from Current Quarter:**\n",
    "        {combined_answers}\n",
    "\n",
    "        ### **Your Task:**\n",
    "        - Write a **single structured financial report** summarizing the key {risk_type} insights for {bank_name} in **‚â§200 words**.\n",
    "        - Synthesize information into a **logical narrative** instead of listing separate answers.\n",
    "        - **No bullet points**‚Äîwrite in paragraph format with an introduction, analysis, and conclusion.\n",
    "        - Compare with historical data where relevant and provide **forward-looking implications**.\n",
    "        - Ensure **financial clarity**, using quantitative figures where possible.\n",
    "        - If the reports contain **some** information but lack full detail, still generate a report based on the available data, even if limited.\n",
    "        - Only return **\"Not enough information\"** if there is **absolutely no relevant data** to work with.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self._rate_limited_api_call(\n",
    "                self.client.chat.completions.create,\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a financial risk analyst producing expert-level structured financial reports. Use only the provided context and avoid speculation, but always generate a report if any relevant information is available.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "            )\n",
    "\n",
    "            insight = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Store the insight in database\n",
    "            self.db.add_metric(\n",
    "                report_id=report_id,\n",
    "                metric_name=f\"AI_INSIGHT_{risk_type.upper()}\",\n",
    "                metric_value=0.0,\n",
    "                formatted_value=insight,\n",
    "            )\n",
    "\n",
    "            results = self.searcher.run_bank_news_search(\n",
    "                bank=bank_name,\n",
    "                quarter=quarter,\n",
    "                year=year,\n",
    "                sentences=[insight],\n",
    "                risk_type=risk_type,\n",
    "            )\n",
    "\n",
    "            # Validate article relevance\n",
    "            alignment_report = self._validate_article_relevance(results, insight)\n",
    "\n",
    "            insights_dict, alignment_reports = self._get_historical_context(\n",
    "                bank_name,\n",
    "                risk_type,\n",
    "                quarter,\n",
    "                year,\n",
    "            )\n",
    "            insights_dict[\"current\"] = insight\n",
    "\n",
    "            # Analyze risk level\n",
    "            risk_analysis = self._analyze_risk(\n",
    "                report,\n",
    "                insights_dict,\n",
    "                alignment_reports,\n",
    "                risk_type,\n",
    "            )\n",
    "\n",
    "            # Store the summary and alignment reports in the database\n",
    "            self.db.add_metric(\n",
    "                report_id=report_id,\n",
    "                metric_name=f\"AI_SUMMARY_{risk_type.upper()}\",\n",
    "                metric_value=0.0,\n",
    "                formatted_value=risk_analysis,\n",
    "            )\n",
    "\n",
    "            self.db.add_metric(\n",
    "                report_id=report_id,\n",
    "                metric_name=f\"AI_ALIGNMENT_{risk_type.upper()}\",\n",
    "                metric_value=0.0,\n",
    "                formatted_value=alignment_report,\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f\"Generated and stored AI insight and summary for {bank_name}, {quarter} {year}, {risk_type}\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error generating risk insight and summary: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    async def run_financial_expert(self, report_id=None, refresh=False, limit=None):\n",
    "        \"\"\"\n",
    "        Generate AI insights for reports and process bank report summaries.\n",
    "        Args:\n",
    "            report_id: Optional specific report ID to process\n",
    "            refresh: Whether to refresh existing insights\n",
    "            limit: Optional limit on reports to process\n",
    "        Returns:\n",
    "            dict: Results summary\n",
    "        \"\"\"\n",
    "        # Single report case\n",
    "        if report_id:\n",
    "            try:\n",
    "                conn = self.db.begin_transaction()  # Use transaction management\n",
    "                try:\n",
    "                    report = self.db.get_report_by_id(report_id)\n",
    "                    if not report:\n",
    "                        self.logger.error(f\"Report with ID {report_id} not found\")\n",
    "                        return {\n",
    "                            \"error\": f\"Report with ID {report_id} not found\",\n",
    "                            \"success\": False,\n",
    "                        }\n",
    "\n",
    "                    for risk_type in RISK_TYPES_AI_INSIGHTS:\n",
    "                        await self._generate_ai_insight_and_summary(\n",
    "                            report,\n",
    "                            risk_type,\n",
    "                            refresh,\n",
    "                        )\n",
    "                        conn.commit()\n",
    "\n",
    "                    self.db.commit_transaction(conn)\n",
    "                    return {\"success\": True}\n",
    "                except Exception as e:\n",
    "                    self.db.rollback_transaction(conn)\n",
    "                    self.logger.error(\n",
    "                        f\"Error processing report {report_id}: {e}\",\n",
    "                    )\n",
    "                    return {\"error\": str(e), \"success\": False}\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error with database connection: {e}\")\n",
    "                return {\"error\": str(e), \"success\": False}\n",
    "\n",
    "        # Multiple reports case\n",
    "        summary_count = 0\n",
    "        reports = None\n",
    "        try:\n",
    "            with self.db.get_db_connection() as conn:\n",
    "                cursor = conn.cursor()\n",
    "                cursor.execute(\"\"\"\n",
    "                SELECT r.id, r.year, r.quarter, b.bank_name\n",
    "                FROM reports r, banks b\n",
    "                WHERE r.bank_id = b.id ORDER BY year ASC, quarter ASC\n",
    "                \"\"\")\n",
    "                reports = self.db.cursor_to_dict(cursor, cursor.fetchall())\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching reports: {e}\")\n",
    "            return {\"error\": str(e), \"success\": False}\n",
    "\n",
    "        if not reports:\n",
    "            self.logger.warning(\"No reports found in the database\")\n",
    "            return {\"error\": \"No reports found in the database\"}\n",
    "\n",
    "        reports_to_process = reports[:limit] if limit else reports\n",
    "\n",
    "        # Process each report with its own db connection\n",
    "        for report in tqdm(reports_to_process, desc=\"Processing reports\"):\n",
    "            try:\n",
    "                conn = self.db.begin_transaction()\n",
    "                try:\n",
    "                    for risk_type in RISK_TYPES_AI_INSIGHTS:\n",
    "                        await self._generate_ai_insight_and_summary(\n",
    "                            report,\n",
    "                            risk_type,\n",
    "                            refresh,\n",
    "                        )\n",
    "                        conn.commit()  # Explicit commit after each risk type\n",
    "\n",
    "                        # Optional: Checkpoint WAL after each commit to manage file size\n",
    "                        self.db.checkpoint_wal(\"PASSIVE\")\n",
    "\n",
    "                    self.db.commit_transaction(conn)\n",
    "                    summary_count += 1\n",
    "                except Exception as e:\n",
    "                    self.db.rollback_transaction(conn)\n",
    "                    self.logger.error(\n",
    "                        f\"Error processing report {report.get('id')}: {e}\",\n",
    "                    )\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error with transaction: {e}\")\n",
    "                continue\n",
    "\n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"generated_summaries\": summary_count,\n",
    "            \"total_reports\": len(reports),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t19XqVC9WcYo"
   },
   "source": [
    "## üîÑ Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TiquRclwWeMU"
   },
   "outputs": [],
   "source": [
    "class GSIBAnalysisPipeline:\n",
    "    \"\"\"Pipeline for G-SIB quarterly announcement analysis\"\"\"\n",
    "\n",
    "    def __init__(self, db: GSIBDatabase, llm):\n",
    "        \"\"\"\n",
    "        Initialize the G-SIB analysis pipeline with required components\n",
    "\n",
    "        Args:\n",
    "            db_path: Path to SQLite database\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "        self.logger.info(\"Initializing GSIBAnalysisPipeline\")\n",
    "\n",
    "        self.db = db\n",
    "        self.llm = llm\n",
    "\n",
    "        # Core components\n",
    "        self.preprocessor = TextPreprocessor(db=db)\n",
    "        self.topic_embedder = TextEmbedder(model_name=\"ProsusAI/finbert\")\n",
    "        self.sentiment_analyzer = SentimentAnalyser(\n",
    "            model_name=\"yiyanghkust/finbert-tone\",\n",
    "            db=db,\n",
    "            preprocessor=self.preprocessor,\n",
    "        )\n",
    "        self.topic_analyzer = TopicAnalyzer(\n",
    "            embedder=self.topic_embedder,\n",
    "            db=db,\n",
    "            preprocessor=self.preprocessor,\n",
    "            sentiment_analyzer=self.sentiment_analyzer,\n",
    "        )\n",
    "        self.creditrisk_analyser = CreditRiskAnalyser(db=db)\n",
    "        self.faiss_manager = FAISSManager(db_manager=db)\n",
    "        self.financial_expert = FinancialExpert(\n",
    "            llm=self.llm, db=self.db, faiss_manager=self.faiss_manager\n",
    "        )\n",
    "        self.run_vector_indexing()\n",
    "\n",
    "    def run_preprocessing(self, limit=None):\n",
    "        \"\"\"\n",
    "        Run text preprocessing\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of documents to process\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting text preprocessing pipeline{' (limited to ' + str(limit) + ' docs)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Process all QA pairs to generate preprocessed versions\n",
    "        self.preprocessor.process_qa_pairs_from_db(limit)\n",
    "        self.db.checkpoint_wal()\n",
    "        self.logger.info(\"Completed text preprocessing pipeline\")\n",
    "\n",
    "    def run_topic_analysis(self, limit=None):\n",
    "        \"\"\"\n",
    "        Run topic analysis\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations to analyze\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting topic analysis{' (limited to ' + str(limit) + ' conversations)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Run topic clustering\n",
    "        results = self.topic_analyzer.analyze_topics(limit=limit)\n",
    "        self.db.checkpoint_wal()\n",
    "\n",
    "        # Log summary\n",
    "        if results:\n",
    "            topics_found = len(set(r[\"topic\"].id for r in results.values()))\n",
    "            self.logger.info(\n",
    "                f\"Topic analysis complete: Found {topics_found} topics across {len(results)} conversations\",\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(\"No new topics found\")\n",
    "\n",
    "    def run_sentiment_analysis(self, limit=None):\n",
    "        \"\"\"\n",
    "        Run sentiment analysis\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of conversations to analyze\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting sentiment analysis{' (limited to ' + str(limit) + ' conversations)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Run sentiment analysis\n",
    "        results = self.sentiment_analyzer.analyze_qa_pairs(limit=limit)\n",
    "        self.db.checkpoint_wal()\n",
    "\n",
    "        # Log summary\n",
    "        if results:\n",
    "            self.logger.info(\n",
    "                f\"Sentiment analysis complete for {len(results)} conversations\",\n",
    "            )\n",
    "\n",
    "            # Calculate average sentiment\n",
    "            avg_positive = np.mean(\n",
    "                [r[\"conversation\"].positive for r in results.values()],\n",
    "            )\n",
    "            avg_negative = np.mean(\n",
    "                [r[\"conversation\"].negative for r in results.values()],\n",
    "            )\n",
    "            avg_compound = np.mean(\n",
    "                [r[\"conversation\"].compound for r in results.values()],\n",
    "            )\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"Average sentiment: Positive={avg_positive:.2f}, Negative={avg_negative:.2f}, Compound={avg_compound:.2f}\",\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(\"No new sentiment analysis results\")\n",
    "\n",
    "    def run_vector_indexing(self, limit=None):\n",
    "        \"\"\"\n",
    "        Index conversations in FAISS\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of documents to index\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting vector indexing{' (limited to ' + str(limit) + ' docs)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Index QA pairs in FAISS\n",
    "        count = self.faiss_manager.index_combined_qa(limit=limit)\n",
    "        self.db.checkpoint_wal()\n",
    "\n",
    "        self.logger.info(f\"Indexed {count} QA pairs in FAISS\")\n",
    "\n",
    "    def run_full_pipeline(self, limit=None):\n",
    "        \"\"\"\n",
    "        Run the full analysis pipeline\n",
    "\n",
    "        Args:\n",
    "            limit: Optional limit on number of documents to process at each stage\n",
    "        \"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Starting full G-SIB analysis pipeline{' (limited to ' + str(limit) + ' docs per stage)' if limit else ''}\",\n",
    "        )\n",
    "\n",
    "        # Run each stage\n",
    "        self.run_preprocessing(limit=limit)\n",
    "        self.run_topic_analysis(limit=limit)\n",
    "        self.creditrisk_analyser.run_credit_risk_analysis(limit=limit)\n",
    "        self.run_sentiment_analysis(limit=limit)\n",
    "        self.run_vector_indexing(limit=limit)\n",
    "        self.financial_expert.run_financial_expert(limit=limit)\n",
    "\n",
    "        self.logger.info(\"G-SIB analysis pipeline complete\")\n",
    "\n",
    "    def search_conversations(self, query, k=5, filter_conditions=None):\n",
    "        \"\"\"\n",
    "        Search for conversations related to a query\n",
    "\n",
    "        Args:\n",
    "            query: Search query\n",
    "            k: Number of results to return\n",
    "            filter_conditions: Optional filter conditions\n",
    "\n",
    "        Returns:\n",
    "            List of search results\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Searching for: '{query}'\")\n",
    "\n",
    "        # Run hybrid search\n",
    "        results = self.faiss_manager.hybrid_search(\n",
    "            query,\n",
    "            k=k,\n",
    "            filter_conditions=filter_conditions,\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_bank_analysis(self, bank_name, year, quarter):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive analysis for a specific bank and quarter\n",
    "\n",
    "        Args:\n",
    "            bank_name: Name of the bank\n",
    "            year: Year\n",
    "            quarter: Quarter (e.g., \"Q1\", \"Q2\")\n",
    "\n",
    "        Returns:\n",
    "            ReportAnalysis object with all analysis data\n",
    "        \"\"\"\n",
    "        conn = self.db.get_db_connection()\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        try:\n",
    "            # Get report ID\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT r.id\n",
    "                FROM reports r\n",
    "                JOIN banks b ON r.bank_id = b.id\n",
    "                WHERE b.bank_name = ? AND r.year = ? AND r.quarter = ?\n",
    "                \"\"\",\n",
    "                (bank_name, year, quarter),\n",
    "            )\n",
    "            report = cursor.fetchone()\n",
    "\n",
    "            if not report:\n",
    "                self.logger.warning(f\"No report found for {bank_name} {year} {quarter}\")\n",
    "                return None\n",
    "\n",
    "            report_id = report[\"id\"]\n",
    "\n",
    "            # Get all conversations for this report\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT ac.id, ac.analyst_name, ac.analyst_company, ac.topic_id, ac.topic_probability\n",
    "                FROM analyst_conversations ac\n",
    "                WHERE ac.report_id = ?\n",
    "                \"\"\",\n",
    "                (report_id,),\n",
    "            )\n",
    "            conversation_rows = cursor.fetchall()\n",
    "\n",
    "            if not conversation_rows:\n",
    "                self.logger.warning(\n",
    "                    f\"No conversations found for {bank_name} {year} {quarter}\",\n",
    "                )\n",
    "                return None\n",
    "\n",
    "            # Create ReportAnalysis\n",
    "            report_analysis = ReportAnalysis(\n",
    "                bank_name=bank_name,\n",
    "                year=year,\n",
    "                quarter=quarter,\n",
    "                conversations=[],\n",
    "                topics={},\n",
    "                credit_risk_score={},\n",
    "                credit_metrics={},\n",
    "            )\n",
    "\n",
    "            # Calculate credit risk score\n",
    "            risk_score = self.creditrisk_analyser.calculate_credit_risk_score(\n",
    "                bank_name,\n",
    "                year,\n",
    "                quarter,\n",
    "            )\n",
    "            report_analysis.credit_risk_score = risk_score\n",
    "\n",
    "            # Process each conversation\n",
    "            for conversation_row in conversation_rows:\n",
    "                conversation_id = conversation_row[\"id\"]\n",
    "\n",
    "                # Get topic\n",
    "                topic = None\n",
    "                topic_id = conversation_row[\"topic_id\"]\n",
    "                if topic_id:\n",
    "                    cursor.execute(\n",
    "                        \"\"\"\n",
    "                        SELECT id, name, keywords, category\n",
    "                        FROM topics\n",
    "                        WHERE id = ?\n",
    "                        \"\"\",\n",
    "                        (topic_id,),\n",
    "                    )\n",
    "                    topic_row = cursor.fetchone()\n",
    "                    if topic_row:\n",
    "                        topic = Topic(\n",
    "                            id=topic_row[\"id\"],\n",
    "                            name=topic_row[\"name\"],\n",
    "                            keywords=json.loads(topic_row[\"keywords\"]),\n",
    "                            category=topic_row[\"category\"],\n",
    "                            probability=conversation_row[\"topic_probability\"] or 0.0,\n",
    "                        )\n",
    "                        report_analysis.topics[topic_id] = topic\n",
    "\n",
    "                # Get conversation sentiment\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT positive_score, negative_score, neutral_score, compound_score\n",
    "                    FROM conversation_sentiment\n",
    "                    WHERE conversation_id = ?\n",
    "                    \"\"\",\n",
    "                    (conversation_id,),\n",
    "                )\n",
    "                sentiment_row = cursor.fetchone()\n",
    "                sentiment = None\n",
    "                if sentiment_row:\n",
    "                    sentiment = SentimentScore(\n",
    "                        positive=sentiment_row[\"positive_score\"],\n",
    "                        negative=sentiment_row[\"negative_score\"],\n",
    "                        neutral=sentiment_row[\"neutral_score\"],\n",
    "                        compound=sentiment_row[\"compound_score\"],\n",
    "                    )\n",
    "\n",
    "                # Get speaker sentiments\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT speaker_name, speaker_role, positive_score, negative_score, neutral_score, compound_score\n",
    "                    FROM speaker_topic_sentiment\n",
    "                    WHERE conversation_id = ?\n",
    "                    \"\"\",\n",
    "                    (conversation_id,),\n",
    "                )\n",
    "                speaker_rows = cursor.fetchall()\n",
    "                speaker_sentiments = {}\n",
    "                for speaker_row in speaker_rows:\n",
    "                    speaker_key = (\n",
    "                        f\"{speaker_row['speaker_name']}|{speaker_row['speaker_role']}\"\n",
    "                    )\n",
    "                    speaker_sentiments[speaker_key] = SentimentScore(\n",
    "                        positive=speaker_row[\"positive_score\"],\n",
    "                        negative=speaker_row[\"negative_score\"],\n",
    "                        neutral=speaker_row[\"neutral_score\"],\n",
    "                        compound=speaker_row[\"compound_score\"],\n",
    "                    )\n",
    "\n",
    "                # Get credit risk mentions\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT crm.keyword_id, crm.mention_count, crm.mention_context,\n",
    "                           crm.positive_score, crm.negative_score, crm.neutral_score, crm.compound_score,\n",
    "                           crk.metric_category\n",
    "                    FROM credit_risk_mentions crm\n",
    "                    JOIN credit_risk_keywords crk ON crm.keyword_id = crk.keyword\n",
    "                    WHERE crm.conversation_id = ?\n",
    "                    \"\"\",\n",
    "                    (conversation_id,),\n",
    "                )\n",
    "                mention_rows = cursor.fetchall()\n",
    "                credit_risk_mentions = []\n",
    "                for mention_row in mention_rows:\n",
    "                    mention_sentiment = SentimentScore(\n",
    "                        positive=mention_row[\"positive_score\"],\n",
    "                        negative=mention_row[\"negative_score\"],\n",
    "                        neutral=mention_row[\"neutral_score\"],\n",
    "                        compound=mention_row[\"compound_score\"],\n",
    "                    )\n",
    "\n",
    "                    mention_text = \"\"\n",
    "                    if mention_row[\"mention_context\"]:\n",
    "                        contexts = json.loads(mention_row[\"mention_context\"])\n",
    "                        if contexts:\n",
    "                            mention_text = contexts[0]\n",
    "\n",
    "                    mention = CreditRiskMention(\n",
    "                        metric_type=mention_row[\"metric_category\"],\n",
    "                        mention_text=mention_text,\n",
    "                        mention_count=mention_row[\"mention_count\"],\n",
    "                        sentiment=mention_sentiment,\n",
    "                        keyword_used=mention_row[\"keyword_id\"],\n",
    "                    )\n",
    "                    credit_risk_mentions.append(mention)\n",
    "\n",
    "                # Get QA pairs\n",
    "                cursor.execute(\n",
    "                    \"\"\"\n",
    "                    SELECT id, question, answer, answer_speaker, answer_role\n",
    "                    FROM qa_pairs\n",
    "                    WHERE conversation_id = ?\n",
    "                    \"\"\",\n",
    "                    (conversation_id,),\n",
    "                )\n",
    "                qa_rows = cursor.fetchall()\n",
    "                qa_pairs = []\n",
    "                for qa_row in qa_rows:\n",
    "                    qa_pair = QAPair(\n",
    "                        question=qa_row[\"question\"],\n",
    "                        answer=qa_row[\"answer\"],\n",
    "                        answer_speaker=qa_row[\"answer_speaker\"],\n",
    "                        answer_role=qa_row[\"answer_role\"],\n",
    "                    )\n",
    "                    qa_pairs.append(qa_pair)\n",
    "\n",
    "                # Create conversation object\n",
    "                conversation = AnalystConversation(\n",
    "                    id=conversation_id,\n",
    "                    analyst_name=conversation_row[\"analyst_name\"],\n",
    "                    analyst_company=conversation_row[\"analyst_company\"],\n",
    "                    qa_pairs=qa_pairs,\n",
    "                    topic=topic,\n",
    "                    conversation_sentiment=sentiment,\n",
    "                    speaker_sentiments=speaker_sentiments,\n",
    "                    credit_risk_mentions=credit_risk_mentions,\n",
    "                )\n",
    "\n",
    "                report_analysis.conversations.append(conversation)\n",
    "\n",
    "            # Get credit metrics if available\n",
    "            cursor.execute(\n",
    "                \"\"\"\n",
    "                SELECT metric_name, metric_value\n",
    "                FROM metrics\n",
    "                WHERE report_id = ?\n",
    "                \"\"\",\n",
    "                (report_id,),\n",
    "            )\n",
    "            metric_rows = cursor.fetchall()\n",
    "            for metric_row in metric_rows:\n",
    "                report_analysis.credit_metrics[metric_row[\"metric_name\"]] = metric_row[\n",
    "                    \"metric_value\"\n",
    "                ]\n",
    "\n",
    "            return report_analysis\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting bank analysis: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "    def compare_banks(self, banks, year, quarter):\n",
    "        \"\"\"\n",
    "        Compare multiple banks for the same quarter\n",
    "\n",
    "        Args:\n",
    "            banks: List of bank names\n",
    "            year: Year\n",
    "            quarter: Quarter\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with comparative analysis\n",
    "        \"\"\"\n",
    "        # Get analysis for each bank\n",
    "        bank_analyses = {}\n",
    "        for bank in banks:\n",
    "            analysis = self.get_bank_analysis(bank, year, quarter)\n",
    "            if analysis:\n",
    "                bank_analyses[bank] = analysis\n",
    "\n",
    "        if not bank_analyses:\n",
    "            return None\n",
    "\n",
    "        # Create comparison results\n",
    "        comparison = {\n",
    "            \"period\": f\"{year} {quarter}\",\n",
    "            \"banks\": list(bank_analyses.keys()),\n",
    "            \"sentiment\": {},\n",
    "            \"topics\": {},\n",
    "            \"metrics\": {},\n",
    "        }\n",
    "\n",
    "        # Compare sentiment\n",
    "        for bank, analysis in bank_analyses.items():\n",
    "            overall_sentiment = analysis.overall_sentiment\n",
    "            comparison[\"sentiment\"][bank] = {\n",
    "                \"positive\": overall_sentiment.positive,\n",
    "                \"negative\": overall_sentiment.negative,\n",
    "                \"compound\": overall_sentiment.compound,\n",
    "            }\n",
    "\n",
    "        # Compare topics\n",
    "        all_topics = {}\n",
    "        for bank, analysis in bank_analyses.items():\n",
    "            topic_counts = analysis.topic_mention_counts()\n",
    "            comparison[\"topics\"][bank] = topic_counts\n",
    "\n",
    "            # Collect all topics\n",
    "            for topic in topic_counts:\n",
    "                if topic not in all_topics:\n",
    "                    all_topics[topic] = {}\n",
    "                all_topics[topic][bank] = topic_counts[topic]\n",
    "\n",
    "        # Compare metrics\n",
    "        all_metrics = {}\n",
    "        for bank, analysis in bank_analyses.items():\n",
    "            for metric, value in analysis.credit_metrics.items():\n",
    "                if metric not in all_metrics:\n",
    "                    all_metrics[metric] = {}\n",
    "                all_metrics[metric][bank] = value\n",
    "\n",
    "        comparison[\"all_topics\"] = all_topics\n",
    "        comparison[\"all_metrics\"] = all_metrics\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    def get_results_dataframe(self):\n",
    "        \"\"\"\n",
    "        Get a DataFrame with analysis results\n",
    "\n",
    "        Returns:\n",
    "            Pandas DataFrame with consolidated analysis results\n",
    "        \"\"\"\n",
    "        conn = self.db.get_db_connection()\n",
    "\n",
    "        # Query for combined results\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "            b.bank_name,\n",
    "            r.year,\n",
    "            r.quarter,\n",
    "            ac.analyst_name,\n",
    "            ac.analyst_company,\n",
    "            t.name as topic_name,\n",
    "            t.category as topic_category,\n",
    "            ac.topic_probability,\n",
    "            cs.positive_score,\n",
    "            cs.negative_score,\n",
    "            cs.neutral_score,\n",
    "            cs.compound_score\n",
    "        FROM analyst_conversations ac\n",
    "        JOIN reports r ON ac.report_id = r.id\n",
    "        JOIN banks b ON r.bank_id = b.id\n",
    "        LEFT JOIN topics t ON ac.topic_id = t.id\n",
    "        LEFT JOIN conversation_sentiment cs ON ac.id = cs.conversation_id\n",
    "        ORDER BY b.bank_name, r.year, r.quarter\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmKTFSEbBQiJ"
   },
   "source": [
    "### ‚ñ∂Ô∏è Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 17:07:22,742 - __main__ - INFO - Initializing GSIBAnalysisPipeline\n",
      "2025-03-28 17:07:22,743 - __main__ - INFO - Initializing __main__\n",
      "2025-03-28 17:07:22,743 - __main__ - INFO - NLTK data downloaded successfully\n",
      "2025-03-28 17:07:26,219 - __main__ - INFO - Loaded spaCy model: en_core_web_sm\n",
      "2025-03-28 17:07:26,339 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /ProsusAI/finbert/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:26,470 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /ProsusAI/finbert/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:26,574 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /ProsusAI/finbert/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:26,583 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-03-28 17:07:26,739 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/ProsusAI/finbert HTTP/1.1\" 200 4528\n",
      "2025-03-28 17:07:26,806 - __main__ - INFO - Initializing __main__ with model yiyanghkust/finbert-tone\n",
      "2025-03-28 17:07:26,863 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/ProsusAI/finbert/commits/main HTTP/1.1\" 200 4974\n",
      "2025-03-28 17:07:26,910 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:27,040 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:27,144 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:27,249 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:27,376 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/ProsusAI/finbert/discussions?p=0 HTTP/1.1\" 200 13969\n",
      "2025-03-28 17:07:27,507 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:27,638 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/ProsusAI/finbert/commits/refs%2Fpr%2F10 HTTP/1.1\" 200 5939\n",
      "2025-03-28 17:07:27,731 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:27,754 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /ProsusAI/finbert/resolve/refs%2Fpr%2F10/model.safetensors.index.json HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:27,755 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n",
      "2025-03-28 17:07:27,867 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /ProsusAI/finbert/resolve/refs%2Fpr%2F10/model.safetensors HTTP/1.1\" 302 0\n",
      "2025-03-28 17:07:27,896 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/yiyanghkust/finbert-tone HTTP/1.1\" 200 2824\n",
      "2025-03-28 17:07:27,940 - __main__ - INFO - Initializing __main__\n",
      "2025-03-28 17:07:28,001 - __main__ - INFO - Generated embeddings for 4 risk categories\n",
      "2025-03-28 17:07:28,001 - __main__ - INFO - Initializing __main__\n",
      "2025-03-28 17:07:28,003 - __main__ - INFO - Populating credit risk keywords\n",
      "2025-03-28 17:07:28,012 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/yiyanghkust/finbert-tone/commits/main HTTP/1.1\" 200 4606\n",
      "2025-03-28 17:07:28,042 - __main__ - INFO - Credit risk keywords populated\n",
      "2025-03-28 17:07:28,044 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: FinLang/finance-embeddings-investopedia\n",
      "2025-03-28 17:07:28,130 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/yiyanghkust/finbert-tone/discussions?p=0 HTTP/1.1\" 200 10211\n",
      "2025-03-28 17:07:28,160 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:28,260 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/yiyanghkust/finbert-tone/commits/refs%2Fpr%2F17 HTTP/1.1\" 200 5571\n",
      "2025-03-28 17:07:28,363 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/refs%2Fpr%2F17/model.safetensors.index.json HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:28,449 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:28,464 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /yiyanghkust/finbert-tone/resolve/refs%2Fpr%2F17/model.safetensors HTTP/1.1\" 302 0\n",
      "2025-03-28 17:07:28,553 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:28,655 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:28,758 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:28,865 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "2025-03-28 17:07:28,971 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:29,150 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /FinLang/finance-embeddings-investopedia/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:29,273 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/FinLang/finance-embeddings-investopedia/revision/main HTTP/1.1\" 200 1749\n",
      "2025-03-28 17:07:29,377 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/FinLang/finance-embeddings-investopedia HTTP/1.1\" 200 1749\n",
      "2025-03-28 17:07:29,382 - __main__ - INFO - Loaded embedding model: FinLang/finance-embeddings-investopedia (768 dimensions)\n",
      "2025-03-28 17:07:29,480 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "2025-03-28 17:07:29,576 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:29,717 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /cross-encoder/ms-marco-MiniLM-L-6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "2025-03-28 17:07:29,818 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /cross-encoder/ms-marco-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2025-03-28 17:07:29,854 - __main__ - INFO - Loaded reranker model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "2025-03-28 17:07:33,425 - __main__ - INFO - Loaded spaCy model: en_core_web_sm\n",
      "2025-03-28 17:07:33,426 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-03-28 17:07:33,427 - httpx - DEBUG - load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/certifi/cacert.pem'\n",
      "2025-03-28 17:07:33,433 - __main__ - INFO - Initializing FinancialExpert\n",
      "2025-03-28 17:07:33,433 - __main__ - INFO - Starting vector indexing\n",
      "2025-03-28 17:07:33,434 - __main__ - INFO - Loading existing FAISS index: main\n",
      "2025-03-28 17:07:33,444 - __main__ - INFO - Loaded 398 vectors into FAISS index\n",
      "2025-03-28 17:11:26,274 - __main__ - INFO - No new QA pairs to index\n",
      "2025-03-28 17:11:26,277 - GSIBDatabase - INFO - WAL checkpoint completed with mode: PASSIVE\n",
      "2025-03-28 17:11:26,277 - GSIBDatabase - INFO - WAL checkpoint completed with mode: PASSIVE\n",
      "2025-03-28 17:11:26,279 - __main__ - INFO - Indexed 0 QA pairs in FAISS\n"
     ]
    }
   ],
   "source": [
    "gsib_analysis_pipeline = GSIBAnalysisPipeline(db=db, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "35dXHm5JcbDn",
    "outputId": "eca82af5-8551-41af-b0f3-3b206eac3c5d"
   },
   "outputs": [],
   "source": [
    "gsib_analysis_pipeline.run_preprocessing()\n",
    "gsib_analysis_pipeline.run_topic_analysis()\n",
    "gsib_analysis_pipeline.creditrisk_analyser.run_credit_risk_analysis()\n",
    "gsib_analysis_pipeline.run_sentiment_analysis()\n",
    "result = await gsib_analysis_pipeline.financial_expert.run_financial_expert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rokPiEUezUS"
   },
   "source": [
    "## üìä Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB7JiqECe2ds"
   },
   "source": [
    "### Credit-risk plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "POltGDjHe3Zv",
    "outputId": "37d3e945-2629-4aa6-9fa8-8ff8732ac787"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "delta": {
          "decreasing": {
           "color": "green"
          },
          "increasing": {
           "color": "red"
          },
          "reference": 41.50001637597245
         },
         "gauge": {
          "axis": {
           "range": [
            0,
            100
           ],
           "tickcolor": "darkblue",
           "tickwidth": 1
          },
          "bar": {
           "color": "purple"
          },
          "bgcolor": "white",
          "bordercolor": "gray",
          "borderwidth": 2,
          "steps": [
           {
            "color": "#2ecc71",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             0,
             15
            ],
            "thickness": 0.75
           },
           {
            "color": "#2ecc71",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             15,
             30
            ],
            "thickness": 0.75
           },
           {
            "color": "#27ae60",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             30,
             45
            ],
            "thickness": 0.75
           },
           {
            "color": "#f1c40f",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             45,
             60
            ],
            "thickness": 0.75
           },
           {
            "color": "#e67e22",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             60,
             75
            ],
            "thickness": 0.75
           },
           {
            "color": "#d35400",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             75,
             85
            ],
            "thickness": 0.75
           },
           {
            "color": "#c0392b",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             85,
             100
            ],
            "thickness": 0.75
           }
          ],
          "threshold": {
           "line": {
            "color": "black",
            "width": 4
           },
           "thickness": 0.75,
           "value": 41.53423146674356
          }
         },
         "mode": "gauge+number+delta",
         "title": {
          "text": "Risk Score: Moderately Low"
         },
         "type": "indicator",
         "value": 41.53423146674356
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 50,
         "t": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "JPMorgan Chase Current Risk"
        },
        "width": 500
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#1f77b4",
          "width": 3
         },
         "marker": {
          "color": "#1f77b4",
          "size": 10
         },
         "mode": "lines+markers",
         "name": "JPMorgan Chase",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2023 Q2",
          "2023 Q3",
          "2023 Q4",
          "2024 Q1",
          "2024 Q2",
          "2024 Q3",
          "2024 Q4"
         ],
         "y": [
          41.50033853067252,
          42.027307280780036,
          41.50070821747852,
          41.50003672726848,
          41.50173308228568,
          41.92879861656468,
          41.50001637597245,
          41.53423146674356
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#2ecc71",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Very Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          15,
          15
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#27ae60",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          30,
          30
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#f1c40f",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderately Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          45,
          45
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#e67e22",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderate Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          60,
          60
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#d35400",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderately High Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          75,
          75
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#c0392b",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "High Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          85,
          85
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "right",
          "font": {
           "color": "#2ecc71",
           "size": 10
          },
          "showarrow": false,
          "text": "Very Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 15
         },
         {
          "align": "right",
          "font": {
           "color": "#27ae60",
           "size": 10
          },
          "showarrow": false,
          "text": "Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 30
         },
         {
          "align": "right",
          "font": {
           "color": "#f1c40f",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderately Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 45
         },
         {
          "align": "right",
          "font": {
           "color": "#e67e22",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderate",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 60
         },
         {
          "align": "right",
          "font": {
           "color": "#d35400",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderately High",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 75
         },
         {
          "align": "right",
          "font": {
           "color": "#c0392b",
           "size": 10
          },
          "showarrow": false,
          "text": "High",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 85
         }
        ],
        "height": 400,
        "margin": {
         "b": 50,
         "t": 100
        },
        "shapes": [
         {
          "fillcolor": "#2ecc71",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 15,
          "y1": 30,
          "yref": "y"
         },
         {
          "fillcolor": "#27ae60",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 30,
          "y1": 45,
          "yref": "y"
         },
         {
          "fillcolor": "#f1c40f",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 45,
          "y1": 60,
          "yref": "y"
         },
         {
          "fillcolor": "#e67e22",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 60,
          "y1": 75,
          "yref": "y"
         },
         {
          "fillcolor": "#d35400",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 75,
          "y1": 85,
          "yref": "y"
         },
         {
          "fillcolor": "#c0392b",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 85,
          "y1": 100,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Bank Risk Score Trend (2023-2024)"
        },
        "width": 600,
        "xaxis": {
         "gridcolor": "LightGray",
         "gridwidth": 1,
         "showgrid": true,
         "title": {
          "text": "Quarter"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "gridwidth": 1,
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "title": {
          "text": "Risk Score (0-100)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_plotly_dashboard(df, bank, current_quarter):\n",
    "    \"\"\"Create separate interactive Plotly gauge and line charts for one bank\"\"\"\n",
    "\n",
    "    # Sort the data by period\n",
    "    bank_data = df[df[\"bank\"] == bank].sort_values(\"period\")\n",
    "\n",
    "    # Get the latest available data point for the bank\n",
    "    if not bank_data.empty:\n",
    "        current_data = bank_data.iloc[-1]\n",
    "        current_score = current_data[\"risk_score\"]\n",
    "        current_level = current_data[\"risk_level\"]\n",
    "\n",
    "        # Get the previous quarter's score if available\n",
    "        if len(bank_data) >= 2:\n",
    "            previous_score = bank_data.iloc[-2][\"risk_score\"]\n",
    "        else:\n",
    "            # If no previous quarter data is available, use current score (no change)\n",
    "            previous_score = current_score\n",
    "    else:\n",
    "        current_score = 0\n",
    "        current_level = \"Unknown\"\n",
    "        previous_score = current_score\n",
    "\n",
    "    # Risk thresholds with labels and colors\n",
    "    risk_thresholds = [\n",
    "        (0, \"Very Low\", \"#2ecc71\"),  # Bright green\n",
    "        (15, \"Very Low\", \"#2ecc71\"),  # Bright green\n",
    "        (30, \"Low\", \"#27ae60\"),  # Dark green\n",
    "        (45, \"Moderately Low\", \"#f1c40f\"),  # Yellow\n",
    "        (60, \"Moderate\", \"#e67e22\"),  # Orange\n",
    "        (75, \"Moderately High\", \"#d35400\"),  # Dark orange\n",
    "        (85, \"High\", \"#c0392b\"),  # Red\n",
    "        (100, \"Very High\", \"#7b241c\"),  # Dark red\n",
    "    ]\n",
    "\n",
    "    # Find risk level colors for gauge steps\n",
    "    steps = []\n",
    "    for i in range(len(risk_thresholds) - 1):\n",
    "        steps.append(\n",
    "            dict(\n",
    "                range=[risk_thresholds[i][0], risk_thresholds[i + 1][0]],\n",
    "                color=risk_thresholds[i][2],\n",
    "                thickness=0.75,\n",
    "                line=dict(width=0.5, color=\"white\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Create gauge chart\n",
    "    gauge_fig = go.Figure()\n",
    "\n",
    "    gauge_fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=current_score,\n",
    "            title={\"text\": f\"Risk Score: {current_level}\"},\n",
    "            delta={\n",
    "                \"reference\": previous_score,\n",
    "                \"increasing\": {\"color\": \"red\"},\n",
    "                \"decreasing\": {\"color\": \"green\"},\n",
    "            },\n",
    "            gauge={\n",
    "                \"axis\": {\"range\": [0, 100], \"tickwidth\": 1, \"tickcolor\": \"darkblue\"},\n",
    "                \"bar\": {\"color\": \"purple\"},  # Purple\n",
    "                \"bgcolor\": \"white\",\n",
    "                \"borderwidth\": 2,\n",
    "                \"bordercolor\": \"gray\",\n",
    "                \"steps\": steps,\n",
    "                \"threshold\": {\n",
    "                    \"line\": {\"color\": \"black\", \"width\": 4},\n",
    "                    \"thickness\": 0.75,\n",
    "                    \"value\": current_score,\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    gauge_fig.update_layout(\n",
    "        height=400,\n",
    "        width=500,\n",
    "        title_text=f\"{bank} Current Risk\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=50),\n",
    "    )\n",
    "\n",
    "    # Create line chart for risk score trend\n",
    "    line_fig = go.Figure()\n",
    "\n",
    "    bank_data = df[df[\"bank\"] == bank].sort_values(\"period\")\n",
    "\n",
    "    if not bank_data.empty:\n",
    "        # Add the main trend line\n",
    "        line_fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=bank_data[\"period\"],\n",
    "                y=bank_data[\"risk_score\"],\n",
    "                name=bank,\n",
    "                mode=\"lines+markers\",\n",
    "                line=dict(width=3, color=\"#1f77b4\"),\n",
    "                marker=dict(size=10, color=\"#1f77b4\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Add horizontal lines for risk thresholds on the trend chart - hide from legend\n",
    "        for threshold, label, color in risk_thresholds[1:-1]:  # Skip first and last\n",
    "            line_fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[bank_data[\"period\"].min(), bank_data[\"period\"].max()],\n",
    "                    y=[threshold, threshold],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=color, width=1, dash=\"dash\"),\n",
    "                    showlegend=False,  # Hide from legend\n",
    "                    hoverinfo=\"text\",\n",
    "                    text=f\"{label} Threshold\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Add annotation for threshold line\n",
    "            line_fig.add_annotation(\n",
    "                x=bank_data[\"period\"].min(),\n",
    "                y=threshold,\n",
    "                text=label,\n",
    "                showarrow=False,\n",
    "                font=dict(size=10, color=color),\n",
    "                xshift=-40,\n",
    "                align=\"right\",\n",
    "            )\n",
    "\n",
    "        # Add colored background regions for risk levels on trend chart\n",
    "        shapes = []\n",
    "        for i in range(1, len(risk_thresholds) - 1):\n",
    "            shapes.append(\n",
    "                {\n",
    "                    \"type\": \"rect\",\n",
    "                    \"xref\": \"x\",\n",
    "                    \"yref\": \"y\",\n",
    "                    \"x0\": bank_data[\"period\"].min(),\n",
    "                    \"y0\": risk_thresholds[i][0],\n",
    "                    \"x1\": bank_data[\"period\"].max(),\n",
    "                    \"y1\": risk_thresholds[i + 1][0],\n",
    "                    \"fillcolor\": risk_thresholds[i][2],\n",
    "                    \"opacity\": 0.1,\n",
    "                    \"layer\": \"below\",\n",
    "                    \"line_width\": 0,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        line_fig.update_layout(shapes=shapes)\n",
    "\n",
    "    # Update line chart layout\n",
    "    line_fig.update_layout(\n",
    "        height=400,\n",
    "        width=600,\n",
    "        title_text=\"Bank Risk Score Trend (2023-2024)\",  # Changed from bank name to generic title\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=50),\n",
    "        xaxis=dict(\n",
    "            title=\"Quarter\",\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Risk Score (0-100)\",\n",
    "            range=[0, 100],\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return gauge_fig, line_fig\n",
    "\n",
    "\n",
    "def run_risk_analysis(gsib_pipeline, bank, current_quarter=(\"2024\", \"Q4\")):\n",
    "    \"\"\"Run risk analysis for a single bank with separate gauge and trend charts\"\"\"\n",
    "    # Define quarters\n",
    "    years = [2023, 2023, 2023, 2023, 2024, 2024, 2024, 2024]\n",
    "    quarters = [\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Calculate risk scores for all quarters\n",
    "    for year, quarter in zip(years, quarters):\n",
    "        try:\n",
    "            # Extract quarter number (Q1->1, Q2->2, etc.)\n",
    "            quarter_num = int(quarter[1])\n",
    "            current_quarter_num = int(current_quarter[1][1])\n",
    "\n",
    "            # Skip if this quarter is in the future\n",
    "            if year > int(current_quarter[0]) or (\n",
    "                year == int(current_quarter[0]) and quarter_num > current_quarter_num\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            score = gsib_pipeline.creditrisk_analyser.calculate_credit_risk_score(\n",
    "                bank,\n",
    "                year,\n",
    "                quarter,\n",
    "            )\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"bank\": bank,\n",
    "                    \"period\": f\"{year} {quarter}\",\n",
    "                    \"year\": year,\n",
    "                    \"quarter\": quarter,\n",
    "                    \"risk_score\": score[\"risk_score\"],\n",
    "                    \"risk_level\": score[\"risk_level\"],\n",
    "                    \"asset_size\": score.get(\"asset_size\", \"Unknown\"),\n",
    "                },\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating {bank} {year} {quarter}: {str(e)}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    # Create separate visualizations with Plotly - gauge and trend chart\n",
    "    gauge_fig, line_fig = create_plotly_dashboard(df, bank, current_quarter)\n",
    "\n",
    "    return df, gauge_fig, line_fig\n",
    "\n",
    "\n",
    "# Run the analysis and display both charts separately\n",
    "df, gauge_fig, line_fig = run_risk_analysis(\n",
    "    gsib_analysis_pipeline, \"JPMorgan Chase\", current_quarter=(\"2024\", \"Q4\")\n",
    ")\n",
    "gauge_fig.show()\n",
    "line_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "adkTwiJ5yF9B",
    "outputId": "e69a437f-a534-4170-9302-ef15fc5f4079"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "delta": {
          "decreasing": {
           "color": "green"
          },
          "increasing": {
           "color": "red"
          },
          "reference": 50.514733494442396
         },
         "gauge": {
          "axis": {
           "range": [
            0,
            100
           ],
           "tickcolor": "darkblue",
           "tickwidth": 1
          },
          "bar": {
           "color": "purple"
          },
          "bgcolor": "white",
          "bordercolor": "gray",
          "borderwidth": 2,
          "steps": [
           {
            "color": "#2ecc71",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             0,
             15
            ],
            "thickness": 0.75
           },
           {
            "color": "#2ecc71",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             15,
             30
            ],
            "thickness": 0.75
           },
           {
            "color": "#27ae60",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             30,
             45
            ],
            "thickness": 0.75
           },
           {
            "color": "#f1c40f",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             45,
             60
            ],
            "thickness": 0.75
           },
           {
            "color": "#e67e22",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             60,
             75
            ],
            "thickness": 0.75
           },
           {
            "color": "#d35400",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             75,
             85
            ],
            "thickness": 0.75
           },
           {
            "color": "#c0392b",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             85,
             100
            ],
            "thickness": 0.75
           }
          ],
          "threshold": {
           "line": {
            "color": "black",
            "width": 4
           },
           "thickness": 0.75,
           "value": 50.504482912611365
          }
         },
         "mode": "gauge+number+delta",
         "title": {
          "text": "Risk Score: Moderate"
         },
         "type": "indicator",
         "value": 50.504482912611365
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 50,
         "t": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UBS Current Risk"
        },
        "width": 500
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#1f77b4",
          "width": 3
         },
         "marker": {
          "color": "#1f77b4",
          "size": 10
         },
         "mode": "lines+markers",
         "name": "UBS",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2023 Q2",
          "2023 Q3",
          "2023 Q4",
          "2024 Q1",
          "2024 Q2",
          "2024 Q3",
          "2024 Q4"
         ],
         "y": [
          50.50038099446465,
          59.39129836295547,
          50.5,
          57.079941289488545,
          62.500812008405774,
          61.200158641528105,
          50.514733494442396,
          50.504482912611365
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#2ecc71",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Very Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          15,
          15
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#27ae60",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          30,
          30
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#f1c40f",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderately Low Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          45,
          45
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#e67e22",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderate Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          60,
          60
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#d35400",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "Moderately High Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          75,
          75
         ]
        },
        {
         "hoverinfo": "text",
         "line": {
          "color": "#c0392b",
          "dash": "dash",
          "width": 1
         },
         "mode": "lines",
         "showlegend": false,
         "text": "High Threshold",
         "type": "scatter",
         "x": [
          "2023 Q1",
          "2024 Q4"
         ],
         "y": [
          85,
          85
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "align": "right",
          "font": {
           "color": "#2ecc71",
           "size": 10
          },
          "showarrow": false,
          "text": "Very Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 15
         },
         {
          "align": "right",
          "font": {
           "color": "#27ae60",
           "size": 10
          },
          "showarrow": false,
          "text": "Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 30
         },
         {
          "align": "right",
          "font": {
           "color": "#f1c40f",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderately Low",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 45
         },
         {
          "align": "right",
          "font": {
           "color": "#e67e22",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderate",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 60
         },
         {
          "align": "right",
          "font": {
           "color": "#d35400",
           "size": 10
          },
          "showarrow": false,
          "text": "Moderately High",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 75
         },
         {
          "align": "right",
          "font": {
           "color": "#c0392b",
           "size": 10
          },
          "showarrow": false,
          "text": "High",
          "x": "2023 Q1",
          "xshift": -40,
          "y": 85
         }
        ],
        "height": 400,
        "margin": {
         "b": 50,
         "t": 100
        },
        "shapes": [
         {
          "fillcolor": "#2ecc71",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 15,
          "y1": 30,
          "yref": "y"
         },
         {
          "fillcolor": "#27ae60",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 30,
          "y1": 45,
          "yref": "y"
         },
         {
          "fillcolor": "#f1c40f",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 45,
          "y1": 60,
          "yref": "y"
         },
         {
          "fillcolor": "#e67e22",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 60,
          "y1": 75,
          "yref": "y"
         },
         {
          "fillcolor": "#d35400",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 75,
          "y1": 85,
          "yref": "y"
         },
         {
          "fillcolor": "#c0392b",
          "layer": "below",
          "line": {
           "width": 0
          },
          "opacity": 0.1,
          "type": "rect",
          "x0": "2023 Q1",
          "x1": "2024 Q4",
          "xref": "x",
          "y0": 85,
          "y1": 100,
          "yref": "y"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Bank Risk Score Trend (2023-2024)"
        },
        "width": 600,
        "xaxis": {
         "gridcolor": "LightGray",
         "gridwidth": 1,
         "showgrid": true,
         "title": {
          "text": "Quarter"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "gridwidth": 1,
         "range": [
          0,
          100
         ],
         "showgrid": true,
         "title": {
          "text": "Risk Score (0-100)"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df, gauge_fig, line_fig = run_risk_analysis(\n",
    "    gsib_analysis_pipeline, \"UBS\", current_quarter=(\"2024\", \"Q4\")\n",
    ")\n",
    "gauge_fig.show()\n",
    "line_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOlZuPzte5lk"
   },
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gG0CQoKte6Zy"
   },
   "outputs": [],
   "source": [
    "def get_sentiment_data(db_path, bank_name, year):\n",
    "    \"\"\"Extract sentiment data from the database for visualizations\"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the bank_id for the given bank name\n",
    "    cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "    bank_id = cursor.fetchone()\n",
    "\n",
    "    if not bank_id:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"Bank '{bank_name}' not found in the database\")\n",
    "\n",
    "    bank_id = bank_id[0]\n",
    "\n",
    "    # Get report IDs for all quarters of the specified year\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT id, quarter\n",
    "        FROM reports\n",
    "        WHERE bank_id = ? AND year = ?\n",
    "        ORDER BY\n",
    "            CASE quarter\n",
    "                WHEN 'Q1' THEN 1\n",
    "                WHEN 'Q2' THEN 2\n",
    "                WHEN 'Q3' THEN 3\n",
    "                WHEN 'Q4' THEN 4\n",
    "                ELSE 5\n",
    "            END\n",
    "    \"\"\",\n",
    "        (bank_id, year),\n",
    "    )\n",
    "\n",
    "    reports = cursor.fetchall()\n",
    "\n",
    "    if not reports:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"No reports found for {bank_name} in {year}\")\n",
    "\n",
    "    # Initialize data structures\n",
    "    quarterly_sentiments = {}\n",
    "    all_role_sentiments = {}\n",
    "\n",
    "    # Process each report (quarter)\n",
    "    for report_id, quarter in reports:\n",
    "        # Get overall sentiment for this quarter's report\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            SELECT AVG(cs.compound_score)\n",
    "            FROM conversation_sentiment cs\n",
    "            JOIN analyst_conversations ac ON cs.conversation_id = ac.id\n",
    "            WHERE ac.report_id = ?\n",
    "        \"\"\",\n",
    "            (report_id,),\n",
    "        )\n",
    "\n",
    "        overall_sentiment = cursor.fetchone()[0]\n",
    "        quarterly_sentiments[quarter] = overall_sentiment\n",
    "\n",
    "        # Get sentiments by role for this quarter\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            SELECT speaker_role, AVG(compound_score)\n",
    "            FROM speaker_topic_sentiment\n",
    "            JOIN analyst_conversations ON speaker_topic_sentiment.conversation_id = analyst_conversations.id\n",
    "            WHERE analyst_conversations.report_id = ?\n",
    "            GROUP BY speaker_role\n",
    "        \"\"\",\n",
    "            (report_id,),\n",
    "        )\n",
    "\n",
    "        role_sentiments = {role: score for role, score in cursor.fetchall()}\n",
    "\n",
    "        # Store role sentiments by quarter\n",
    "        for role, sentiment in role_sentiments.items():\n",
    "            if role not in all_role_sentiments:\n",
    "                all_role_sentiments[role] = {}\n",
    "            all_role_sentiments[role][quarter] = sentiment\n",
    "\n",
    "    # For the latest quarter, determine sentiment gap between roles\n",
    "    latest_quarter = max(\n",
    "        quarterly_sentiments.keys(),\n",
    "        key=lambda q: {\"Q1\": 1, \"Q2\": 2, \"Q3\": 3, \"Q4\": 4}.get(q, 0),\n",
    "    )\n",
    "\n",
    "    # Get role sentiments for the latest quarter\n",
    "    latest_report_id = [\n",
    "        report_id for report_id, quarter in reports if quarter == latest_quarter\n",
    "    ][0]\n",
    "\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT speaker_role, AVG(compound_score)\n",
    "        FROM speaker_topic_sentiment\n",
    "        JOIN analyst_conversations ON speaker_topic_sentiment.conversation_id = analyst_conversations.id\n",
    "        WHERE analyst_conversations.report_id = ?\n",
    "        GROUP BY speaker_role\n",
    "    \"\"\",\n",
    "        (latest_report_id,),\n",
    "    )\n",
    "\n",
    "    latest_role_sentiments = {role: score for role, score in cursor.fetchall()}\n",
    "\n",
    "    # Calculate sentiment gap between roles\n",
    "    sentiment_gap = None\n",
    "    if len(latest_role_sentiments) >= 2:\n",
    "        roles = list(latest_role_sentiments.keys())\n",
    "        max_gap = 0\n",
    "        gap_roles = (roles[0], roles[1])\n",
    "\n",
    "        for i in range(len(roles)):\n",
    "            for j in range(i + 1, len(roles)):\n",
    "                gap = abs(\n",
    "                    latest_role_sentiments[roles[i]] - latest_role_sentiments[roles[j]]\n",
    "                )\n",
    "                if gap > max_gap:\n",
    "                    max_gap = gap\n",
    "                    gap_roles = (roles[i], roles[j])\n",
    "\n",
    "        sentiment_gap = {\n",
    "            \"role1\": gap_roles[0],\n",
    "            \"sentiment1\": latest_role_sentiments[gap_roles[0]],\n",
    "            \"role2\": gap_roles[1],\n",
    "            \"sentiment2\": latest_role_sentiments[gap_roles[1]],\n",
    "            \"gap\": max_gap,\n",
    "        }\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return {\n",
    "        \"overall_sentiment\": quarterly_sentiments.get(latest_quarter, 0),\n",
    "        \"role_sentiments\": latest_role_sentiments,\n",
    "        \"quarterly_sentiments\": quarterly_sentiments,\n",
    "        \"sentiment_gap\": sentiment_gap,\n",
    "        \"latest_quarter\": latest_quarter,\n",
    "        \"all_role_sentiments\": all_role_sentiments,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_sentiment_gauge(sentiment_score):\n",
    "    \"\"\"Create a gauge chart for the overall sentiment\"\"\"\n",
    "\n",
    "    # Define thresholds for sentiment interpretation\n",
    "    sentiment_thresholds = [\n",
    "        (-1.0, \"Very Negative\", \"#7b241c\"),  # Dark red\n",
    "        (-0.6, \"Negative\", \"#c0392b\"),  # Red\n",
    "        (-0.2, \"Slightly Negative\", \"#e74c3c\"),  # Light red\n",
    "        (0.0, \"Neutral\", \"#f39c12\"),  # Yellow\n",
    "        (0.2, \"Slightly Positive\", \"#2ecc71\"),  # Light green\n",
    "        (0.6, \"Positive\", \"#27ae60\"),  # Green\n",
    "        (1.0, \"Very Positive\", \"#1e8449\"),  # Dark green\n",
    "    ]\n",
    "\n",
    "    # Create steps for gauge\n",
    "    steps = []\n",
    "    for i in range(len(sentiment_thresholds) - 1):\n",
    "        steps.append(\n",
    "            dict(\n",
    "                range=[sentiment_thresholds[i][0], sentiment_thresholds[i + 1][0]],\n",
    "                color=sentiment_thresholds[i][2],\n",
    "                thickness=0.75,\n",
    "                line=dict(width=0.5, color=\"white\"),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Determine sentiment label based on score\n",
    "    sentiment_label = \"Unknown\"\n",
    "    for i in range(len(sentiment_thresholds) - 1):\n",
    "        if (\n",
    "            sentiment_thresholds[i][0]\n",
    "            <= sentiment_score\n",
    "            < sentiment_thresholds[i + 1][0]\n",
    "        ):\n",
    "            sentiment_label = sentiment_thresholds[i + 1][1]\n",
    "            break\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=sentiment_score,\n",
    "            title={\"text\": f\"Overall Sentiment: {sentiment_label}\"},\n",
    "            gauge={\n",
    "                \"axis\": {\"range\": [-1, 1], \"tickwidth\": 1, \"tickcolor\": \"darkblue\"},\n",
    "                \"bar\": {\"color\": \"purple\"},  # Purple\n",
    "                \"bgcolor\": \"white\",\n",
    "                \"borderwidth\": 2,\n",
    "                \"bordercolor\": \"gray\",\n",
    "                \"steps\": steps,\n",
    "                \"threshold\": {\n",
    "                    \"line\": {\"color\": \"black\", \"width\": 4},\n",
    "                    \"thickness\": 0.75,\n",
    "                    \"value\": sentiment_score,\n",
    "                },\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300,\n",
    "        width=400,\n",
    "        margin=dict(t=30, b=30, l=30, r=30),\n",
    "        template=\"plotly_white\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_sentiment_by_role_chart(role_sentiments):\n",
    "    \"\"\"Create a bar chart for sentiment by role\"\"\"\n",
    "\n",
    "    roles = list(role_sentiments.keys())\n",
    "    sentiments = list(role_sentiments.values())\n",
    "\n",
    "    # Define colors based on sentiment values\n",
    "    colors = []\n",
    "    for sentiment in sentiments:\n",
    "        if sentiment < -0.6:\n",
    "            colors.append(\"#7b241c\")  # Very negative\n",
    "        elif sentiment < -0.2:\n",
    "            colors.append(\"#c0392b\")  # Negative\n",
    "        elif sentiment < 0:\n",
    "            colors.append(\"#e74c3c\")  # Slightly negative\n",
    "        elif sentiment < 0.2:\n",
    "            colors.append(\"#f39c12\")  # Neutral\n",
    "        elif sentiment < 0.6:\n",
    "            colors.append(\"#2ecc71\")  # Slightly positive\n",
    "        else:\n",
    "            colors.append(\"#27ae60\")  # Positive\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=roles,\n",
    "            y=sentiments,\n",
    "            marker_color=colors,\n",
    "            text=[f\"{s:.2f}\" for s in sentiments],\n",
    "            textposition=\"auto\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300,\n",
    "        width=500,\n",
    "        title=\"Sentiment by Role\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=40, b=30, l=40, r=30),\n",
    "        yaxis=dict(\n",
    "            title=\"Sentiment Score\",\n",
    "            range=[-1, 1],\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Role\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_quarterly_sentiment_chart(quarterly_sentiments):\n",
    "    \"\"\"Create a line chart for quarterly sentiment trend\"\"\"\n",
    "\n",
    "    # Sort quarters in chronological order\n",
    "    quarter_order = {\"Q1\": 1, \"Q2\": 2, \"Q3\": 3, \"Q4\": 4}\n",
    "    quarters = sorted(\n",
    "        quarterly_sentiments.keys(), key=lambda q: quarter_order.get(q, 5)\n",
    "    )\n",
    "    sentiments = [quarterly_sentiments[q] for q in quarters]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=quarters,\n",
    "            y=sentiments,\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(width=3, color=\"#1f77b4\"),\n",
    "            marker=dict(size=10, color=\"#1f77b4\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at sentiment=0\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=quarters[0],\n",
    "        y0=0,\n",
    "        x1=quarters[-1],\n",
    "        y1=0,\n",
    "        line=dict(\n",
    "            color=\"gray\",\n",
    "            width=1,\n",
    "            dash=\"dash\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=300,\n",
    "        width=400,\n",
    "        title=\"Quarterly Sentiment Trend\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=40, b=30, l=40, r=30),\n",
    "        yaxis=dict(\n",
    "            title=\"Sentiment Score\",\n",
    "            range=[-1, 1],\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            title=\"Quarter\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_sentiment_gap_card(gap_data):\n",
    "    \"\"\"Create a card-style visualization for sentiment gap using an indicator.\"\"\"\n",
    "\n",
    "    # Create a figure with subplots\n",
    "    fig = make_subplots(rows=1, cols=1, specs=[[{\"type\": \"indicator\"}]])\n",
    "\n",
    "    # Determine color based on gap size\n",
    "    gap_color = (\n",
    "        \"red\"\n",
    "        if gap_data[\"gap\"] > 0.5\n",
    "        else (\"orange\" if gap_data[\"gap\"] > 0.3 else \"black\")\n",
    "    )\n",
    "\n",
    "    # Use an indicator for a cleaner, more separated display\n",
    "    fig.add_trace(\n",
    "        go.Indicator(\n",
    "            mode=\"number+delta\",\n",
    "            value=gap_data[\"gap\"],\n",
    "            number={\"font\": {\"size\": 50, \"color\": gap_color}},\n",
    "            title={\n",
    "                \"text\": f\"<b>Sentiment Gap</b><br>{gap_data['role1']} vs {gap_data['role2']}\",\n",
    "                \"font\": {\"size\": 16},\n",
    "            },\n",
    "            delta={\n",
    "                \"reference\": 0.3,  # Threshold for concern\n",
    "                \"increasing\": {\"color\": \"red\"},\n",
    "                \"decreasing\": {\"color\": \"green\"},\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Add details for individual sentiment scores\n",
    "    fig.add_annotation(\n",
    "        x=0.5,\n",
    "        y=0.15,\n",
    "        text=f\"{gap_data['role1']}: {gap_data['sentiment1']:.2f} | {gap_data['role2']}: {gap_data['sentiment2']:.2f}\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=14),\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "    )\n",
    "\n",
    "    # Update layout for better spacing and separation\n",
    "    fig.update_layout(\n",
    "        height=350,  # Taller than default\n",
    "        margin=dict(t=100, b=100, l=60, r=60),  # Increased margins\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\",\n",
    "        # Add a border around the card\n",
    "        shapes=[\n",
    "            dict(\n",
    "                type=\"rect\",\n",
    "                xref=\"paper\",\n",
    "                yref=\"paper\",\n",
    "                x0=0,\n",
    "                y0=0,\n",
    "                x1=1,\n",
    "                y1=1,\n",
    "                line=dict(color=\"rgba(0,0,0,0.2)\", width=1),\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "arVox-pZj-5p",
    "outputId": "f0852554-d1fc-4962-cd71-5a1e0b25730a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved sentiment data for JPMorgan Chase in 2024\n",
      "Latest quarter: Q4\n",
      "Overall sentiment: -0.19\n",
      "Roles with sentiment data: ['CFO', 'Chairman and CEO']\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "gauge": {
          "axis": {
           "range": [
            -1,
            1
           ],
           "tickcolor": "darkblue",
           "tickwidth": 1
          },
          "bar": {
           "color": "purple"
          },
          "bgcolor": "white",
          "bordercolor": "gray",
          "borderwidth": 2,
          "steps": [
           {
            "color": "#7b241c",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             -1,
             -0.6
            ],
            "thickness": 0.75
           },
           {
            "color": "#c0392b",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             -0.6,
             -0.2
            ],
            "thickness": 0.75
           },
           {
            "color": "#e74c3c",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             -0.2,
             0
            ],
            "thickness": 0.75
           },
           {
            "color": "#f39c12",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             0,
             0.2
            ],
            "thickness": 0.75
           },
           {
            "color": "#2ecc71",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             0.2,
             0.6
            ],
            "thickness": 0.75
           },
           {
            "color": "#27ae60",
            "line": {
             "color": "white",
             "width": 0.5
            },
            "range": [
             0.6,
             1
            ],
            "thickness": 0.75
           }
          ],
          "threshold": {
           "line": {
            "color": "black",
            "width": 4
           },
           "thickness": 0.75,
           "value": -0.19056844396982342
          }
         },
         "mode": "gauge+number",
         "title": {
          "text": "Overall Sentiment: Neutral"
         },
         "type": "indicator",
         "value": -0.19056844396982342
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 30,
         "l": 30,
         "r": 30,
         "t": 30
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 400
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "#e74c3c",
           "#f39c12"
          ]
         },
         "text": [
          "-0.05",
          "0.16"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "CFO",
          "Chairman and CEO"
         ],
         "y": [
          -0.051648927586419244,
          0.1629880222802361
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 30,
         "l": 40,
         "r": 30,
         "t": 40
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Sentiment by Role"
        },
        "width": 500,
        "xaxis": {
         "title": {
          "text": "Role"
         }
        },
        "yaxis": {
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#1f77b4",
          "width": 3
         },
         "marker": {
          "color": "#1f77b4",
          "size": 10
         },
         "mode": "lines+markers",
         "type": "scatter",
         "x": [
          "Q1",
          "Q2",
          "Q3",
          "Q4"
         ],
         "y": [
          0.34950330887328496,
          0.09985019117593766,
          0.08831567466259002,
          -0.19056844396982342
         ]
        }
       ],
       "layout": {
        "height": 300,
        "margin": {
         "b": 30,
         "l": 40,
         "r": 30,
         "t": 40
        },
        "shapes": [
         {
          "line": {
           "color": "gray",
           "dash": "dash",
           "width": 1
          },
          "type": "line",
          "x0": "Q1",
          "x1": "Q4",
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Quarterly Sentiment Trend"
        },
        "width": 400,
        "xaxis": {
         "title": {
          "text": "Quarter"
         }
        },
        "yaxis": {
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "delta": {
          "decreasing": {
           "color": "green"
          },
          "increasing": {
           "color": "red"
          },
          "reference": 0.3
         },
         "mode": "number+delta",
         "number": {
          "font": {
           "color": "black",
           "size": 50
          }
         },
         "title": {
          "font": {
           "size": 16
          },
          "text": "<b>Sentiment Gap</b><br>CFO vs Chairman and CEO"
         },
         "type": "indicator",
         "value": 0.21463694986665532
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 14
          },
          "showarrow": false,
          "text": "CFO: -0.05 | Chairman and CEO: 0.16",
          "x": 0.5,
          "xref": "paper",
          "y": 0.15,
          "yref": "paper"
         }
        ],
        "height": 350,
        "margin": {
         "b": 100,
         "l": 60,
         "r": 60,
         "t": 100
        },
        "paper_bgcolor": "white",
        "plot_bgcolor": "white",
        "shapes": [
         {
          "line": {
           "color": "rgba(0,0,0,0.2)",
           "width": 1
          },
          "type": "rect",
          "x0": 0,
          "x1": 1,
          "xref": "paper",
          "y0": 0,
          "y1": 1,
          "yref": "paper"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bank_name = \"JPMorgan Chase\"\n",
    "year = 2024\n",
    "\n",
    "sentiment_data = get_sentiment_data(DB_PATH, bank_name, year)\n",
    "\n",
    "print(f\"Successfully retrieved sentiment data for {bank_name} in {year}\")\n",
    "print(f\"Latest quarter: {sentiment_data['latest_quarter']}\")\n",
    "print(f\"Overall sentiment: {sentiment_data['overall_sentiment']:.2f}\")\n",
    "print(f\"Roles with sentiment data: {list(sentiment_data['role_sentiments'].keys())}\")\n",
    "\n",
    "# Create figures using real data\n",
    "gauge_fig = create_sentiment_gauge(sentiment_data[\"overall_sentiment\"])\n",
    "bar_fig = create_sentiment_by_role_chart(sentiment_data[\"role_sentiments\"])\n",
    "line_fig = create_quarterly_sentiment_chart(sentiment_data[\"quarterly_sentiments\"])\n",
    "gap_fig = create_sentiment_gap_card(sentiment_data[\"sentiment_gap\"])\n",
    "\n",
    "# Display the figures\n",
    "gauge_fig.show()\n",
    "bar_fig.show()\n",
    "line_fig.show()\n",
    "gap_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gNon6anTzrP"
   },
   "source": [
    "### Topic clustering + Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "bq5WONzHT3t2"
   },
   "outputs": [],
   "source": [
    "def get_topic_data(db_path, bank_name, year, quarter):\n",
    "    \"\"\"Extract topic data from the database for visualizations\"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the bank_id for the given bank name\n",
    "    cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "    bank_id = cursor.fetchone()\n",
    "\n",
    "    if not bank_id:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"Bank '{bank_name}' not found in the database\")\n",
    "\n",
    "    bank_id = bank_id[0]\n",
    "\n",
    "    # Get report ID for the specified quarter\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT id\n",
    "        FROM reports\n",
    "        WHERE bank_id = ? AND year = ? AND quarter = ?\n",
    "    \"\"\",\n",
    "        (bank_id, year, quarter),\n",
    "    )\n",
    "\n",
    "    report_id = cursor.fetchone()\n",
    "\n",
    "    if not report_id:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"No report found for {bank_name} in {year} {quarter}\")\n",
    "\n",
    "    report_id = report_id[0]\n",
    "\n",
    "    # Get topic mentions and sentiment data\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT t.name as topic_name,\n",
    "               COUNT(*) as mention_count,\n",
    "               AVG(sts.compound_score) as avg_sentiment\n",
    "        FROM analyst_conversations ac\n",
    "        JOIN topics t ON ac.topic_id = t.id\n",
    "        LEFT JOIN speaker_topic_sentiment sts ON sts.conversation_id = ac.id\n",
    "        WHERE ac.report_id = ?\n",
    "        GROUP BY t.name\n",
    "        ORDER BY mention_count DESC\n",
    "    \"\"\",\n",
    "        (report_id,),\n",
    "    )\n",
    "\n",
    "    topic_data = [\n",
    "        {\n",
    "            \"topic_name\": row[0],\n",
    "            \"mention_count\": row[1],\n",
    "            \"avg_sentiment\": row[2] if row[2] is not None else 0,\n",
    "        }\n",
    "        for row in cursor.fetchall()\n",
    "    ]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return {\n",
    "        \"topic_data\": topic_data,\n",
    "        \"bank_name\": bank_name,\n",
    "        \"year\": year,\n",
    "        \"quarter\": quarter,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_topic_trends(db_path, bank_name, year, topic_names=None):\n",
    "    \"\"\"Extract topic sentiment trends data from the database\"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the bank_id for the given bank name\n",
    "    cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "    bank_id = cursor.fetchone()\n",
    "\n",
    "    if not bank_id:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"Bank '{bank_name}' not found in the database\")\n",
    "\n",
    "    bank_id = bank_id[0]\n",
    "\n",
    "    # Get report IDs for all quarters of the specified year\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT id, quarter\n",
    "        FROM reports\n",
    "        WHERE bank_id = ? AND year = ?\n",
    "        ORDER BY\n",
    "            CASE quarter\n",
    "                WHEN 'Q1' THEN 1\n",
    "                WHEN 'Q2' THEN 2\n",
    "                WHEN 'Q3' THEN 3\n",
    "                WHEN 'Q4' THEN 4\n",
    "                ELSE 5\n",
    "            END\n",
    "    \"\"\",\n",
    "        (bank_id, year),\n",
    "    )\n",
    "\n",
    "    reports = cursor.fetchall()\n",
    "\n",
    "    if not reports:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"No reports found for {bank_name} in {year}\")\n",
    "\n",
    "    # Initialize data structure\n",
    "    topic_trends = {}\n",
    "\n",
    "    # Process each report (quarter)\n",
    "    for report_id, quarter in reports:\n",
    "        query_params = [report_id]\n",
    "        topic_filter = \"\"\n",
    "\n",
    "        if topic_names:\n",
    "            topic_filter = \"AND t.name IN ({})\".format(\n",
    "                \",\".join(\"?\" for _ in topic_names)\n",
    "            )\n",
    "            query_params.extend(topic_names)\n",
    "\n",
    "        # Get topic sentiment for this quarter\n",
    "        cursor.execute(\n",
    "            f\"\"\"\n",
    "            SELECT t.name as topic_name,\n",
    "                   AVG(sts.compound_score) as avg_sentiment\n",
    "            FROM analyst_conversations ac\n",
    "            JOIN topics t ON ac.topic_id = t.id\n",
    "            LEFT JOIN speaker_topic_sentiment sts ON sts.conversation_id = ac.id\n",
    "            WHERE ac.report_id = ? {topic_filter}\n",
    "            GROUP BY t.name\n",
    "        \"\"\",\n",
    "            query_params,\n",
    "        )\n",
    "\n",
    "        quarter_data = cursor.fetchall()\n",
    "\n",
    "        for topic_name, avg_sentiment in quarter_data:\n",
    "            if topic_name not in topic_trends:\n",
    "                topic_trends[topic_name] = {}\n",
    "\n",
    "            topic_trends[topic_name][quarter] = (\n",
    "                avg_sentiment if avg_sentiment is not None else 0\n",
    "            )\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return {\n",
    "        \"topic_trends\": topic_trends,\n",
    "        \"bank_name\": bank_name,\n",
    "        \"year\": year,\n",
    "        \"quarters\": [q for _, q in reports],\n",
    "    }\n",
    "\n",
    "\n",
    "def get_bank_comparison_data(db_path, bank_names, year, quarter):\n",
    "    \"\"\"Extract data for comparing multiple banks\"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Initialize data structures\n",
    "    risk_metrics = []\n",
    "    sentiment_data = []\n",
    "\n",
    "    for bank_name in bank_names:\n",
    "        # Get the bank_id for the given bank name\n",
    "        cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "        bank_id = cursor.fetchone()\n",
    "\n",
    "        if not bank_id:\n",
    "            print(f\"Bank '{bank_name}' not found in the database, skipping\")\n",
    "            continue\n",
    "\n",
    "        bank_id = bank_id[0]\n",
    "\n",
    "        # Get report ID for the specified quarter\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            SELECT id\n",
    "            FROM reports\n",
    "            WHERE bank_id = ? AND year = ? AND quarter = ?\n",
    "        \"\"\",\n",
    "            (bank_id, year, quarter),\n",
    "        )\n",
    "\n",
    "        report_id = cursor.fetchone()\n",
    "\n",
    "        if not report_id:\n",
    "            print(f\"No report found for {bank_name} in {year} {quarter}, skipping\")\n",
    "            continue\n",
    "\n",
    "        report_id = report_id[0]\n",
    "\n",
    "        # Get risk metrics\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            SELECT metric_name, metric_value\n",
    "            FROM metrics\n",
    "            WHERE report_id = ?\n",
    "            AND metric_name IN ('NPL Ratio', 'Coverage Ratio', 'Provisions', 'CET1 Ratio')\n",
    "        \"\"\",\n",
    "            (report_id,),\n",
    "        )\n",
    "\n",
    "        metrics = {metric: value for metric, value in cursor.fetchall()}\n",
    "        metrics[\"bank_name\"] = bank_name\n",
    "        risk_metrics.append(metrics)\n",
    "\n",
    "        # Get sentiment data\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            SELECT AVG(cs.compound_score)\n",
    "            FROM conversation_sentiment cs\n",
    "            JOIN analyst_conversations ac ON cs.conversation_id = ac.id\n",
    "            WHERE ac.report_id = ?\n",
    "        \"\"\",\n",
    "            (report_id,),\n",
    "        )\n",
    "\n",
    "        overall_sentiment = cursor.fetchone()[0]\n",
    "\n",
    "        sentiment_data.append(\n",
    "            {\n",
    "                \"bank_name\": bank_name,\n",
    "                \"overall_sentiment\": overall_sentiment\n",
    "                if overall_sentiment is not None\n",
    "                else 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return {\n",
    "        \"risk_metrics\": risk_metrics,\n",
    "        \"sentiment_data\": sentiment_data,\n",
    "        \"year\": year,\n",
    "        \"quarter\": quarter,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_risk_metrics_trend(db_path, bank_name, year):\n",
    "    \"\"\"Extract risk metrics trends for a bank\"\"\"\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get the bank_id for the given bank name\n",
    "    cursor.execute(\"SELECT id FROM banks WHERE bank_name = ?\", (bank_name,))\n",
    "    bank_id = cursor.fetchone()\n",
    "\n",
    "    if not bank_id:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"Bank '{bank_name}' not found in the database\")\n",
    "\n",
    "    bank_id = bank_id[0]\n",
    "\n",
    "    # Get report IDs for all quarters of the specified year\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT id, quarter\n",
    "        FROM reports\n",
    "        WHERE bank_id = ? AND year = ?\n",
    "        ORDER BY\n",
    "            CASE quarter\n",
    "                WHEN 'Q1' THEN 1\n",
    "                WHEN 'Q2' THEN 2\n",
    "                WHEN 'Q3' THEN 3\n",
    "                WHEN 'Q4' THEN 4\n",
    "                ELSE 5\n",
    "            END\n",
    "    \"\"\",\n",
    "        (bank_id, year),\n",
    "    )\n",
    "\n",
    "    reports = cursor.fetchall()\n",
    "\n",
    "    if not reports:\n",
    "        conn.close()\n",
    "        raise ValueError(f\"No reports found for {bank_name} in {year}\")\n",
    "\n",
    "    # Get all metrics for these reports\n",
    "    report_ids = [r[0] for r in reports]\n",
    "    quarters = [r[1] for r in reports]\n",
    "\n",
    "    placeholders = \",\".join(\"?\" for _ in report_ids)\n",
    "\n",
    "    cursor.execute(\n",
    "        f\"\"\"\n",
    "        SELECT r.quarter, m.metric_name, m.metric_value, m.in_typical_range\n",
    "        FROM metrics m\n",
    "        JOIN reports r ON m.report_id = r.id\n",
    "        WHERE m.report_id IN ({placeholders})\n",
    "    \"\"\",\n",
    "        report_ids,\n",
    "    )\n",
    "\n",
    "    metrics_data = cursor.fetchall()\n",
    "\n",
    "    # Transform to structured format\n",
    "    metrics_by_quarter = {}\n",
    "    for quarter, metric_name, metric_value, in_typical_range in metrics_data:\n",
    "        if quarter not in metrics_by_quarter:\n",
    "            metrics_by_quarter[quarter] = {}\n",
    "\n",
    "        metrics_by_quarter[quarter][metric_name] = {\n",
    "            \"value\": metric_value,\n",
    "            \"in_typical_range\": in_typical_range,\n",
    "        }\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    return {\n",
    "        \"metrics_by_quarter\": metrics_by_quarter,\n",
    "        \"bank_name\": bank_name,\n",
    "        \"year\": year,\n",
    "        \"quarters\": quarters,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_topic_mentions_plot(db_path, bank_name, year, quarter, n_topics=4):\n",
    "    \"\"\"Create a Plotly bar chart for top topics by mention count\"\"\"\n",
    "    try:\n",
    "        topic_data = get_topic_data(db_path, bank_name, year, quarter)\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        return None, []\n",
    "\n",
    "    # Sort by mention count and select top N\n",
    "    sorted_topics = sorted(\n",
    "        topic_data[\"topic_data\"], key=lambda x: x[\"mention_count\"], reverse=True\n",
    "    )\n",
    "\n",
    "    top_n = sorted_topics[:n_topics]\n",
    "\n",
    "    if not top_n:\n",
    "        print(f\"No topic data found for {bank_name} in {year} {quarter}\")\n",
    "        return None, []\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[t[\"topic_name\"] for t in top_n],\n",
    "            y=[t[\"mention_count\"] for t in top_n],\n",
    "            marker_color=\"#4682B4\",  # Steel blue color\n",
    "            text=[t[\"mention_count\"] for t in top_n],\n",
    "            textposition=\"auto\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=600,\n",
    "        title_text=f\"{bank_name} - {year} {quarter}: Top {n_topics} Topics by Mention Count\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=80, l=40, r=40),\n",
    "        xaxis=dict(\n",
    "            title=\"Topic\",\n",
    "            tickangle=45,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Mention Count\",\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig, [t[\"topic_name\"] for t in top_n]\n",
    "\n",
    "\n",
    "def create_topic_sentiment_plot(\n",
    "    db_path, bank_name, year, quarter, n_topics=None, topic_names=None\n",
    "):\n",
    "    \"\"\"Create a Plotly bar chart for topic sentiment comparison\"\"\"\n",
    "    try:\n",
    "        topic_data = get_topic_data(db_path, bank_name, year, quarter)\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    # Filter by provided topic names or use n_topics\n",
    "    if topic_names:\n",
    "        selected_topics = [\n",
    "            t for t in topic_data[\"topic_data\"] if t[\"topic_name\"] in topic_names\n",
    "        ]\n",
    "    else:\n",
    "        # Sort by absolute sentiment to show most polarized topics\n",
    "        selected_topics = sorted(\n",
    "            topic_data[\"topic_data\"],\n",
    "            key=lambda x: abs(x[\"avg_sentiment\"]),\n",
    "            reverse=True,\n",
    "        )\n",
    "        if n_topics:\n",
    "            selected_topics = selected_topics[:n_topics]\n",
    "\n",
    "    if not selected_topics:\n",
    "        print(f\"No matching topics found for {bank_name} in {year} {quarter}\")\n",
    "        return None\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create color mapping based on sentiment\n",
    "    colors = []\n",
    "    for topic in selected_topics:\n",
    "        sentiment = topic[\"avg_sentiment\"]\n",
    "        if sentiment < -0.6:\n",
    "            colors.append(\"#7b241c\")  # Very negative\n",
    "        elif sentiment < -0.2:\n",
    "            colors.append(\"#c0392b\")  # Negative\n",
    "        elif sentiment < 0:\n",
    "            colors.append(\"#e74c3c\")  # Slightly negative\n",
    "        elif sentiment < 0.2:\n",
    "            colors.append(\"#f39c12\")  # Neutral\n",
    "        elif sentiment < 0.6:\n",
    "            colors.append(\"#2ecc71\")  # Slightly positive\n",
    "        else:\n",
    "            colors.append(\"#27ae60\")  # Positive\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=[t[\"topic_name\"] for t in selected_topics],\n",
    "            y=[t[\"avg_sentiment\"] for t in selected_topics],\n",
    "            marker_color=colors,\n",
    "            text=[f\"{t['avg_sentiment']:.2f}\" for t in selected_topics],\n",
    "            textposition=\"auto\",\n",
    "            hovertext=[f\"Mentions: {t['mention_count']}\" for t in selected_topics],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=-0.5,\n",
    "        y0=0,\n",
    "        x1=len(selected_topics) - 0.5,\n",
    "        y1=0,\n",
    "        line=dict(\n",
    "            color=\"black\",\n",
    "            width=1,\n",
    "            dash=\"dash\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=600,\n",
    "        title_text=f\"{bank_name} - {year} {quarter}: Topic Sentiment Analysis\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=80, l=40, r=40),\n",
    "        xaxis=dict(\n",
    "            title=\"Topic\",\n",
    "            tickangle=45,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Average Sentiment Score\",\n",
    "            range=[-1, 1],\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_topic_sentiment_trends_plot(db_path, bank_name, year, topic_names=None):\n",
    "    \"\"\"Create a Plotly line chart for topic sentiment trends over time\"\"\"\n",
    "    try:\n",
    "        trend_data = get_topic_trends(db_path, bank_name, year, topic_names)\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    if not trend_data[\"topic_trends\"]:\n",
    "        print(f\"No topic sentiment trend data found for {bank_name}\")\n",
    "        return None\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Sort quarters chronologically\n",
    "    quarter_order = {\"Q1\": 1, \"Q2\": 2, \"Q3\": 3, \"Q4\": 4}\n",
    "    quarters = sorted(trend_data[\"quarters\"], key=lambda q: quarter_order.get(q, 5))\n",
    "\n",
    "    # Define a consistent color palette\n",
    "    colors = [\n",
    "        \"#1f77b4\",  # blue\n",
    "        \"#ff7f0e\",  # orange\n",
    "        \"#2ca02c\",  # green\n",
    "        \"#d62728\",  # red\n",
    "        \"#9467bd\",  # purple\n",
    "        \"#8c564b\",  # brown\n",
    "        \"#e377c2\",  # pink\n",
    "        \"#7f7f7f\",  # gray\n",
    "        \"#bcbd22\",  # olive\n",
    "        \"#17becf\",  # teal\n",
    "    ]\n",
    "\n",
    "    # Plot each topic\n",
    "    i = 0\n",
    "    for topic_name, sentiment_by_quarter in trend_data[\"topic_trends\"].items():\n",
    "        # Only include topics with data for at least 2 quarters\n",
    "        quarters_with_data = [q for q in quarters if q in sentiment_by_quarter]\n",
    "        if len(quarters_with_data) < 2:\n",
    "            continue\n",
    "\n",
    "        # Create data points\n",
    "        x_values = quarters_with_data\n",
    "        y_values = [sentiment_by_quarter.get(q, 0) for q in quarters_with_data]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_values,\n",
    "                y=y_values,\n",
    "                mode=\"lines+markers\",\n",
    "                name=topic_name,\n",
    "                line=dict(width=2, color=colors[i % len(colors)]),\n",
    "                marker=dict(size=8, color=colors[i % len(colors)]),\n",
    "            ),\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "    # Add a horizontal line at y=0\n",
    "    if quarters:\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=quarters[0],\n",
    "            y0=0,\n",
    "            x1=quarters[-1],\n",
    "            y1=0,\n",
    "            line=dict(\n",
    "                color=\"gray\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=600,\n",
    "        title_text=f\"{bank_name}: Topic Sentiment Trends ({year})\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=80, l=40, r=40),\n",
    "        xaxis=dict(\n",
    "            title=\"Quarter\",\n",
    "            tickangle=45,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Average Sentiment Score\",\n",
    "            range=[-1, 1],\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "        legend_title=\"Topics\",\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_bank_comparison_plot(\n",
    "    db_path, bank_names, year, quarter, metric_type=\"sentiment\"\n",
    "):\n",
    "    \"\"\"Create Plotly comparison charts between banks\"\"\"\n",
    "    try:\n",
    "        comparison_data = get_bank_comparison_data(db_path, bank_names, year, quarter)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving comparison data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    if metric_type == \"sentiment\":\n",
    "        # Create sentiment comparison plot\n",
    "        sentiment_data = comparison_data[\"sentiment_data\"]\n",
    "        if not sentiment_data:\n",
    "            print(\"No sentiment data available for comparison\")\n",
    "            return None\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Determine colors based on sentiment\n",
    "        colors = []\n",
    "        for bank in sentiment_data:\n",
    "            sentiment = bank[\"overall_sentiment\"]\n",
    "            if sentiment < -0.2:\n",
    "                colors.append(\"#c0392b\")  # Negative - red\n",
    "            elif sentiment < 0.2:\n",
    "                colors.append(\"#f1c40f\")  # Neutral - yellow\n",
    "            else:\n",
    "                colors.append(\"#2ecc71\")  # Positive - green\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[b[\"bank_name\"] for b in sentiment_data],\n",
    "                y=[b[\"overall_sentiment\"] for b in sentiment_data],\n",
    "                marker_color=colors,\n",
    "                text=[f\"{s['overall_sentiment']:.2f}\" for s in sentiment_data],\n",
    "                textposition=\"auto\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Add a horizontal line at y=0\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=-0.5,\n",
    "            y0=0,\n",
    "            x1=len(sentiment_data) - 0.5,\n",
    "            y1=0,\n",
    "            line=dict(\n",
    "                color=\"gray\",\n",
    "                width=1,\n",
    "                dash=\"dash\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=400,\n",
    "            width=600,\n",
    "            title_text=f\"Bank Sentiment Comparison - {year} {quarter}\",\n",
    "            template=\"plotly_white\",\n",
    "            margin=dict(t=100, b=50, l=40, r=40),\n",
    "            xaxis=dict(\n",
    "                title=\"Bank\",\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=\"Overall Sentiment\",\n",
    "                range=[-1, 1],\n",
    "                gridcolor=\"LightGray\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Create risk metric comparison plot\n",
    "        risk_metrics = comparison_data[\"risk_metrics\"]\n",
    "        if not risk_metrics:\n",
    "            print(\"No risk metric data available for comparison\")\n",
    "            return None\n",
    "\n",
    "        # Find the most common available metric\n",
    "        available_metrics = [\"NPL Ratio\", \"Coverage Ratio\", \"Provisions\", \"CET1 Ratio\"]\n",
    "        selected_metric = None\n",
    "\n",
    "        for metric in available_metrics:\n",
    "            count = sum(1 for bank in risk_metrics if metric in bank)\n",
    "            if count >= len(risk_metrics) / 2:  # Available in at least half of banks\n",
    "                selected_metric = metric\n",
    "                break\n",
    "\n",
    "        if not selected_metric:\n",
    "            print(\"No consistent risk metric available across banks\")\n",
    "            return None\n",
    "\n",
    "        # Filter banks with this metric\n",
    "        filtered_data = [\n",
    "            {\"bank_name\": bank[\"bank_name\"], \"value\": bank[selected_metric]}\n",
    "            for bank in risk_metrics\n",
    "            if selected_metric in bank\n",
    "        ]\n",
    "\n",
    "        if not filtered_data:\n",
    "            return None\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Determine colors based on metric type (higher is better or worse)\n",
    "        if selected_metric in [\"NPL Ratio\"]:\n",
    "            # Lower is better\n",
    "            median_value = np.median([b[\"value\"] for b in filtered_data])\n",
    "            colors = [\n",
    "                \"#2ecc71\" if b[\"value\"] < median_value else \"#c0392b\"\n",
    "                for b in filtered_data\n",
    "            ]\n",
    "        else:\n",
    "            # Higher is better\n",
    "            median_value = np.median([b[\"value\"] for b in filtered_data])\n",
    "            colors = [\n",
    "                \"#2ecc71\" if b[\"value\"] > median_value else \"#c0392b\"\n",
    "                for b in filtered_data\n",
    "            ]\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[b[\"bank_name\"] for b in filtered_data],\n",
    "                y=[b[\"value\"] for b in filtered_data],\n",
    "                marker_color=colors,\n",
    "                text=[f\"{b['value']:.2f}\" for b in filtered_data],\n",
    "                textposition=\"auto\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=400,\n",
    "            width=600,\n",
    "            title_text=f\"Bank Comparison: {selected_metric} - {year} {quarter}\",\n",
    "            template=\"plotly_white\",\n",
    "            margin=dict(t=100, b=50, l=40, r=40),\n",
    "            xaxis=dict(\n",
    "                title=\"Bank\",\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                title=selected_metric,\n",
    "                gridcolor=\"LightGray\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_risk_metrics_plot(db_path, bank_name, year, metric_name=None):\n",
    "    \"\"\"Create Plotly line charts for risk metrics trends\"\"\"\n",
    "    try:\n",
    "        metrics_data = get_risk_metrics_trend(db_path, bank_name, year)\n",
    "    except ValueError as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    if not metrics_data[\"metrics_by_quarter\"]:\n",
    "        print(f\"No risk metrics data found for {bank_name} in {year}\")\n",
    "        return None\n",
    "\n",
    "    # Identify available metrics\n",
    "    all_metrics = set()\n",
    "    for quarter_data in metrics_data[\"metrics_by_quarter\"].values():\n",
    "        all_metrics.update(quarter_data.keys())\n",
    "\n",
    "    # If metric_name is provided, check if it's available\n",
    "    if metric_name and metric_name not in all_metrics:\n",
    "        print(f\"Metric '{metric_name}' not found for {bank_name}\")\n",
    "        metric_name = None\n",
    "\n",
    "    # If no specific metric requested, take the first available one\n",
    "    if not metric_name:\n",
    "        # Prefer some common metrics if available\n",
    "        preferred_metrics = [\"NPL Ratio\", \"Coverage Ratio\", \"CET1 Ratio\", \"ROE\", \"ROA\"]\n",
    "        for metric in preferred_metrics:\n",
    "            if metric in all_metrics:\n",
    "                metric_name = metric\n",
    "                break\n",
    "\n",
    "        # If none of the preferred metrics are available, use the first one\n",
    "        if not metric_name and all_metrics:\n",
    "            metric_name = next(iter(all_metrics))\n",
    "\n",
    "    if not metric_name:\n",
    "        print(\"No metrics found for plotting\")\n",
    "        return None\n",
    "\n",
    "    # Create Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Sort quarters chronologically\n",
    "    quarter_order = {\"Q1\": 1, \"Q2\": 2, \"Q3\": 3, \"Q4\": 4}\n",
    "    quarters = sorted(metrics_data[\"quarters\"], key=lambda q: quarter_order.get(q, 5))\n",
    "\n",
    "    # Get values for the selected metric across quarters\n",
    "    values = []\n",
    "    in_range_status = []\n",
    "\n",
    "    for quarter in quarters:\n",
    "        quarter_data = metrics_data[\"metrics_by_quarter\"].get(quarter, {})\n",
    "        metric_data = quarter_data.get(metric_name)\n",
    "\n",
    "        if metric_data:\n",
    "            values.append(metric_data[\"value\"])\n",
    "            in_range_status.append(metric_data.get(\"in_typical_range\", True))\n",
    "        else:\n",
    "            values.append(None)\n",
    "            in_range_status.append(True)\n",
    "\n",
    "    # Remove None values while preserving quarter alignment\n",
    "    plot_quarters = []\n",
    "    plot_values = []\n",
    "    plot_colors = []\n",
    "\n",
    "    for i, val in enumerate(values):\n",
    "        if val is not None:\n",
    "            plot_quarters.append(quarters[i])\n",
    "            plot_values.append(val)\n",
    "            # Use red for out-of-range values\n",
    "            plot_colors.append(\"#c0392b\" if in_range_status[i] is False else \"#1f77b4\")\n",
    "\n",
    "    if not plot_values:\n",
    "        print(f\"No data available for {metric_name}\")\n",
    "        return None\n",
    "\n",
    "    # Add traces for the metric\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=plot_quarters,\n",
    "            y=plot_values,\n",
    "            mode=\"lines+markers\",\n",
    "            line=dict(width=3, color=\"#1f77b4\"),\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=plot_colors,\n",
    "            ),\n",
    "            text=[f\"{v:.2f}\" for v in plot_values],\n",
    "            textposition=\"top center\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=400,\n",
    "        width=600,\n",
    "        title_text=f\"{bank_name}: {metric_name} Trend ({year})\",\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(t=100, b=80, l=40, r=40),\n",
    "        xaxis=dict(\n",
    "            title=\"Quarter\",\n",
    "            tickangle=45,\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=metric_name,\n",
    "            gridcolor=\"LightGray\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def run_comprehensive_topic_analysis(\n",
    "    db_path, bank_name, year, quarter, n_topics=4, display=True\n",
    "):\n",
    "    \"\"\"Run a comprehensive topic analysis and display all visualizations\"\"\"\n",
    "\n",
    "    # Create topic mentions plot\n",
    "    mentions_fig, top_topics = create_topic_mentions_plot(\n",
    "        db_path,\n",
    "        bank_name,\n",
    "        year,\n",
    "        quarter,\n",
    "        n_topics,\n",
    "    )\n",
    "\n",
    "    # Create topic sentiment plot for the same topics\n",
    "    sentiment_fig = create_topic_sentiment_plot(\n",
    "        db_path,\n",
    "        bank_name,\n",
    "        year,\n",
    "        quarter,\n",
    "        topic_names=top_topics,\n",
    "    )\n",
    "\n",
    "    # Create topic trends plot\n",
    "    trends_fig = create_topic_sentiment_trends_plot(\n",
    "        DB_PATH,\n",
    "        bank_name,\n",
    "        year,\n",
    "        topic_names=top_topics,\n",
    "    )\n",
    "\n",
    "    # Display the figures\n",
    "    if display:\n",
    "        if mentions_fig:\n",
    "            mentions_fig.show()\n",
    "        if sentiment_fig:\n",
    "            sentiment_fig.show()\n",
    "        if trends_fig:\n",
    "            trends_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EDSSwQiEVv3L",
    "outputId": "95cc3dd2-7411-4b49-e2f8-33a3bb0e908d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#4682B4"
         },
         "text": [
          "3",
          "2",
          "1",
          "1"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "GROUP_RISK: term, see, say",
          "GROUP_RISK: republic, tech, spend",
          "MARKET_RISK: reprice, cut, since",
          "MARKET_RISK: market, office, cre"
         ],
         "y": [
          3,
          2,
          1,
          1
         ]
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "JPMorgan Chase - 2023 Q4: Top 4 Topics by Mention Count"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Topic"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "title": {
          "text": "Mention Count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertext": [
          "Mentions: 3",
          "Mentions: 2",
          "Mentions: 1",
          "Mentions: 1"
         ],
         "marker": {
          "color": [
           "#f39c12",
           "#27ae60",
           "#27ae60",
           "#7b241c"
          ]
         },
         "text": [
          "0.05",
          "1.00",
          "0.99",
          "-1.00"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "GROUP_RISK: term, see, say",
          "GROUP_RISK: republic, tech, spend",
          "MARKET_RISK: reprice, cut, since",
          "MARKET_RISK: market, office, cre"
         ],
         "y": [
          0.049038549264272056,
          0.9998375177383423,
          0.9858313202857971,
          -0.9976088404655457
         ]
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "shapes": [
         {
          "line": {
           "color": "black",
           "dash": "dash",
           "width": 1
          },
          "type": "line",
          "x0": -0.5,
          "x1": 3.5,
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "JPMorgan Chase - 2023 Q4: Topic Sentiment Analysis"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Topic"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Average Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#1f77b4",
          "width": 2
         },
         "marker": {
          "color": "#1f77b4",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "GROUP_RISK: term, see, say",
         "type": "scatter",
         "x": [
          "Q1",
          "Q2",
          "Q3",
          "Q4"
         ],
         "y": [
          0.6198114275932312,
          0.5412015654146671,
          0.20587258599698544,
          0.049038549264272056
         ]
        },
        {
         "line": {
          "color": "#ff7f0e",
          "width": 2
         },
         "marker": {
          "color": "#ff7f0e",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "MARKET_RISK: market, office, cre",
         "type": "scatter",
         "x": [
          "Q1",
          "Q3",
          "Q4"
         ],
         "y": [
          0.7801565229892731,
          0.999487578868866,
          -0.9976088404655457
         ]
        },
        {
         "line": {
          "color": "#2ca02c",
          "width": 2
         },
         "marker": {
          "color": "#2ca02c",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "MARKET_RISK: reprice, cut, since",
         "type": "scatter",
         "x": [
          "Q2",
          "Q4"
         ],
         "y": [
          -0.9822900891304016,
          0.9858313202857971
         ]
        },
        {
         "line": {
          "color": "#d62728",
          "width": 2
         },
         "marker": {
          "color": "#d62728",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "GROUP_RISK: republic, tech, spend",
         "type": "scatter",
         "x": [
          "Q3",
          "Q4"
         ],
         "y": [
          -0.028400063514709473,
          0.9998375177383423
         ]
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "Topics"
         }
        },
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "shapes": [
         {
          "line": {
           "color": "gray",
           "dash": "dash",
           "width": 1
          },
          "type": "line",
          "x0": "Q1",
          "x1": "Q4",
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "JPMorgan Chase: Topic Sentiment Trends (2023)"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Quarter"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Average Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": "#4682B4"
         },
         "text": [
          "14",
          "2",
          "1",
          "1"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "GROUP_RISK: term, see, say",
          "GROUP_RISK: merger, ask, markdown",
          "LIQUIDITY_RISK: liquidity, deterioration, excess",
          "LIQUIDITY_RISK: investment, peer, margin"
         ],
         "y": [
          14,
          2,
          1,
          1
         ]
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UBS - 2023 Q4: Top 4 Topics by Mention Count"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Topic"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "title": {
          "text": "Mention Count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertext": [
          "Mentions: 14",
          "Mentions: 2",
          "Mentions: 1",
          "Mentions: 1"
         ],
         "marker": {
          "color": [
           "#2ecc71",
           "#27ae60",
           "#7b241c",
           "#7b241c"
          ]
         },
         "text": [
          "0.27",
          "0.66",
          "-0.88",
          "-1.00"
         ],
         "textposition": "auto",
         "type": "bar",
         "x": [
          "GROUP_RISK: term, see, say",
          "GROUP_RISK: merger, ask, markdown",
          "LIQUIDITY_RISK: liquidity, deterioration, excess",
          "LIQUIDITY_RISK: investment, peer, margin"
         ],
         "y": [
          0.27244334667921066,
          0.6649852246046066,
          -0.8813439607620239,
          -0.9976111054420471
         ]
        }
       ],
       "layout": {
        "height": 400,
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "shapes": [
         {
          "line": {
           "color": "black",
           "dash": "dash",
           "width": 1
          },
          "type": "line",
          "x0": -0.5,
          "x1": 3.5,
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UBS - 2023 Q4: Topic Sentiment Analysis"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Topic"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Average Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "color": "#1f77b4",
          "width": 2
         },
         "marker": {
          "color": "#1f77b4",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "GROUP_RISK: term, see, say",
         "type": "scatter",
         "x": [
          "Q1",
          "Q2",
          "Q3",
          "Q4"
         ],
         "y": [
          0.5961036562919617,
          -0.4974166750907898,
          0.7463265284895897,
          0.27244334667921066
         ]
        },
        {
         "line": {
          "color": "#ff7f0e",
          "width": 2
         },
         "marker": {
          "color": "#ff7f0e",
          "size": 8
         },
         "mode": "lines+markers",
         "name": "GROUP_RISK: merger, ask, markdown",
         "type": "scatter",
         "x": [
          "Q2",
          "Q4"
         ],
         "y": [
          0.9999161958694458,
          0.6649852246046066
         ]
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "Topics"
         }
        },
        "margin": {
         "b": 80,
         "l": 40,
         "r": 40,
         "t": 100
        },
        "shapes": [
         {
          "line": {
           "color": "gray",
           "dash": "dash",
           "width": 1
          },
          "type": "line",
          "x0": "Q1",
          "x1": "Q4",
          "y0": 0,
          "y1": 0
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UBS: Topic Sentiment Trends (2023)"
        },
        "width": 600,
        "xaxis": {
         "tickangle": 45,
         "title": {
          "text": "Quarter"
         }
        },
        "yaxis": {
         "gridcolor": "LightGray",
         "range": [
          -1,
          1
         ],
         "title": {
          "text": "Average Sentiment Score"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the plot\n",
    "run_comprehensive_topic_analysis(DB_PATH, \"JPMorgan Chase\", 2023, \"Q4\")\n",
    "run_comprehensive_topic_analysis(DB_PATH, \"UBS\", 2023, \"Q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxNneNW56KOW"
   },
   "source": [
    "## üîç Testing FAISS Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHU6PjjRz0sa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd74fae6108436dbbc0424efdf7073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_query = \"What is the affecct of tighter lending standards?\"\n",
    "results = gsib_analysis_pipeline.faiss_manager.hybrid_search(user_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "r1cI_WkpjOQe",
    "outputId": "0cef4f64-507c-4949-9d84-6eae6bcc947b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vector_embedding_id': 343,\n",
       "  'preprocessed_text_id': 2058,\n",
       "  'qa_pair_id': 343,\n",
       "  'question': \"All fair points. And maybe just a follow-up on John's question on the lending environment. You talked about the industry likely pulling back. Are you changing your underwriting standards in any way? Just trying to think through, is there a potential for some market share gains given your strength of capital and liquidity, or how are you thinking about the loan environment?\",\n",
       "  'answer': \"Yes. And we always say, right, we underwrite through the cycle. And I think notably, we don't loosen our underwriting standards when all the numbers looked crazy good during the pandemic. And we're not going to like overreact now and tighten unreasonably. Some of that correction happens naturally. Credit metrics deteriorate for borrowers, whether in consumer or wholesale and that might make them leave our pre-existing risk appetite. But we're not running around aggressively tightening standards right now.\",\n",
       "  'answer_speaker': 'Jeremy Barnum',\n",
       "  'answer_role': 'CFO',\n",
       "  'text': 'fair point maybe follow lending environment talk industry likely pull back change underwriting standard way try potential market share gain give strength capital liquidity thinking loan environment yes always say right underwrite cycle notably loosen underwriting standard number look crazy pandemic go overreact tighten unreasonably correction happen naturally credit metric deteriorate borrower whether consumer wholesale might make leave pre exist risk appetite run around aggressively tighten standard right',\n",
       "  'vector_score': -0.15550768375396729,\n",
       "  'metadata': {'preprocessed_text_id': 2058,\n",
       "   'qa_pair_id': 343,\n",
       "   'conversation_id': 152,\n",
       "   'analyst_name': 'Jim Mitchell',\n",
       "   'analyst_company': 'Seaport Global Securities',\n",
       "   'bank': 'JPMorgan Chase',\n",
       "   'year': 2023,\n",
       "   'quarter': 'Q1',\n",
       "   'preprocessing_level': 'lemmatized'},\n",
       "  'rerank_score': -5.228943824768066},\n",
       " {'vector_embedding_id': 342,\n",
       "  'preprocessed_text_id': 2052,\n",
       "  'qa_pair_id': 342,\n",
       "  'question': \"All fair points. And maybe just a follow-up on John's question on the lending environment. You talked about the industry likely pulling back. Are you changing your underwriting standards in any way? Just trying to think through, is there a potential for some market share gains given your strength of capital and liquidity, or how are you thinking about the loan environment?\",\n",
       "  'answer': 'We say very modestly, but we look at that all the time.',\n",
       "  'answer_speaker': 'Jamie Dimon',\n",
       "  'answer_role': 'Chairman and CEO',\n",
       "  'text': 'fair point maybe follow lending environment talk industry likely pull back change underwriting standard way try potential market share gain give strength capital liquidity thinking loan environment say modestly look time',\n",
       "  'vector_score': -0.2532418966293335,\n",
       "  'metadata': {'preprocessed_text_id': 2052,\n",
       "   'qa_pair_id': 342,\n",
       "   'conversation_id': 152,\n",
       "   'analyst_name': 'Jim Mitchell',\n",
       "   'analyst_company': 'Seaport Global Securities',\n",
       "   'bank': 'JPMorgan Chase',\n",
       "   'year': 2023,\n",
       "   'quarter': 'Q1',\n",
       "   'preprocessing_level': 'lemmatized'},\n",
       "  'rerank_score': -9.646520614624023},\n",
       " {'vector_embedding_id': 27,\n",
       "  'preprocessed_text_id': 162,\n",
       "  'qa_pair_id': 27,\n",
       "  'question': \"Actually, I think you hit on it. So I'll just do a follow-up on a related -- so the notion of private credit doing large traditional investment-grade lending activity, it may be part of the competitive landscape that limits the ability to push price in Jamie's letter, you talked about the downside or my question is, what's the downside if more of the mortgage credit asset-backed intermediation business is pushed out of the banking system?\",\n",
       "  'answer': \"I mean, I guess it depends on what you mean by downside, but I just think societally speaking, I think we've seen in recent history that when home lending is happening outside the regulated perimeter and things get bad, when you have economic downturns, it produces bad outcomes for individuals and homeowners and society as a whole. So I mean, Jamie has written about this extensively. Beyond that, financially, we've talked about how mortgage lending -- I mean, the profitability swings obviously is reasonably cyclical. And in the recent past, it's actually been very profitable, then it was less so like the correspondent channel right now is actually picking up a little bit. But it's a thin margin business, it's challenging. And when you increase the capital requirements, it makes it even harder. So that just becomes one of the areas where you're in that tension between remixing versus pricing power that we talked about a second ago. And it might impact me in fact we do less, less credit available for homeowners and more regulatory risk as the activity moves outside the perimeter.\",\n",
       "  'answer_speaker': 'Jeremy Barnum',\n",
       "  'answer_role': 'CFO',\n",
       "  'text': 'actually hit follow related notion private credit large traditional investment grade lending activity may part competitive landscape limit ability push price talk downside downside mortgage credit asset back intermediation business push banking system mean guess depend mean downside societally speak see recent history home lending happen outside regulate perimeter thing get bad economic downturn produce bad outcome individual homeowner society whole mean write extensively beyond financially talk mortgage lending mean profitability swing obviously reasonably cyclical recent past actually profitable less correspondent channel right actually pick little bit thin margin business challenge increase capital requirement make even hard become one area tension remixe versus pricing power talk second ago might impact fact less less credit available homeowner regulatory risk activity move outside perimeter',\n",
       "  'vector_score': -0.22843551635742188,\n",
       "  'metadata': {'preprocessed_text_id': 162,\n",
       "   'qa_pair_id': 27,\n",
       "   'conversation_id': 9,\n",
       "   'analyst_name': 'Glenn Schorr',\n",
       "   'analyst_company': 'Evercore ISI',\n",
       "   'bank': 'JPMorgan Chase',\n",
       "   'year': 2023,\n",
       "   'quarter': 'Q2',\n",
       "   'preprocessing_level': 'lemmatized'},\n",
       "  'rerank_score': -10.845845222473145}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13J0qkVh4zje"
   },
   "outputs": [],
   "source": [
    "# Pass to LLM with context\n",
    "contexts = [result[\"text\"] for result in results]\n",
    "prompt = f\"Answer based on this context: {' '.join(contexts)}\\n\\nQuestion: {user_query}\"\n",
    "response = llm.generate(prompt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-htEFUnXF8i6",
    "nTYn7waG5p_F",
    "mzC0WxmFI96b",
    "V2-nD6aRASVf",
    "0QsY3TwaAkp6",
    "vxmlNlMrS9KB",
    "RJGaTOaoMtYG",
    "ydmU8qP4WQn8",
    "PL8bbAleXKRA",
    "SZBL9YzA7Hy4",
    "TGPvgl-h65TH",
    "c-zbSpiC_sk3"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
